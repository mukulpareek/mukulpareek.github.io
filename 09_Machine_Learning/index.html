<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Machine Learning - Business Analytics, Mukul Pareek</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Machine Learning";
        var mkdocs_page_input_path = "09_Machine_Learning.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Business Analytics, Mukul Pareek
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Introduction to Business Analytics</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../02_Exploratory_Data_Analysis/">Exploratory Data Analysis</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../03_Visualization_Basics/">Visualization Basics</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../04_Data_Preparation/">Data Preparation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../05_Introduction_to_Modeling/">Introduction to Modeling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../06_Recommender_Systems/">Recommender Systems</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../07_Regression/">Regression</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../08_Feature_Engineering/">Feature Engineering</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Machine Learning</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#what-we-will-cover">What we will cover</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#decision-trees">Decision Trees</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#measuring-purity-entropy-and-gini-impurity">Measuring Purity - Entropy and Gini Impurity</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#confusion-matrix-and-classification-report">Confusion Matrix and Classification Report</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#predictions-with-decision-trees">Predictions with decision trees</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#decision-tree-regression">Decision Tree Regression</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#how-well-did-my-model-generalize">How well did my model generalize?</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#addressing-overfitting-in-decision-trees">Addressing Overfitting in Decision Trees</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#random-forest">Random Forest</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#random-forest-for-classification">Random Forest for Classification</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#random-forest-for-regression">Random Forest for Regression</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#random-forest-regression-another-example">Random Forest Regression - Another Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#xgboost">XGBoost</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#xgboost-classification">XGBoost - Classification</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#change-results-by-varying-threshold">Change results by varying threshold</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboost-for-regression">XGBoost for Regression</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#linear-methods">Linear Methods</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#linear-discriminant-analysis">Linear Discriminant Analysis</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#support-vector-machines">Support Vector Machines</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#classification-with-svm">Classification with SVM</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#regression-with-svm">Regression with SVM</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#naive-bayes">Naive Bayes</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#k-nearest-neighbors">k-Nearest Neighbors</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#knn-classifier">kNN classifier</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#knn-regressor">kNN Regressor</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#k-means-clustering">k-Means Clustering</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#k-means-example">k-means - example</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#right-number-of-clusters">Right number of clusters</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#elbow-method">Elbow Method</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#silhouette-plot">Silhouette Plot</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#hierarchical-clustering">Hierarchical Clustering</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#key-takeaways">Key Takeaways</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pickle">Pickle</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#end">END</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#random-stuff">Random stuff</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#distances">Distances</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#diagram-for-lda">Diagram for LDA</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../10_Deep_Learning/">Deep Learning</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../11_Time_Series/">Time Series</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../12_Text_Data/">Text as Data</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../13.1_Transformers_and_LLMs/">Transformers and LLMs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../13.2_OpenAI/">OpenAI</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../13.3_Local_LLMs/">Local LLMs</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Business Analytics, Mukul Pareek</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Machine Learning</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="machine-learning-and-modeling">Machine Learning and Modeling</h1>
<h2 id="what-we-will-cover">What we will cover</h2>
<p>In the previous chapters, we looked at the larger field of artificial intelligence which relates to automating intellectual tasks performed by humans.  At one time, it was thought that all human decision making could be coded as a set of rules, which, if followed, would mimic intelligence.  The idea was that while these rules could be extremely complex in terms of their length and count, and in the way these were nested with each other, but in the end a set of properly structured if-else rules held the key to creating an artificial mind.  </p>
<p>Of course, we know now that is not accurate.  Rule based systems cannot generalize from patterns like the human mind does, and tend to be brittle to the point that they can be practically unusable.</p>
<p>Machine learning algorithms attempt to identify patterns in the data with which they create a solution to solve problems that haven't been seen before.  That is the topic for the discussion in this chapter.</p>
<p>Deep learning is a special case (or a subset) of machine learning where layers of data abstractions (called neural networks) are used.  However, you may hear of a distinction being sometimes made between machine learning, also sometimes called 'shallow learning', from deep learning that we will cover in the next chapter.  Machine learning is sometimes called 'shallow learning' because it is based on a single layer of data transformations.  It is called so to distinguish it from 'deep learning' that relies upon multiple layers of data transformations, with each layer extracting a different elements of useful information from the input.  </p>
<p>This is not to suggest that machine learning is less useful or less powerful than deep learning - on the contrary simpler algorithms regularly beat deep learning algorithms for certain kinds of tasks.  The type of learning to use is driven by the use case, performance obtained, and the desired explainability.</p>
<p>Next, we will cover the key machine learning algorithms that are used for classification, regression and clustering.  We will cover deep learning in the next chapter.  </p>
<p><strong>Agenda:</strong>  </p>
<ol>
<li>Decision Trees  </li>
<li>Random Forest  </li>
<li>XGBoost  </li>
<li>Linear Discriminant Analysis  </li>
<li>Support Vector Machines  </li>
<li>Naïve Bayes  </li>
<li>K-Nearest Neighbors  </li>
<li>K-Means Clustering  </li>
<li>Hierarchical Clustering  </li>
</ol>
<p>All our work will follow the ML workflow discussed earlier, and repeated below:</p>
<ol>
<li>Prepare your data – cleanse, convert to numbers, etc</li>
<li>Split the data into training and test sets<ul>
<li>Training sets are what algorithms learn from</li>
<li>Test sets are the ‘hold-out’ data on which model effectiveness is measured</li>
<li>No set rules, often a 80:20 split between train and test data suffices.  If there is a lot of training data, you may keep a smaller number as the test set.</li>
</ul>
</li>
<li>Fit a model.  </li>
<li>Check model accuracy based on the test set. </li>
<li>Use for predictions.</li>
</ol>
<p><strong>What you need to think about</strong><br />
As we cover the algorithms, think about the below ideas for each.</p>
<ol>
<li>What is the conceptual basis for the algorithm?  <ul>
<li>This will help you think about the problems the algorithm can be applied to,</li>
<li>You should also think about the parameters you can control in the model,  </li>
<li>You should think about model explainability, how essential is it to your use case, and who your audience is.</li>
</ul>
</li>
<li>Do you need to scale/standardize the data?<ul>
<li>Or can you use the raw data as is?</li>
</ul>
</li>
<li>Whether it can perform regression, classification or clustering<ul>
<li>Regression models help forecast numeric quantities, while classification algorithms help determine class membership.</li>
<li>Some algorithms can only perform either regression or classification, while others can do both.</li>
</ul>
</li>
<li>If it is a classification algorithm, does it provide just the class membership, or probability estimates<ul>
<li>If reliable probability estimates are available from the model, you can perform more advanced model evaluations, and tweak the probability cut-off to obtain your desired True Positive/False Positive rates.</li>
</ul>
</li>
</ol>
<p><strong>Some library imports first...</strong></p>
<pre><code class="language-python">import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
import seaborn as sns
from sklearn import tree
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.metrics import mean_absolute_error, mean_squared_error, ConfusionMatrixDisplay
from sklearn import metrics
# from sklearn.metrics import mean_absolute_percentage_error

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm

import sklearn.preprocessing as preproc
</code></pre>
<h2 id="decision-trees">Decision Trees</h2>
<p>Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression.  The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.  In simple words, decision trees are a collection of if/else conditions that are applied to data till a prediction is reached.  Trees are constructed by splitting data by a variable, and then doing the same again and again till the desired level of accuracy is reached (or we run out of data).  Trees can be visualized, and are therefore easier to interpret – and in that sense they are a ‘white box model’.  </p>
<p><strong>Trivial Example</strong><br />
Consider a made-up dataset where we know the employment and housing status of our customers, and whether they have paid back or defaulted on their loans.  When a new customer requests a loan, can we use this data to decide the whether there is likely to be a default?</p>
<p><img alt="image.png" src="../09_Machine_Learning_files/ed4945d9-3af3-4766-9b0f-f54ffeb27753.png" /></p>
<p>We would like the ‘leaf nodes’ to be pure, ie contain instances that tend to belong to the same class.  </p>
<p>Several practical issues arise in the above example:
 - Attributes rarely neatly split a group.  In the made up example, everything lined up neatly but rarely will in reality.<br />
 - How does one select what order to select attributes in?  We could have started with housing instead of looking at whether a person was an employee or not.<br />
 - Many attributes will not be binary, may have multiple unique values.<br />
 - Some attributes may be numeric.  For example, we may know their credit scores.  In such a case, how do we split the nodes?<br />
 - Finally, how do we decide we are done?  Should we keep going till we run out of variables, or till all leaf nodes are pure?  </p>
<h3 id="measuring-purity-entropy-and-gini-impurity">Measuring Purity - Entropy and Gini Impurity</h3>
<p><strong>Entropy</strong><br />
The most common splitting criterion is called information gain, and is based on a measure called entropy.
Entropy is a measure of disorder that can be applied to a collection. <br />
Disorder corresponds to how mixed (impure) the group is with respect to the properties of interest.  </p>
<p>
<script type="math/tex">\mbox{Entropy} = -p_1 log_2(p_1) -p_2 log_2(p_2) - ...</script>
</p>
<p>A node is pure when entropy = 0.  So we are looking for ways to minimize entropy.  </p>
<p><strong>Gini Impurity</strong><br />
Another measure of impurity is the Gini Impurity.<br />
<script type="math/tex">\mbox{Gini Index} = 1 - p_1^2 - p_2^2 </script>
</p>
<p>Like entropy, the Gini Impurity has a minimum of 0.  In a two class problem, the maximum value for the Gini Impurity will be 0.5.  Both Entropy and the Gini Impurity behave similarly, the Gini Impurity is supposedly less computationally intensive.  </p>
<p>With entropy as the measure of disorder, we calculate Information Gain offered by each attribute when used as the basis of segmentation.  </p>
<p>Information gain is the reduction in entropy by splitting our data on the basis of a single attribute.  </p>
<p>For our toy example, the entropy for the top parent node was 0.95.  This was reduced to 0.41 at the next child node, calculated as <script type="math/tex">p(c_1) * entropy(c_1) + p(c_2) * entropy(c_2) + ...</script>.  We start our segmentation with the attribute that provides the most information gain.  </p>
<p>Fortunately, automated algorithms do this for us, so we do not have to calculate any of this.  But the concept of information gain and how regression tree algorithms decide to split the data is important to be aware of.  </p>
<p><strong>Toy Example Continued</strong><br />
Let us continue the example introduced earlier.  </p>
<p><img alt="image.png" src="../09_Machine_Learning_files/80b71d9b-b1a0-4a36-a388-3979d8f8de7d.png" /></p>
<pre><code class="language-python"># Entropy before the first split
entropy1 = -((6/16) * np.log2(6/16))-((10/16) * np.log2(10/16))
entropy1
</code></pre>
<pre><code>0.954434002924965
</code></pre>
<pre><code class="language-python"># Entroy after the split
entropy2 = \
(8/16) * (-(8/8) * np.log2(8/8)) \
+ \
((8/16) * (- (2/8) * np.log2(2/8) - (6/8) * np.log2(6/8) ))
entropy2
</code></pre>
<pre><code>0.4056390622295664
</code></pre>
<pre><code class="language-python">#Information Gain
entropy1 - entropy2
</code></pre>
<pre><code>0.5487949406953987
</code></pre>
<hr />
<p><strong>Another simple example</strong><br />
We look at another example where we try to build a decision tree to predict whether a debt was written-off for a customer given other attributes.  </p>
<pre><code class="language-python">df = pd.read_excel('write-off.xlsx')
</code></pre>
<pre><code class="language-python">df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Balance</th>
      <th>Age</th>
      <th>Employed</th>
      <th>Write-off</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Mike</td>
      <td>200000</td>
      <td>42</td>
      <td>no</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Mary</td>
      <td>35000</td>
      <td>33</td>
      <td>yes</td>
      <td>no</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Claudio</td>
      <td>115000</td>
      <td>40</td>
      <td>no</td>
      <td>no</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Robert</td>
      <td>29000</td>
      <td>23</td>
      <td>yes</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Dora</td>
      <td>72000</td>
      <td>31</td>
      <td>no</td>
      <td>no</td>
    </tr>
  </tbody>
</table>
</div>

<p>Balance, Age and Employed are independent variables, and Write-off is the predicted variable. Of these, the Write-off and Employed columns are strings and have to be converted to numerical variables so they can be used in algorithms.</p>
<pre><code class="language-python">df['Write-off'] = df['Write-off'].astype('category') #convert to category
df['write-off-label'] = df['Write-off'].cat.codes #use category codes as labels
df = pd.get_dummies(df, columns=[&quot;Employed&quot;]) #one hot encoding using pandas
df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Balance</th>
      <th>Age</th>
      <th>Write-off</th>
      <th>write-off-label</th>
      <th>Employed_no</th>
      <th>Employed_yes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Mike</td>
      <td>200000</td>
      <td>42</td>
      <td>yes</td>
      <td>1</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Mary</td>
      <td>35000</td>
      <td>33</td>
      <td>no</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Claudio</td>
      <td>115000</td>
      <td>40</td>
      <td>no</td>
      <td>0</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Robert</td>
      <td>29000</td>
      <td>23</td>
      <td>yes</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Dora</td>
      <td>72000</td>
      <td>31</td>
      <td>no</td>
      <td>0</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">type(df['Write-off'])
</code></pre>
<pre><code>pandas.core.series.Series
</code></pre>
<pre><code class="language-python">df = df.iloc[:,[0,3,4,1,2,5,6]]
df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Write-off</th>
      <th>write-off-label</th>
      <th>Balance</th>
      <th>Age</th>
      <th>Employed_no</th>
      <th>Employed_yes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Mike</td>
      <td>yes</td>
      <td>1</td>
      <td>200000</td>
      <td>42</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Mary</td>
      <td>no</td>
      <td>0</td>
      <td>35000</td>
      <td>33</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Claudio</td>
      <td>no</td>
      <td>0</td>
      <td>115000</td>
      <td>40</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Robert</td>
      <td>yes</td>
      <td>1</td>
      <td>29000</td>
      <td>23</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Dora</td>
      <td>no</td>
      <td>0</td>
      <td>72000</td>
      <td>31</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">df.iloc[:, 2:]
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>write-off-label</th>
      <th>Balance</th>
      <th>Age</th>
      <th>Employed_no</th>
      <th>Employed_yes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>200000</td>
      <td>42</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>35000</td>
      <td>33</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>115000</td>
      <td>40</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>29000</td>
      <td>23</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>72000</td>
      <td>31</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># This below command is required only to get back to the home folder if you aren't there already

# import os
# os.chdir('/home/jovyan')
</code></pre>
<pre><code class="language-python">X = df.iloc[:,3:]
y = df.iloc[:,2]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, y)

import graphviz 
dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
# graph.render(&quot;df&quot;) 

dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names=X.columns,  
                         class_names=['yes', 'no'],  # Plain English names for classes_
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = graphviz.Source(dot_data)  
graph 
</code></pre>
<p><img alt="svg" src="../09_Machine_Learning_files/09_Machine_Learning_17_0.svg" /></p>
<pre><code class="language-python">clf.classes_
</code></pre>
<pre><code>array([0, 1], dtype=int8)
</code></pre>
<pre><code class="language-python">y
</code></pre>
<pre><code>0    1
1    0
2    0
3    1
4    0
Name: write-off-label, dtype: int8
</code></pre>
<hr />
<p><strong>Iris Flower Dataset</strong><br />
We consider the Iris dataset, a multivariate data set introduced by the British statistician and biologist Ronald Fisher in a 1936 paper.  The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor).  </p>
<p>Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.  </p>
<p><em>Source: Wikipedia</em>  </p>
<p><img alt="image.png" src="../09_Machine_Learning_files/090494b3-8773-41c7-adaf-4abcbe783a26.png" /><br />
<em>Image Source/Attribution: https://commons.wikimedia.org/w/index.php?curid=248095</em></p>
<p>Difference between a petal and a sepal:<br />
<img alt="image.png" src="../09_Machine_Learning_files/5f37d9dc-bb7e-462e-b2e9-817d0919eb1d.png" /></p>
<p>Scikit Learn’s decision tree classifier algorithm, combined with another package called graphviz, can provide decision trees together with good graphing capabilities.  </p>
<p>Unfortunately, sklearn requires all data to be numeric and as numpy arrays.  This creates practical problems for the data analyst – categorical variables have to be labeled or one-hot encoded, and their plain English meanings have to be tracked separately.  </p>
<pre><code class="language-python"># Load the data
iris = sm.datasets.get_rdataset('iris').data
iris.sample(6)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>77</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>1.7</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>103</th>
      <td>6.3</td>
      <td>2.9</td>
      <td>5.6</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>25</th>
      <td>5.0</td>
      <td>3.0</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>126</th>
      <td>6.2</td>
      <td>2.8</td>
      <td>4.8</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>55</th>
      <td>5.7</td>
      <td>2.8</td>
      <td>4.5</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>131</th>
      <td>7.9</td>
      <td>3.8</td>
      <td>6.4</td>
      <td>2.0</td>
      <td>virginica</td>
    </tr>
  </tbody>
</table>
</div>

<p>Our task is: Based on these features, can we create a decision tree to distinguish between the three species of the Iris flower?  </p>
<pre><code class="language-python"># Let us look at some basic descriptive stats for each of the flower species.  

iris.pivot_table(columns = ['Species'], aggfunc = [np.mean, min, max]).transpose()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
    </tr>
    <tr>
      <th></th>
      <th>Species</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3" valign="top">mean</th>
      <th>setosa</th>
      <td>1.462</td>
      <td>0.246</td>
      <td>5.006</td>
      <td>3.428</td>
    </tr>
    <tr>
      <th>versicolor</th>
      <td>4.260</td>
      <td>1.326</td>
      <td>5.936</td>
      <td>2.770</td>
    </tr>
    <tr>
      <th>virginica</th>
      <td>5.552</td>
      <td>2.026</td>
      <td>6.588</td>
      <td>2.974</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">min</th>
      <th>setosa</th>
      <td>1.000</td>
      <td>0.100</td>
      <td>4.300</td>
      <td>2.300</td>
    </tr>
    <tr>
      <th>versicolor</th>
      <td>3.000</td>
      <td>1.000</td>
      <td>4.900</td>
      <td>2.000</td>
    </tr>
    <tr>
      <th>virginica</th>
      <td>4.500</td>
      <td>1.400</td>
      <td>4.900</td>
      <td>2.200</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">max</th>
      <th>setosa</th>
      <td>1.900</td>
      <td>0.600</td>
      <td>5.800</td>
      <td>4.400</td>
    </tr>
    <tr>
      <th>versicolor</th>
      <td>5.100</td>
      <td>1.800</td>
      <td>7.000</td>
      <td>3.400</td>
    </tr>
    <tr>
      <th>virginica</th>
      <td>6.900</td>
      <td>2.500</td>
      <td>7.900</td>
      <td>3.800</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Next, we build the decision tree  

import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn import tree
iris = load_iris()
X, y = iris.data, iris.target
</code></pre>
<pre><code class="language-python"># Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
</code></pre>
<pre><code class="language-python"># Create the classifier and visualize the decision tree
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X_train, y_train)

import graphviz 
dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render(&quot;iris&quot;) 

dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names=iris.feature_names,  
                         class_names=iris.target_names,  
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = graphviz.Source(dot_data)  
graph 
</code></pre>
<p><img alt="svg" src="../09_Machine_Learning_files/09_Machine_Learning_26_0.svg" /></p>
<pre><code class="language-python"># Another way to build the tree is something as simple as 
# typing `tree.plot_tree(clf)` but the above code gives 
# us much better results.

print(tree.plot_tree(clf))
</code></pre>
<pre><code>[Text(0.4444444444444444, 0.9166666666666666, 'x[2] &lt;= 2.5\ngini = 0.666\nsamples = 120\nvalue = [43, 38, 39]'), Text(0.3333333333333333, 0.75, 'gini = 0.0\nsamples = 43\nvalue = [43, 0, 0]'), Text(0.5555555555555556, 0.75, 'x[3] &lt;= 1.75\ngini = 0.5\nsamples = 77\nvalue = [0, 38, 39]'), Text(0.3333333333333333, 0.5833333333333334, 'x[2] &lt;= 5.35\ngini = 0.139\nsamples = 40\nvalue = [0, 37, 3]'), Text(0.2222222222222222, 0.4166666666666667, 'x[2] &lt;= 4.95\ngini = 0.051\nsamples = 38\nvalue = [0, 37, 1]'), Text(0.1111111111111111, 0.25, 'gini = 0.0\nsamples = 35\nvalue = [0, 35, 0]'), Text(0.3333333333333333, 0.25, 'x[1] &lt;= 2.45\ngini = 0.444\nsamples = 3\nvalue = [0, 2, 1]'), Text(0.2222222222222222, 0.08333333333333333, 'gini = 0.0\nsamples = 1\nvalue = [0, 0, 1]'), Text(0.4444444444444444, 0.08333333333333333, 'gini = 0.0\nsamples = 2\nvalue = [0, 2, 0]'), Text(0.4444444444444444, 0.4166666666666667, 'gini = 0.0\nsamples = 2\nvalue = [0, 0, 2]'), Text(0.7777777777777778, 0.5833333333333334, 'x[2] &lt;= 4.85\ngini = 0.053\nsamples = 37\nvalue = [0, 1, 36]'), Text(0.6666666666666666, 0.4166666666666667, 'x[0] &lt;= 5.95\ngini = 0.5\nsamples = 2\nvalue = [0, 1, 1]'), Text(0.5555555555555556, 0.25, 'gini = 0.0\nsamples = 1\nvalue = [0, 1, 0]'), Text(0.7777777777777778, 0.25, 'gini = 0.0\nsamples = 1\nvalue = [0, 0, 1]'), Text(0.8888888888888888, 0.4166666666666667, 'gini = 0.0\nsamples = 35\nvalue = [0, 0, 35]')]
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_27_1.png" /></p>
<pre><code class="language-python"># List categories in the classifier
iris.target_names
</code></pre>
<pre><code>array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')
</code></pre>
<pre><code class="language-python"># Perform predictions
clf.predict(X_test)
</code></pre>
<pre><code>array([1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 2, 1, 2, 2,
       1, 0, 2, 0, 2, 0, 2, 0])
</code></pre>
<pre><code class="language-python">
</code></pre>
<h3 id="confusion-matrix-and-classification-report">Confusion Matrix and Classification Report</h3>
<p>We did not split the data into train/test sets.  For now, we will evaluate the model based on the entire data set (ie, on the training set).  </p>
<p>For this trivial example, the decision tree has done a perfect job of predicting flower species.</p>
<pre><code class="language-python">confusion_matrix(y_true = y_test, y_pred = clf.predict(X_test))
</code></pre>
<pre><code>array([[ 7,  0,  0],
       [ 0, 12,  0],
       [ 0,  2,  9]], dtype=int64)
</code></pre>
<pre><code class="language-python">print(classification_report(y_true = y_test, y_pred = clf.predict(X_test)))
</code></pre>
<pre><code>              precision    recall  f1-score   support

           0       1.00      1.00      1.00         7
           1       0.86      1.00      0.92        12
           2       1.00      0.82      0.90        11

    accuracy                           0.93        30
   macro avg       0.95      0.94      0.94        30
weighted avg       0.94      0.93      0.93        30
</code></pre>
<pre><code class="language-python">ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, display_labels=iris.target_names);
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_34_0.png" /></p>
<p><strong>Confusion Matrix for All Data</strong>  </p>
<pre><code class="language-python"># Just for the heck of it, let us predict the entire dataset using our model, 
# and check the results
print(classification_report(y_true = y, y_pred = clf.predict(X)))
</code></pre>
<pre><code>              precision    recall  f1-score   support

           0       1.00      1.00      1.00        50
           1       0.96      1.00      0.98        50
           2       1.00      0.96      0.98        50

    accuracy                           0.99       150
   macro avg       0.99      0.99      0.99       150
weighted avg       0.99      0.99      0.99       150
</code></pre>
<pre><code class="language-python">ConfusionMatrixDisplay.from_estimator(clf, X, y, display_labels=iris.target_names);
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_37_0.png" /></p>
<p><strong>Class probabilities with decision trees</strong>  </p>
<p>Decision Trees do not do a great job of predicting the probability of belonging to a particular class, for example, when compared to Logistic Regression.  </p>
<p>Probabilities for class membership are just the proportion of observations in a particular class in the appropriate leaf node.  For a tree with unlimited nodes, we will always mostly have p=100% for most predictions.  </p>
<p>Scikit Learn provides a method to predict probabilities, <code>clf.predict_proba()</code>.  If we apply this to our decision tree (first five observations only), we get as below:   </p>
<pre><code class="language-python"># As can be seen below, the model does not give class probabilities
clf.predict(X)
</code></pre>
<pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
</code></pre>
<pre><code class="language-python"># Get class probabilities
clf.predict_proba(X[:5])
</code></pre>
<pre><code>array([[1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.]])
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<hr />
<h3 id="predictions-with-decision-trees">Predictions with decision trees</h3>
<p><strong>With this model, how do I predict if I have the measurements for a new flower?</strong>  </p>
<p>Once a Decision Tree Classifier is built, new predictions can be obtained using the predict(X) method.  </p>
<p>Imagine we have a new flower with dimensions 5, 3, 1 and 2 and need to predict its species.
Since we have the featureset, we feed this information to the model and obtain the prediction.</p>
<p>Refer below for the steps in Python</p>
<pre><code class="language-python"># let us remind ourselves of what features need to predict a flower's species

iris.feature_names
</code></pre>
<pre><code>['sepal length (cm)',
 'sepal width (cm)',
 'petal length (cm)',
 'petal width (cm)']
</code></pre>
<pre><code class="language-python"># let us also look at existing feature set

X[:4]
</code></pre>
<pre><code>array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2]])
</code></pre>
<pre><code class="language-python"># Next, the measurements for the new flower

new_flower = [[5,3,1,2]]
</code></pre>
<pre><code class="language-python"># Now the prediction

clf.predict(new_flower)
</code></pre>
<pre><code>array([0])
</code></pre>
<pre><code class="language-python"># The above means it is the category at index 1 in the target
# Let us look at what the target names are.
# We see that the 'versicolor' is at index 1, so that is the prediction for the new flower

iris.target_names
</code></pre>
<pre><code>array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')
</code></pre>
<pre><code class="language-python"># or, all of the above in one line

print(iris.target_names[clf.predict(new_flower)])
</code></pre>
<pre><code>['setosa']
</code></pre>
<h3 id="decision-tree-regression">Decision Tree Regression</h3>
<p>Decision trees can also be applied to estimating continuous values for the target variable.  </p>
<p>They work in the same way as decision trees for classification, except that information gain is measured differently, eg by a reduction in standard deviation at the node level.  </p>
<p>So splits for a node would be performed based on a variable/value that creates the maximum reduction in the standard deviation of the y values in the node.  </p>
<p>The prediction is then the average of the observations in the leaf node.  </p>
<p>As an example, let us consider the Boston House Price dataset that is built into sklearn.  There are 506 rows × 14 variables</p>
<pre><code class="language-python"># Load the data

from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()

X = housing['data']
y = housing['target']
features = housing['feature_names']
DESCR = housing['DESCR']

cali_df = pd.DataFrame(X, columns = features)
cali_df.insert(0,'medv', y)
</code></pre>
<pre><code class="language-python">cali_df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>medv</th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4.526</td>
      <td>8.3252</td>
      <td>41.0</td>
      <td>6.984127</td>
      <td>1.023810</td>
      <td>322.0</td>
      <td>2.555556</td>
      <td>37.88</td>
      <td>-122.23</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.585</td>
      <td>8.3014</td>
      <td>21.0</td>
      <td>6.238137</td>
      <td>0.971880</td>
      <td>2401.0</td>
      <td>2.109842</td>
      <td>37.86</td>
      <td>-122.22</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.521</td>
      <td>7.2574</td>
      <td>52.0</td>
      <td>8.288136</td>
      <td>1.073446</td>
      <td>496.0</td>
      <td>2.802260</td>
      <td>37.85</td>
      <td>-122.24</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.413</td>
      <td>5.6431</td>
      <td>52.0</td>
      <td>5.817352</td>
      <td>1.073059</td>
      <td>558.0</td>
      <td>2.547945</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.422</td>
      <td>3.8462</td>
      <td>52.0</td>
      <td>6.281853</td>
      <td>1.081081</td>
      <td>565.0</td>
      <td>2.181467</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>20635</th>
      <td>0.781</td>
      <td>1.5603</td>
      <td>25.0</td>
      <td>5.045455</td>
      <td>1.133333</td>
      <td>845.0</td>
      <td>2.560606</td>
      <td>39.48</td>
      <td>-121.09</td>
    </tr>
    <tr>
      <th>20636</th>
      <td>0.771</td>
      <td>2.5568</td>
      <td>18.0</td>
      <td>6.114035</td>
      <td>1.315789</td>
      <td>356.0</td>
      <td>3.122807</td>
      <td>39.49</td>
      <td>-121.21</td>
    </tr>
    <tr>
      <th>20637</th>
      <td>0.923</td>
      <td>1.7000</td>
      <td>17.0</td>
      <td>5.205543</td>
      <td>1.120092</td>
      <td>1007.0</td>
      <td>2.325635</td>
      <td>39.43</td>
      <td>-121.22</td>
    </tr>
    <tr>
      <th>20638</th>
      <td>0.847</td>
      <td>1.8672</td>
      <td>18.0</td>
      <td>5.329513</td>
      <td>1.171920</td>
      <td>741.0</td>
      <td>2.123209</td>
      <td>39.43</td>
      <td>-121.32</td>
    </tr>
    <tr>
      <th>20639</th>
      <td>0.894</td>
      <td>2.3886</td>
      <td>16.0</td>
      <td>5.254717</td>
      <td>1.162264</td>
      <td>1387.0</td>
      <td>2.616981</td>
      <td>39.37</td>
      <td>-121.24</td>
    </tr>
  </tbody>
</table>
<p>20640 rows × 9 columns</p>
</div>
<pre><code class="language-python"># Let us look at the data dictionary

print(DESCR)
</code></pre>
<pre><code>.. _california_housing_dataset:

California Housing dataset
--------------------------

**Data Set Characteristics:**

    :Number of Instances: 20640

    :Number of Attributes: 8 numeric, predictive attributes and the target

    :Attribute Information:
        - MedInc        median income in block group
        - HouseAge      median house age in block group
        - AveRooms      average number of rooms per household
        - AveBedrms     average number of bedrooms per household
        - Population    block group population
        - AveOccup      average number of household members
        - Latitude      block group latitude
        - Longitude     block group longitude

    :Missing Attribute Values: None

This dataset was obtained from the StatLib repository.
https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html

The target variable is the median house value for California districts,
expressed in hundreds of thousands of dollars ($100,000).

This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).

A household is a group of people residing within a home. Since the average
number of rooms and bedrooms in this dataset are provided per household, these
columns may take surprisingly large values for block groups with few households
and many empty houses, such as vacation resorts.

It can be downloaded/loaded using the
:func:`sklearn.datasets.fetch_california_housing` function.

.. topic:: References

    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
      Statistics and Probability Letters, 33 (1997) 291-297
</code></pre>
<p>We can fit a decision tree regressor to the data.<br />
1. First, we load the data.<br />
1. Next, we split the data into train and test sets, keeping 20% for the test set.<br />
1. Then we fit a model to the training data, and store the model object in the variable model.<br />
1. Next we use the model to predict the test cases.<br />
1. Finally, we evaluate the results.  </p>
<pre><code class="language-python"># Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
</code></pre>
<pre><code class="language-python"># model = tree.DecisionTreeRegressor()
model = tree.DecisionTreeRegressor(max_depth=9)
model = model.fit(X_train, y_train)
model.predict(X_test)
</code></pre>
<pre><code>array([3.0852    , 0.96175   , 0.96001341, ..., 2.26529224, 2.15065625,
       2.02124038])
</code></pre>
<pre><code class="language-python">print(model.tree_.max_depth)
</code></pre>
<pre><code>9
</code></pre>
<pre><code class="language-python">y_pred  =  model.predict(X_test)

print('MSE = ', mean_squared_error(y_test,y_pred))
print('RMSE = ', np.sqrt(mean_squared_error(y_test,y_pred)))
print('MAE = ', mean_absolute_error(y_test,y_pred))

</code></pre>
<pre><code>MSE =  0.38773905612449
RMSE =  0.622686964794101
MAE =  0.4204310076086696
</code></pre>
<pre><code class="language-python"># Just checking to see if we have everything working right

print('Count of predictions:', len(y_pred))
print('Count of ground truth labels:', len(y_test))
</code></pre>
<pre><code>Count of predictions: 4128
Count of ground truth labels: 4128
</code></pre>
<pre><code class="language-python"># We plot the actual home prices vs the predictions in a scatterplot

plt.figure(figsize = (8,8))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.title('Actual vs Predicted Home Value in $000s \n Closer to red line (identity) \
          means more accurate prediction')
plt.plot( [0,5],[0,5], color='red' )
plt.xlabel(&quot;Actual&quot;)
plt.ylabel(&quot;Predicted&quot;)
</code></pre>
<pre><code>Text(0, 0.5, 'Predicted')
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_60_1.png" /></p>
<pre><code class="language-python"># Context for the RMSE.  What is the mean, min and max?

cali_df.medv.describe()
</code></pre>
<pre><code>count    20640.000000
mean         2.068558
std          1.153956
min          0.149990
25%          1.196000
50%          1.797000
75%          2.647250
max          5.000010
Name: medv, dtype: float64
</code></pre>
<pre><code class="language-python"># R-squared calculation
pd.DataFrame({'actual':y_test, 'predicted':y_pred}).corr()**2
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>actual</th>
      <th>predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>actual</th>
      <td>1.000000</td>
      <td>0.720348</td>
    </tr>
    <tr>
      <th>predicted</th>
      <td>0.720348</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="how-well-did-my-model-generalize">How well did my model generalize?</h3>
<p>Let us see how my model did on the training data</p>
<pre><code class="language-python"># R-squared
pd.DataFrame({'actual':y_train, 'predicted':model.predict(X_train)}).corr()**2
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>actual</th>
      <th>predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>actual</th>
      <td>1.000000</td>
      <td>0.794566</td>
    </tr>
    <tr>
      <th>predicted</th>
      <td>0.794566</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Calculate MSE, RMSE and MAE

y_pred  =  model.predict(X_train)

print('MSE = ', mean_squared_error(y_train,y_pred))
print('RMSE = ', np.sqrt(mean_squared_error(y_train,y_pred)))
print('MAE = ', mean_absolute_error(y_train,y_pred))
</code></pre>
<pre><code>MSE =  0.27139484003226105
RMSE =  0.5209556987232802
MAE =  0.3574534397649355
</code></pre>
<pre><code class="language-python"># Scatterplot for actual vs predicted on TRAINING data

plt.figure(figsize = (8,8))
plt.scatter(y_train, model.predict(X_train), alpha=0.5)
plt.title('Actual vs Predicted Home Value in $000s \n Closer to red line (identity) means more accurate prediction\n TRAINING DATA')
plt.plot( [0,5],[0,5], color='red' )
plt.xlabel(&quot;Actual&quot;)
plt.ylabel(&quot;Predicted&quot;)
</code></pre>
<pre><code>Text(0, 0.5, 'Predicted')
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_66_1.png" /></p>
<h3 id="addressing-overfitting-in-decision-trees">Addressing Overfitting in Decision Trees</h3>
<p>The simplest way to address overfitting in decision trees is to limit the depth of the trees using the <code>max_depth</code> parameter when fitting the model. The depth of a decision tree is the length of the longest path from a root to a leaf.</p>
<p>Find out the current value of the max tree depth in the example (<code>print(model.tree_.max_depth)</code>), and change the <code>max_depth</code> parameter to see if you can reduce the RMSE for the test set.</p>
<p>You can also change the minimum count of samples required to be present in a leaf node (<code>min_samples_leaf</code>), and the minimum number of observations required before a node is allowed to split (<code>min_samples_split</code>).</p>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<h2 id="random-forest">Random Forest</h2>
<p>A random forest fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.  </p>
<p>Random Forest almost always gives results superior to decision trees, and is therefore preferred over decision trees.  However, because the results provided by random forest are the result of averaging multiple trees, explainability can become an issue.  </p>
<p>Therefore decision trees may still be preferred over random forest in the interest of explainability.  </p>
<p>At this point, it is important to introduce two new concepts: bootstrapping, and bagging.  </p>
<p><strong>Bootstrapping</strong><br />
In bootstrapping, you treat the sample as if it were the population, and draw repeated samples of equal size from it.  The samples are drawn with replacement. Now think that for each of these new samples you calculate a population characteristic, say the median.  Because you potentially have a very large number of samples (theoretically infinite), you can get a distribution of the median of the population from our original single sample. </p>
<p>If we hadn’t done bootstrapping (ie resample from the sample with replacement), we would have only one point estimate for the median.  </p>
<p>Bootstrapping improves the estimation process and reduces variance.</p>
<p><strong>Bagging (Bootstrap + Aggregation)</strong><br />
Bagging is a type of ensemble learning. Ensemble learning is where we combine multiple models to produce a better prediction or classification.  </p>
<p>In bagging, we produce multiple different training sets (called bootstrap samples), by sampling with replacement from the original dataset.  Then, for each bootstrap sample, we build a model.<br />
The results in an ensemble of models, where each model votes with the equal weight.  Typically, the goal of this procedure is to reduce the variance of the model of interest (e.g. decision trees).  </p>
<p>The Random Forest algorithm is when the above technique is applied to decision trees.  </p>
<p><strong>Random Forests</strong><br />
Random forests are an example of ensemble learning, where multiple models are combined to produce a better prediction or classification.  </p>
<p>Random forests are collections of trees.  Predictions are equivalent to the average prediction of component trees.  </p>
<p>Multiple decision trees are created from the source data using a technique called bagging.  Multiple different training sets (called bootstrap samples) are created by sampling with replacement from the original dataset.  </p>
<p>Then, for each bootstrap sample, we build a model. The results in an ensemble of models, where each model votes with the equal weight. Typically, the goal of this procedure is to reduce the variance of the model of interest.  </p>
<p>When applied to decision trees, this becomes random forest.  </p>
<h3 id="random-forest-for-classification">Random Forest for Classification</h3>
<pre><code class="language-python"># load the data

college = pd.read_csv('collegePlace.csv')
college.shape
</code></pre>
<pre><code>(2966, 8)
</code></pre>
<pre><code class="language-python">college
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Stream</th>
      <th>Internships</th>
      <th>CGPA</th>
      <th>Hostel</th>
      <th>HistoryOfBacklogs</th>
      <th>PlacedOrNot</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22</td>
      <td>Male</td>
      <td>Electronics And Communication</td>
      <td>1</td>
      <td>8</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21</td>
      <td>Female</td>
      <td>Computer Science</td>
      <td>0</td>
      <td>7</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Female</td>
      <td>Information Technology</td>
      <td>1</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>21</td>
      <td>Male</td>
      <td>Information Technology</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>22</td>
      <td>Male</td>
      <td>Mechanical</td>
      <td>0</td>
      <td>8</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2961</th>
      <td>23</td>
      <td>Male</td>
      <td>Information Technology</td>
      <td>0</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2962</th>
      <td>23</td>
      <td>Male</td>
      <td>Mechanical</td>
      <td>1</td>
      <td>7</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2963</th>
      <td>22</td>
      <td>Male</td>
      <td>Information Technology</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2964</th>
      <td>22</td>
      <td>Male</td>
      <td>Computer Science</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2965</th>
      <td>23</td>
      <td>Male</td>
      <td>Civil</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>2966 rows × 8 columns</p>
</div>

<pre><code class="language-python"># divide the dataset into train and test sets, separating the features and target variable

X = college[['Age', 'Internships', 'CGPA', 'Hostel', 'HistoryOfBacklogs']].values
y = college['PlacedOrNot'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
</code></pre>
<pre><code class="language-python"># classify using random forest classifier

RandomForest = RandomForestClassifier()
model_rf = RandomForest.fit(X_train, y_train)
pred = model_rf.predict(X_test)
</code></pre>
<pre><code class="language-python">print(classification_report(y_true = y_test, y_pred = pred))
ConfusionMatrixDisplay.from_estimator(model_rf, X_test, y_test);
</code></pre>
<pre><code>              precision    recall  f1-score   support

           0       0.79      0.96      0.87       279
           1       0.96      0.77      0.86       315

    accuracy                           0.86       594
   macro avg       0.87      0.87      0.86       594
weighted avg       0.88      0.86      0.86       594
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_75_1.png" /></p>
<pre><code class="language-python"># get probabilities for each observation in the test set

model_rf.predict_proba(X_test)
</code></pre>
<pre><code>array([[0.92120988, 0.07879012],
       [0.79331614, 0.20668386],
       [0.        , 1.        ],
       ...,
       [0.01571429, 0.98428571],
       [0.        , 1.        ],
       [0.73565005, 0.26434995]])
</code></pre>
<pre><code class="language-python">y_test
</code></pre>
<pre><code>array([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
       0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
       1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
       1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
       0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
       1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
       1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
       0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
       1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
       1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
       1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
       1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1],
      dtype=int64)
</code></pre>
<pre><code class="language-python"># get probabilities for each observation in the test set

pred_prob = model_rf.predict_proba(X_test)[:,1]
</code></pre>
<pre><code class="language-python">model_rf.classes_
</code></pre>
<pre><code>array([0, 1], dtype=int64)
</code></pre>
<pre><code class="language-python"># Source for code below: https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python
fpr, tpr, thresholds = metrics.roc_curve(y_test, pred_prob)
roc_auc = metrics.auc(fpr, tpr)
plt.figure(figsize = (9,8))
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
for i, txt in enumerate(thresholds):
    if i in np.arange(1, len(thresholds), 10): # print every 10th point to prevent overplotting:
        plt.annotate(text = round(txt,3), xy = (fpr[i], tpr[i]),  
                     xytext=(-44, 0), textcoords='offset points',
                     arrowprops={'arrowstyle':&quot;simple&quot;}, color='green',fontsize=8)
plt.show()

threshold_dataframe = pd.DataFrame({'fpr':fpr, 'tpr': tpr, 'threshold':thresholds}).sort_values(by='threshold')
threshold_dataframe.head()
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_80_0.png" /></p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fpr</th>
      <th>tpr</th>
      <th>threshold</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>91</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>90</th>
      <td>0.921147</td>
      <td>1.000000</td>
      <td>0.001429</td>
    </tr>
    <tr>
      <th>89</th>
      <td>0.906810</td>
      <td>1.000000</td>
      <td>0.002843</td>
    </tr>
    <tr>
      <th>88</th>
      <td>0.903226</td>
      <td>1.000000</td>
      <td>0.003333</td>
    </tr>
    <tr>
      <th>87</th>
      <td>0.903226</td>
      <td>0.996825</td>
      <td>0.006190</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">
</code></pre>
<h3 id="random-forest-for-regression">Random Forest for Regression</h3>
<p>The Random Forest algorithm can also be used effectively for regression problems.  Let us try a larger dataset this time.  </p>
<p>We will try to predict diamond prices based on all the other attributes we know about the diamonds.  </p>
<p>However, our data contains a number of categorical variables.  We will need to convert these into numerical using one-hot encoding.  Let us do that next!</p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor
</code></pre>
<pre><code class="language-python">diamonds = sns.load_dataset(&quot;diamonds&quot;)
</code></pre>
<pre><code class="language-python">diamonds.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>carat</th>
      <th>cut</th>
      <th>color</th>
      <th>clarity</th>
      <th>depth</th>
      <th>table</th>
      <th>price</th>
      <th>x</th>
      <th>y</th>
      <th>z</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.23</td>
      <td>Ideal</td>
      <td>E</td>
      <td>SI2</td>
      <td>61.5</td>
      <td>55.0</td>
      <td>326</td>
      <td>3.95</td>
      <td>3.98</td>
      <td>2.43</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.21</td>
      <td>Premium</td>
      <td>E</td>
      <td>SI1</td>
      <td>59.8</td>
      <td>61.0</td>
      <td>326</td>
      <td>3.89</td>
      <td>3.84</td>
      <td>2.31</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.23</td>
      <td>Good</td>
      <td>E</td>
      <td>VS1</td>
      <td>56.9</td>
      <td>65.0</td>
      <td>327</td>
      <td>4.05</td>
      <td>4.07</td>
      <td>2.31</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.29</td>
      <td>Premium</td>
      <td>I</td>
      <td>VS2</td>
      <td>62.4</td>
      <td>58.0</td>
      <td>334</td>
      <td>4.20</td>
      <td>4.23</td>
      <td>2.63</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.31</td>
      <td>Good</td>
      <td>J</td>
      <td>SI2</td>
      <td>63.3</td>
      <td>58.0</td>
      <td>335</td>
      <td>4.34</td>
      <td>4.35</td>
      <td>2.75</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">diamonds = pd.get_dummies(diamonds)
</code></pre>
<pre><code class="language-python">diamonds.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>carat</th>
      <th>depth</th>
      <th>table</th>
      <th>price</th>
      <th>x</th>
      <th>y</th>
      <th>z</th>
      <th>cut_Ideal</th>
      <th>cut_Premium</th>
      <th>cut_Very Good</th>
      <th>...</th>
      <th>color_I</th>
      <th>color_J</th>
      <th>clarity_IF</th>
      <th>clarity_VVS1</th>
      <th>clarity_VVS2</th>
      <th>clarity_VS1</th>
      <th>clarity_VS2</th>
      <th>clarity_SI1</th>
      <th>clarity_SI2</th>
      <th>clarity_I1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.23</td>
      <td>61.5</td>
      <td>55.0</td>
      <td>326</td>
      <td>3.95</td>
      <td>3.98</td>
      <td>2.43</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.21</td>
      <td>59.8</td>
      <td>61.0</td>
      <td>326</td>
      <td>3.89</td>
      <td>3.84</td>
      <td>2.31</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.23</td>
      <td>56.9</td>
      <td>65.0</td>
      <td>327</td>
      <td>4.05</td>
      <td>4.07</td>
      <td>2.31</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.29</td>
      <td>62.4</td>
      <td>58.0</td>
      <td>334</td>
      <td>4.20</td>
      <td>4.23</td>
      <td>2.63</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>...</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.31</td>
      <td>63.3</td>
      <td>58.0</td>
      <td>335</td>
      <td>4.34</td>
      <td>4.35</td>
      <td>2.75</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 27 columns</p>
</div>

<pre><code class="language-python"># Define X and y as arrays. y is the price column, X is everything else

X = diamonds.loc[:, diamonds.columns != 'price'].values
y = diamonds.price.values
</code></pre>
<pre><code class="language-python"># Train test split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
</code></pre>
<pre><code class="language-python"># Fit model

model_rf_regr = RandomForestRegressor(max_depth=2, random_state=0)
model_rf_regr.fit(X_train, y_train)
model_rf_regr.predict(X_test)
</code></pre>
<pre><code>array([1054.29089419, 1054.29089419, 1054.29089419, ..., 6145.62603236,
       1054.29089419, 1054.29089419])
</code></pre>
<pre><code class="language-python"># Evaluate model

y_pred  =  model_rf_regr.predict(X_test)
from sklearn.metrics import mean_absolute_error, mean_squared_error
print('MSE = ', mean_squared_error(y_test,y_pred))
print('RMSE = ', np.sqrt(mean_squared_error(y_test,y_pred)))
print('MAE = ', mean_absolute_error(y_test,y_pred))

</code></pre>
<pre><code>MSE =  2757832.1354701095
RMSE =  1660.6721938631083
MAE =  1036.4110791707412
</code></pre>
<pre><code class="language-python"># Evaluate residuals

plt.figure(figsize = (8,8))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.title('Actual vs Predicted Diamond Value\n Closer to red line (identity) means more accurate prediction')
plt.plot( [0,19000],[0,19000], color='red' )
plt.xlabel(&quot;Actual&quot;)
plt.ylabel(&quot;Predicted&quot;)
</code></pre>
<pre><code>Text(0, 0.5, 'Predicted')
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_92_1.png" /></p>
<pre><code class="language-python">diamonds.price.describe()
</code></pre>
<pre><code>count    53940.000000
mean      3932.799722
std       3989.439738
min        326.000000
25%        950.000000
50%       2401.000000
75%       5324.250000
max      18823.000000
Name: price, dtype: float64
</code></pre>
<pre><code class="language-python"># R-squared calculation
pd.DataFrame({'actual':y_test, 'predicted':y_pred}).corr()**2
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>actual</th>
      <th>predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>actual</th>
      <td>1.000000</td>
      <td>0.827564</td>
    </tr>
    <tr>
      <th>predicted</th>
      <td>0.827564</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">importance = model_rf_regr.feature_importances_
feature_names = diamonds.loc[:, diamonds.columns != 'price'].columns
pd.DataFrame({'Feature':feature_names, 'Importance':importance}).sort_values(by='Importance', ascending=False)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Feature</th>
      <th>Importance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>carat</td>
      <td>0.668208</td>
    </tr>
    <tr>
      <th>4</th>
      <td>y</td>
      <td>0.331792</td>
    </tr>
    <tr>
      <th>14</th>
      <td>color_G</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>24</th>
      <td>clarity_SI2</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>23</th>
      <td>clarity_SI1</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>22</th>
      <td>clarity_VS2</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>21</th>
      <td>clarity_VS1</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>20</th>
      <td>clarity_VVS2</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>19</th>
      <td>clarity_VVS1</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>18</th>
      <td>clarity_IF</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>17</th>
      <td>color_J</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>16</th>
      <td>color_I</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>15</th>
      <td>color_H</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>13</th>
      <td>color_F</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>depth</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>12</th>
      <td>color_E</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>11</th>
      <td>color_D</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>10</th>
      <td>cut_Fair</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>9</th>
      <td>cut_Good</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>8</th>
      <td>cut_Very Good</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>cut_Premium</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>cut_Ideal</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>z</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>x</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>table</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25</th>
      <td>clarity_I1</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">
</code></pre>
<h3 id="random-forest-regression-another-example">Random Forest Regression - Another Example</h3>
<p>Let us look at our California Housing Dataset that we examined before to predict home prices.  </p>
<pre><code class="language-python"># Load the data

from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()

X = housing['data']
y = housing['target']
features = housing['feature_names']
DESCR = housing['DESCR']

cali_df = pd.DataFrame(X, columns = features)
cali_df.insert(0,'medv', y)
</code></pre>
<pre><code class="language-python">cali_df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>medv</th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4.526</td>
      <td>8.3252</td>
      <td>41.0</td>
      <td>6.984127</td>
      <td>1.023810</td>
      <td>322.0</td>
      <td>2.555556</td>
      <td>37.88</td>
      <td>-122.23</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.585</td>
      <td>8.3014</td>
      <td>21.0</td>
      <td>6.238137</td>
      <td>0.971880</td>
      <td>2401.0</td>
      <td>2.109842</td>
      <td>37.86</td>
      <td>-122.22</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.521</td>
      <td>7.2574</td>
      <td>52.0</td>
      <td>8.288136</td>
      <td>1.073446</td>
      <td>496.0</td>
      <td>2.802260</td>
      <td>37.85</td>
      <td>-122.24</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.413</td>
      <td>5.6431</td>
      <td>52.0</td>
      <td>5.817352</td>
      <td>1.073059</td>
      <td>558.0</td>
      <td>2.547945</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.422</td>
      <td>3.8462</td>
      <td>52.0</td>
      <td>6.281853</td>
      <td>1.081081</td>
      <td>565.0</td>
      <td>2.181467</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>20635</th>
      <td>0.781</td>
      <td>1.5603</td>
      <td>25.0</td>
      <td>5.045455</td>
      <td>1.133333</td>
      <td>845.0</td>
      <td>2.560606</td>
      <td>39.48</td>
      <td>-121.09</td>
    </tr>
    <tr>
      <th>20636</th>
      <td>0.771</td>
      <td>2.5568</td>
      <td>18.0</td>
      <td>6.114035</td>
      <td>1.315789</td>
      <td>356.0</td>
      <td>3.122807</td>
      <td>39.49</td>
      <td>-121.21</td>
    </tr>
    <tr>
      <th>20637</th>
      <td>0.923</td>
      <td>1.7000</td>
      <td>17.0</td>
      <td>5.205543</td>
      <td>1.120092</td>
      <td>1007.0</td>
      <td>2.325635</td>
      <td>39.43</td>
      <td>-121.22</td>
    </tr>
    <tr>
      <th>20638</th>
      <td>0.847</td>
      <td>1.8672</td>
      <td>18.0</td>
      <td>5.329513</td>
      <td>1.171920</td>
      <td>741.0</td>
      <td>2.123209</td>
      <td>39.43</td>
      <td>-121.32</td>
    </tr>
    <tr>
      <th>20639</th>
      <td>0.894</td>
      <td>2.3886</td>
      <td>16.0</td>
      <td>5.254717</td>
      <td>1.162264</td>
      <td>1387.0</td>
      <td>2.616981</td>
      <td>39.37</td>
      <td>-121.24</td>
    </tr>
  </tbody>
</table>
<p>20640 rows × 9 columns</p>
</div>

<pre><code class="language-python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
</code></pre>
<pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(max_depth=2, random_state=0)
model.fit(X_train, y_train)
</code></pre>
<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomForestRegressor(max_depth=2, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(max_depth=2, random_state=0)</pre></div></div></div></div></div>

<pre><code class="language-python">y_pred  =  model.predict(X_test)
from sklearn.metrics import mean_absolute_error, mean_squared_error
print('MSE = ', mean_squared_error(y_test,y_pred))
print('RMSE = ', np.sqrt(mean_squared_error(y_test,y_pred)))
print('MAE = ', mean_absolute_error(y_test,y_pred))

</code></pre>
<pre><code>MSE =  0.7109737317347218
RMSE =  0.8431925828271509
MAE =  0.6387472402358885
</code></pre>
<pre><code class="language-python">print(cali_df.medv.describe())
cali_df.medv.plot.hist(bins=20)
</code></pre>
<pre><code>count    20640.000000
mean         2.068558
std          1.153956
min          0.149990
25%          1.196000
50%          1.797000
75%          2.647250
max          5.000010
Name: medv, dtype: float64





&lt;Axes: ylabel='Frequency'&gt;
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_103_2.png" /></p>
<pre><code class="language-python">plt.figure(figsize = (8,8))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.title('Actual vs Predicted Home Value in $000s \n Closer to red line (identity) means more accurate prediction')
plt.plot( [0,5],[0,5], color='red' )
plt.xlabel(&quot;Actual&quot;)
plt.ylabel(&quot;Predicted&quot;)
</code></pre>
<pre><code>Text(0, 0.5, 'Predicted')
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_104_1.png" /></p>
<pre><code class="language-python">
</code></pre>
<h2 id="xgboost">XGBoost</h2>
<p>Like Random Forest, XGBoost is a tree based algorithm.  In Random Forest, multiple trees are built in parallel, and averaged.  In XGBoost, trees are built sequentially, with each tree correcting the errors of the previous one.  </p>
<p>Trees are built in sequence, with each next tree in the sequence targeting the errors of the previous one.  The trees are then added, with a multiplicative constant ‘learning rate’ between 0 and 1 applied to each tree.  </p>
<p>XGBoost has by far exceeded the performance of other algorithms, and is one of the most used algorithms on Kaggle. In many cases, it outperforms Neural Nets.  </p>
<p>Extensive documentation is available at https://xgboost.readthedocs.io/en/latest  </p>
<p><strong>Example</strong><br />
Let us consider our college placement dataset, and check if we are able to predict the ‘PlacedOrNot’ variable correctly.  </p>
<p>We will convert the categorical variables (stream of study, gender, etc) into numerical using one-hot encoding.  </p>
<p>We will keep 20% of the data as the test set, and fit a model using the XGBoost algorithm.  </p>
<h3 id="xgboost-classification">XGBoost - Classification</h3>
<pre><code class="language-python"># load the data
college = pd.read_csv('collegePlace.csv')
college = pd.get_dummies(college)
</code></pre>
<pre><code class="language-python">college
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Internships</th>
      <th>CGPA</th>
      <th>Hostel</th>
      <th>HistoryOfBacklogs</th>
      <th>PlacedOrNot</th>
      <th>Gender_Female</th>
      <th>Gender_Male</th>
      <th>Stream_Civil</th>
      <th>Stream_Computer Science</th>
      <th>Stream_Electrical</th>
      <th>Stream_Electronics And Communication</th>
      <th>Stream_Information Technology</th>
      <th>Stream_Mechanical</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22</td>
      <td>1</td>
      <td>8</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21</td>
      <td>0</td>
      <td>7</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>1</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>21</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>22</td>
      <td>0</td>
      <td>8</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2961</th>
      <td>23</td>
      <td>0</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2962</th>
      <td>23</td>
      <td>1</td>
      <td>7</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2963</th>
      <td>22</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2964</th>
      <td>22</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2965</th>
      <td>23</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
<p>2966 rows × 14 columns</p>
</div>

<pre><code class="language-python"># Test train split
X = college.loc[:, college.columns != 'PlacedOrNot']
y = college['PlacedOrNot']
feature_names = college.loc[:, college.columns != 'PlacedOrNot'].columns

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
</code></pre>
<pre><code class="language-python"># Fit the model
from xgboost import XGBClassifier

model_xgb = XGBClassifier(use_label_encoder=False, objective= 'binary:logistic')
model_xgb.fit(X_train, y_train)
</code></pre>
<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=None,
              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" checked><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">XGBClassifier</label><div class="sk-toggleable__content"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=None,
              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>

<pre><code class="language-python"># Perform predictions, and store the results in a variable called 'pred'
pred = model_xgb.predict(X_test)
</code></pre>
<pre><code class="language-python"># Check the classification report and the confusion matrix
print(classification_report(y_true = y_test, y_pred = pred))
ConfusionMatrixDisplay.from_estimator(model_xgb, X_test, y_test);
</code></pre>
<pre><code>              precision    recall  f1-score   support

           0       0.82      0.92      0.87       269
           1       0.93      0.84      0.88       325

    accuracy                           0.88       594
   macro avg       0.88      0.88      0.88       594
weighted avg       0.88      0.88      0.88       594
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_112_1.png" /></p>
<p><strong>Class Probabilities</strong><br />
We can obtain class probabilities from an XGBoost model.  These can help us use different thresholds for cutoff and decide on the error rates we are comfortable with. </p>
<pre><code class="language-python">model_xgb.classes_
</code></pre>
<pre><code>array([0, 1])
</code></pre>
<pre><code class="language-python">y_test
</code></pre>
<pre><code>1435    0
1899    1
1475    1
1978    1
100     1
       ..
1614    1
1717    0
556     0
1773    0
1294    0
Name: PlacedOrNot, Length: 594, dtype: int64
</code></pre>
<pre><code class="language-python">pred_prob = model_xgb.predict_proba(X_test).round(3)
pred_prob
</code></pre>
<pre><code>array([[0.314, 0.686],
       [0.004, 0.996],
       [0.002, 0.998],
       ...,
       [0.984, 0.016],
       [0.758, 0.242],
       [0.773, 0.227]], dtype=float32)
</code></pre>
<pre><code class="language-python"># Source for code below: https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python
fpr, tpr, thresholds = metrics.roc_curve(y_test, pred_prob[:,1])
roc_auc = metrics.auc(fpr, tpr)
plt.figure(figsize = (9,8))
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
for i, txt in enumerate(thresholds):
    if i in np.arange(1, len(thresholds), 10): # print every 10th point to prevent overplotting:
        plt.annotate(text = round(txt,3), xy = (fpr[i], tpr[i]),  
                     xytext=(-44, 0), textcoords='offset points',
                     arrowprops={'arrowstyle':&quot;simple&quot;}, color='green',fontsize=8)
plt.show()

threshold_dataframe = pd.DataFrame({'fpr':fpr, 'tpr': tpr, 'threshold':thresholds}).sort_values(by='threshold')
threshold_dataframe.head()
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_117_0.png" /></p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fpr</th>
      <th>tpr</th>
      <th>threshold</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>150</th>
      <td>1.000000</td>
      <td>1.0</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>149</th>
      <td>0.985130</td>
      <td>1.0</td>
      <td>0.002</td>
    </tr>
    <tr>
      <th>148</th>
      <td>0.981413</td>
      <td>1.0</td>
      <td>0.003</td>
    </tr>
    <tr>
      <th>147</th>
      <td>0.947955</td>
      <td>1.0</td>
      <td>0.006</td>
    </tr>
    <tr>
      <th>146</th>
      <td>0.929368</td>
      <td>1.0</td>
      <td>0.007</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="change-results-by-varying-threshold">Change results by varying threshold</h3>
<pre><code class="language-python"># Look at how the probabilities look for the first 10 observations
# The first column is class 0, and the second column is class 1

model_xgb.predict_proba(X_test)[:10]
</code></pre>
<pre><code>array([[3.143e-01, 6.857e-01],
       [4.300e-03, 9.957e-01],
       [2.000e-03, 9.980e-01],
       [8.000e-04, 9.992e-01],
       [8.798e-01, 1.202e-01],
       [5.881e-01, 4.119e-01],
       [8.866e-01, 1.134e-01],
       [3.000e-04, 9.997e-01],
       [9.956e-01, 4.400e-03],
       [1.463e-01, 8.537e-01]], dtype=float32)
</code></pre>
<pre><code class="language-python"># Let us round the above as to make it a bit easier to read...
# same thing as prior cell, just presentation
np.round(model_xgb.predict_proba(X_test)[:10], 3)
</code></pre>
<pre><code>array([[0.314, 0.686],
       [0.004, 0.996],
       [0.002, 0.998],
       [0.001, 0.999],
       [0.88 , 0.12 ],
       [0.588, 0.412],
       [0.887, 0.113],
       [0.   , 1.   ],
       [0.996, 0.004],
       [0.146, 0.854]], dtype=float32)
</code></pre>
<pre><code class="language-python"># Now see what the actual prediction is for the first 10 items
# You can see the model has picked the most probable item
# for identifying which category it should be assigned.
#
# We can vary the threshold to change the predictions.
# We do this next

model_xgb.predict(X_test)[:10]
</code></pre>
<pre><code>array([1, 1, 1, 1, 0, 0, 0, 1, 0, 1])
</code></pre>
<pre><code class="language-python"># Set threshold for identifying class 1
threshold = 0.9

# Create predictions.  Note that predictions give us probabilities, not classes!
pred_prob = model_xgb.predict_proba(X_test)

# We drop the probabilities for class 0, and keep just the second column
pred_prob = pred_prob[:,1]

# Convert probabilities to 1s and 0s based on threshold
pred = (pred_prob&gt;threshold).astype(int)

# confusion matrix
cm = confusion_matrix(y_test, pred)
print (&quot;Confusion Matrix : \n&quot;, cm)
ConfusionMatrixDisplay(confusion_matrix=cm).plot();

# accuracy score of the model
print('Test accuracy = ', accuracy_score(y_test, pred))
print(classification_report(y_true = y_test, y_pred = pred,))
</code></pre>
<pre><code>Confusion Matrix : 
 [[272   1]
 [ 59 262]]
Test accuracy =  0.898989898989899
              precision    recall  f1-score   support

           0       0.82      1.00      0.90       273
           1       1.00      0.82      0.90       321

    accuracy                           0.90       594
   macro avg       0.91      0.91      0.90       594
weighted avg       0.92      0.90      0.90       594
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_122_1.png" /></p>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<p><strong>Feature Importance</strong><br />
Using the method feature_importances_, we can get a sense for what the model considers more important than others.  However, feature importance identified in this way should be reviewed in the context of domain knowledge.  Refer article at https://explained.ai/rf-importance/  </p>
<pre><code class="language-python"># Check feature importance
# This can be misleading though - check out https://explained.ai/rf-importance/
importance = model_xgb.feature_importances_
pd.DataFrame({'Feature':feature_names, 'Importance':importance}).sort_values(by='Importance', ascending=False)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Feature</th>
      <th>Importance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>CGPA</td>
      <td>0.525521</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Stream_Electrical</td>
      <td>0.086142</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Internships</td>
      <td>0.073115</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Stream_Electronics And Communication</td>
      <td>0.059893</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Stream_Civil</td>
      <td>0.049865</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Age</td>
      <td>0.047633</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Stream_Mechanical</td>
      <td>0.042866</td>
    </tr>
    <tr>
      <th>4</th>
      <td>HistoryOfBacklogs</td>
      <td>0.041009</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Stream_Information Technology</td>
      <td>0.019961</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Hostel</td>
      <td>0.018885</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Gender_Female</td>
      <td>0.017859</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Stream_Computer Science</td>
      <td>0.017251</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Gender_Male</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">from xgboost import plot_importance

# plot feature importance
plot_importance(model_xgb)

plt.show()
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_129_0.png" /></p>
<pre><code class="language-python">
</code></pre>
<h3 id="xgboost-for-regression">XGBoost for Regression</h3>
<p>Let us try to predict diamond prices again, this time using XGBoost.  As we can see below, RMSE is half of what we had with Random Forest.</p>
<pre><code class="language-python"># Load data

diamonds = sns.load_dataset(&quot;diamonds&quot;)
</code></pre>
<pre><code class="language-python">diamonds.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>carat</th>
      <th>cut</th>
      <th>color</th>
      <th>clarity</th>
      <th>depth</th>
      <th>table</th>
      <th>price</th>
      <th>x</th>
      <th>y</th>
      <th>z</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.23</td>
      <td>Ideal</td>
      <td>E</td>
      <td>SI2</td>
      <td>61.5</td>
      <td>55.0</td>
      <td>326</td>
      <td>3.95</td>
      <td>3.98</td>
      <td>2.43</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.21</td>
      <td>Premium</td>
      <td>E</td>
      <td>SI1</td>
      <td>59.8</td>
      <td>61.0</td>
      <td>326</td>
      <td>3.89</td>
      <td>3.84</td>
      <td>2.31</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.23</td>
      <td>Good</td>
      <td>E</td>
      <td>VS1</td>
      <td>56.9</td>
      <td>65.0</td>
      <td>327</td>
      <td>4.05</td>
      <td>4.07</td>
      <td>2.31</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.29</td>
      <td>Premium</td>
      <td>I</td>
      <td>VS2</td>
      <td>62.4</td>
      <td>58.0</td>
      <td>334</td>
      <td>4.20</td>
      <td>4.23</td>
      <td>2.63</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.31</td>
      <td>Good</td>
      <td>J</td>
      <td>SI2</td>
      <td>63.3</td>
      <td>58.0</td>
      <td>335</td>
      <td>4.34</td>
      <td>4.35</td>
      <td>2.75</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Get dummy variables

diamonds = pd.get_dummies(diamonds)
</code></pre>
<pre><code class="language-python">diamonds.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>carat</th>
      <th>depth</th>
      <th>table</th>
      <th>price</th>
      <th>x</th>
      <th>y</th>
      <th>z</th>
      <th>cut_Ideal</th>
      <th>cut_Premium</th>
      <th>cut_Very Good</th>
      <th>cut_Good</th>
      <th>cut_Fair</th>
      <th>color_D</th>
      <th>color_E</th>
      <th>color_F</th>
      <th>color_G</th>
      <th>color_H</th>
      <th>color_I</th>
      <th>color_J</th>
      <th>clarity_IF</th>
      <th>clarity_VVS1</th>
      <th>clarity_VVS2</th>
      <th>clarity_VS1</th>
      <th>clarity_VS2</th>
      <th>clarity_SI1</th>
      <th>clarity_SI2</th>
      <th>clarity_I1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.23</td>
      <td>61.5</td>
      <td>55.0</td>
      <td>326</td>
      <td>3.95</td>
      <td>3.98</td>
      <td>2.43</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.21</td>
      <td>59.8</td>
      <td>61.0</td>
      <td>326</td>
      <td>3.89</td>
      <td>3.84</td>
      <td>2.31</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.23</td>
      <td>56.9</td>
      <td>65.0</td>
      <td>327</td>
      <td>4.05</td>
      <td>4.07</td>
      <td>2.31</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.29</td>
      <td>62.4</td>
      <td>58.0</td>
      <td>334</td>
      <td>4.20</td>
      <td>4.23</td>
      <td>2.63</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.31</td>
      <td>63.3</td>
      <td>58.0</td>
      <td>335</td>
      <td>4.34</td>
      <td>4.35</td>
      <td>2.75</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Define X and y as arrays. y is the price column, X is everything else

X = diamonds.loc[:, diamonds.columns != 'price'].values
y = diamonds.price.values
</code></pre>
<pre><code class="language-python"># Define X and y as arrays. y is the price column, X is everything else

X = diamonds.loc[:, diamonds.columns != 'price']
y = diamonds.price
</code></pre>
<pre><code class="language-python"># Train test split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
</code></pre>
<pre><code class="language-python"># Fit model

from xgboost import XGBRegressor

model_xgb_regr = XGBRegressor()
model_xgb_regr.fit(X_train, y_train)
model_xgb_regr.predict(X_test)
</code></pre>
<pre><code>array([ 7206.3213,  3110.482 ,  5646.054 , ..., 13976.481 ,  5555.7554,
       11428.439 ], dtype=float32)
</code></pre>
<pre><code class="language-python"># Evaluate model

y_pred  =  model_xgb_regr.predict(X_test)
from sklearn.metrics import mean_absolute_error, mean_squared_error
print('MSE = ', mean_squared_error(y_test,y_pred))
print('RMSE = ', np.sqrt(mean_squared_error(y_test,y_pred)))
print('MAE = ', mean_absolute_error(y_test,y_pred))

</code></pre>
<pre><code>MSE =  280824.1563066477
RMSE =  529.9284445155287
MAE =  276.8015830181774
</code></pre>
<pre><code class="language-python"># Evaluate residuals

plt.figure(figsize = (8,8))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.title('Actual vs Predicted Diamond Value\n Closer to red line (identity) means more accurate prediction')
plt.plot( [0,19000],[0,19000], color='red' )
plt.xlabel(&quot;Actual&quot;)
plt.ylabel(&quot;Predicted&quot;);
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_141_0.png" /></p>
<pre><code class="language-python"># R-squared calculation
pd.DataFrame({'actual':y_test, 'predicted':y_pred}).corr()**2
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>actual</th>
      <th>predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>actual</th>
      <td>1.000000</td>
      <td>0.982202</td>
    </tr>
    <tr>
      <th>predicted</th>
      <td>0.982202</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">from xgboost import plot_importance

# plot feature importance
plot_importance(model_xgb_regr);
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_143_0.png" /></p>
<p>As we can see, XGBoost has vastly improved the prediction results.  R-squared is 0.98, and the residual plot looks much better than with Random Forest.</p>
<pre><code class="language-python">diamonds.price.describe()
</code></pre>
<pre><code>count    53940.000000
mean      3932.799722
std       3989.439738
min        326.000000
25%        950.000000
50%       2401.000000
75%       5324.250000
max      18823.000000
Name: price, dtype: float64
</code></pre>
<pre><code class="language-python">
</code></pre>
<h2 id="linear-methods">Linear Methods</h2>
<p>Linear methods are different from tree based methods that we looked at earlier.  They approach the problem from the perspective of plotting the points and drawing a line (or a plane) that separates the categories.  </p>
<p>Let us consider a toy dataset that we create at random.  The dataset has two features (Feature_1 and Feature_2), that help us distinguish between two classes - 0 and 1.</p>
<p>The data is graphed in the scatterplot below.  The point to note here is that it is pretty easy to distinguish between the two classes by drawing a straight line between the two classes.  The question though is which line is the best possible line for classification, given an infinite number of such lines can be drawn?</p>
<pre><code class="language-python"># Import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_blobs, make_classification


# Generate random data
X, y, centers = make_blobs(n_samples=30, centers=2,
                           n_features=2, random_state=14,
                           return_centers=True,
                           center_box=(0,20), cluster_std = 5)

# Round to one place of decimal

X = np.round_(X,1)
y = np.round_(y,1)

# Create a dataframe with the features and the y variable

df = pd.DataFrame(dict(Feature_1=X[:,0], Feature_2=X[:,1], Label_y=y))
df = round(df,ndigits=2)


# Plot the data

plt.figure(figsize=(9,9))
sns.scatterplot(data = df, x = 'Feature_1', y = 'Feature_2', style = 'Label_y', hue = 'Label_y', 
                alpha = .8, palette=&quot;deep&quot;,edgecolor = 'None')

# Plot possible lines to discriminate between classes

plt.plot([0,30],[2.5,9], 'k--')
plt.plot([0,30], [0,12], 'k--')
plt.plot([0,20], [-10,20], 'k--');
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_148_0.png" /></p>
<p>If we were to create a decision tree, the problem is solved as the decision tree draws two straight line boundaries - first at Feature_2 &gt; 4.55, and the second at Feature_1 &gt; 20.  While this works for the current data, we can obviously see that a more robust and simpler solution would be to draw a straight line between the data that is at an angle separating the two classes.</p>
<pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn import tree
# iris = load_iris()

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, y)

import graphviz 
dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
# graph.render(&quot;iris&quot;) 

dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names=['Feature_1', 'Feature_2'],  
                         class_names=['Class_0', 'Class_1'],  
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = graphviz.Source(dot_data)  
graph 
</code></pre>
<p><img alt="svg" src="../09_Machine_Learning_files/09_Machine_Learning_150_0.svg" /></p>
<pre><code class="language-python"># Plot the data

plt.figure(figsize=(9,9))
sns.scatterplot(data = df, x = 'Feature_1', y = 'Feature_2', style = 'Label_y', hue = 'Label_y', 
                alpha = .8, palette=&quot;deep&quot;,edgecolor = 'None')

# Plot possible lines to discriminate between classes

plt.plot([0,20],[4.55,4.55], color='green')
plt.plot([20,20], [-12,22], color = 'purple');

</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_151_0.png" /></p>
<p>However, we can see that a single linear split provides better results.  </p>
<p>This is an example of a Linear Classifier.  The decision boundary is essentially a line represented as the weighted sum of the two axes.  This is called a linear discriminant because it discriminates between the two classes using a linear combination of the independent attributes.  </p>
<p>A general linear model would look as follows:  </p>
<p>
<script type="math/tex">f(x) = w_0 + w_1 x_1 + w_2 x_2+ ...</script>
</p>
<p>For our example, the linear classifier line is defined by the following example:  </p>
<p>
<script type="math/tex">[\mbox{Constant Intercept}] + [\mbox{Coefficient 1} * \mbox{Feature2}] + [\mbox{Coefficient 2} * \mbox{Feature2}] = 0</script>
</p>
<p>The coefficients, or weights, are often loosely interpreted as the importance of the features, assuming all feature values have been normalized.  </p>
<p><strong>The question is: How do we identify the correct line as many different lines are possible.</strong>  </p>
<p>There are many methods to determine the line that serves as our linear discriminant.  Each method differs in the ‘objective function’ that is optimized to arrive at the solution.  </p>
<p>Two of the common methods used are:<br />
 - Linear Discriminant Analysis, and 
 - Support Vector Machines  </p>
<pre><code class="language-python"># Fit linear model
from sklearn.svm import SVC
model_svc = SVC(kernel=&quot;linear&quot;)
model_svc.fit(X, y)
</code></pre>
<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-8" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" checked><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">SVC</label><div class="sk-toggleable__content"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>

<pre><code class="language-python"># Plot the data, and line dividing the classification

plt.figure(figsize=(9,9))
sns.scatterplot(data = df, x = 'Feature_1', y = 'Feature_2', style = 'Label_y', hue = 'Label_y', 
                alpha = .8, palette=&quot;deep&quot;,edgecolor = 'None');



# Plot the equation of the linear discriminant

w = model_svc.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(df.Feature_1.min()-1, df.Feature_1.max()+1)
yy = a * xx - (model_svc.intercept_[0]) / w[1]
plt.plot(xx, yy, 'k-')

# Identify the support vectors, ie the points that decide the decision boundary

plt.scatter(
    model_svc.support_vectors_[:, 0],
    model_svc.support_vectors_[:, 1],
    s=80,
    facecolors=&quot;none&quot;,
    zorder=10,
    edgecolors=&quot;k&quot;,
   )

# Plot the margin lines

margin = 1 / np.sqrt(np.sum(model_svc.coef_**2))
yy_down = yy - np.sqrt(1 + a**2) * margin
yy_up = yy + np.sqrt(1 + a**2) * margin

plt.plot(xx, yy_down, &quot;k--&quot;)
plt.plot(xx, yy_up, &quot;k--&quot;);
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_154_0.png" /></p>
<pre><code class="language-python"># Another way to plot the decision boundary for SVM models
# Source: https://stackoverflow.com/questions/51297423/plot-scikit-learn-sklearn-svm-decision-boundary-surface

from sklearn.svm import SVC
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_decision_regions

svm_graph = SVC(kernel='linear')
svm_graph.fit(X, y)
plot_decision_regions(X, y, clf=svm_graph, legend=2)
plt.show()
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_155_0.png" /></p>
<h3 id="linear-discriminant-analysis">Linear Discriminant Analysis</h3>
<p>LDA assumes a normal distribution for the data points for the different categories, and attempts to create a 1D projection in a way that separates classes well.  </p>
<p><img alt="image.png" src="../09_Machine_Learning_files/92236e69-1def-44bf-b274-fdf87f285e24.png" />  </p>
<p>Fortunately, there are libraries available that do all the tough math for us.  </p>
<p>LDA expects predictor variables to be continuous due to its distributional assumption of independent variables being multivariate normal.  This limits its use in situations where the predictor variables are categorical.  </p>
<p>You do not need to standardize the feature set prior to using linear discriminant analysis.
You should rule out logistic regression as a better alternative before using linear discriminant analysis.</p>
<p><strong>LDA in Action</strong>  </p>
<p>We revisit the collegePlace.csv data.</p>
<p>About the data:<br />
A University Announced Its On-Campus Placement Records For The Engineering Course. The Data Is From The Years 2013 And 2014.  </p>
<p>Data Fields:<br />
 - Age: Age At The Time Of Final Year
 - Gender: Gender Of Candidate
 - Stream: Engineering Stream That The Candidate Belongs To
 - Internships: Number Of Internships Undertaken During The Course Of Studies, Not Necessarily Related To College Studies Or Stream
 - CGPA: CGPA Till 6th Semester
 - Hostel: Whether Student Lives In College Accomodation
 - HistoryOfBacklogs: Whether Student Ever Had Any Backlogs In Any Subjects
 - PlacedOrNot: Target Variable</p>
<pre><code class="language-python"># load the data

college = pd.read_csv('collegePlace.csv')
</code></pre>
<pre><code class="language-python">college
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Gender</th>
      <th>Stream</th>
      <th>Internships</th>
      <th>CGPA</th>
      <th>Hostel</th>
      <th>HistoryOfBacklogs</th>
      <th>PlacedOrNot</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22</td>
      <td>Male</td>
      <td>Electronics And Communication</td>
      <td>1</td>
      <td>8</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21</td>
      <td>Female</td>
      <td>Computer Science</td>
      <td>0</td>
      <td>7</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Female</td>
      <td>Information Technology</td>
      <td>1</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>21</td>
      <td>Male</td>
      <td>Information Technology</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>22</td>
      <td>Male</td>
      <td>Mechanical</td>
      <td>0</td>
      <td>8</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2961</th>
      <td>23</td>
      <td>Male</td>
      <td>Information Technology</td>
      <td>0</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2962</th>
      <td>23</td>
      <td>Male</td>
      <td>Mechanical</td>
      <td>1</td>
      <td>7</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2963</th>
      <td>22</td>
      <td>Male</td>
      <td>Information Technology</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2964</th>
      <td>22</td>
      <td>Male</td>
      <td>Computer Science</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2965</th>
      <td>23</td>
      <td>Male</td>
      <td>Civil</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>2966 rows × 8 columns</p>
</div>

<pre><code class="language-python">college.columns
</code></pre>
<pre><code>Index(['Age', 'Gender', 'Stream', 'Internships', 'CGPA', 'Hostel', 'HistoryOfBacklogs', 'PlacedOrNot'], dtype='object')
</code></pre>
<pre><code class="language-python"># divide the dataset into train and test sets, separating the features and target variable
X = college[['Age', 'Internships', 'CGPA', 'Hostel', 'HistoryOfBacklogs']].values
y = college['PlacedOrNot'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
</code></pre>
<pre><code class="language-python"># apply Linear Discriminant Analysis
LDA = LinearDiscriminantAnalysis()
model_lda = LDA.fit(X = X_train, y = y_train)
pred = model_lda.predict(X_test)
</code></pre>
<pre><code class="language-python">college.PlacedOrNot.value_counts()
</code></pre>
<pre><code>PlacedOrNot
1    1639
0    1327
Name: count, dtype: int64
</code></pre>
<pre><code class="language-python">1639/(1639+1327)
</code></pre>
<pre><code>0.552596089008766
</code></pre>
<pre><code class="language-python"># evaluate performance

ConfusionMatrixDisplay.from_estimator(model_lda, X_test, y_test);
print(classification_report(y_true = y_test, y_pred = pred))
</code></pre>
<pre><code>              precision    recall  f1-score   support

           0       0.74      0.73      0.73       275
           1       0.77      0.78      0.77       319

    accuracy                           0.76       594
   macro avg       0.75      0.75      0.75       594
weighted avg       0.76      0.76      0.76       594
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_165_1.png" /></p>
<pre><code class="language-python">confusion_matrix(y_true = y_test, y_pred = pred)
</code></pre>
<pre><code>array([[200,  75],
       [ 70, 249]], dtype=int64)
</code></pre>
<pre><code class="language-python"># Get predictions

model_lda.predict(X_test)
</code></pre>
<pre><code>array([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
       1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
       1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
       1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
       1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
       0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
       1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1],
      dtype=int64)
</code></pre>
<pre><code class="language-python"># Get probability of class membership 

pred_prob = model_lda.predict_proba(X_test)
pred_prob
</code></pre>
<pre><code>array([[0.876743  , 0.123257  ],
       [0.0832324 , 0.9167676 ],
       [0.23516243, 0.76483757],
       ...,
       [0.05691089, 0.94308911],
       [0.11393595, 0.88606405],
       [0.06217806, 0.93782194]])
</code></pre>
<pre><code class="language-python">y_test
</code></pre>
<pre><code>array([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
       0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
       1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
       0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
       1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
       1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
       1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
       0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
       1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
       0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
       1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
       0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
       0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1],
      dtype=int64)
</code></pre>
<pre><code class="language-python"># Source for code below: https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python
fpr, tpr, thresholds = metrics.roc_curve(y_test, pred_prob[:,1])
roc_auc = metrics.auc(fpr, tpr)
plt.figure(figsize = (9,8))
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
for i, txt in enumerate(thresholds):
    if i in np.arange(1, len(thresholds), 10): # print every 10th point to prevent overplotting:
        plt.annotate(text = round(txt,3), xy = (fpr[i], tpr[i]),  
                     xytext=(-44, 0), textcoords='offset points',
                     arrowprops={'arrowstyle':&quot;simple&quot;}, color='green',fontsize=8)
plt.show()

threshold_dataframe = pd.DataFrame({'fpr':fpr, 'tpr': tpr, 'threshold':thresholds}).sort_values(by='threshold')
threshold_dataframe.head()
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_170_0.png" /></p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fpr</th>
      <th>tpr</th>
      <th>threshold</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>155</th>
      <td>1.000000</td>
      <td>1.0</td>
      <td>0.006040</td>
    </tr>
    <tr>
      <th>154</th>
      <td>0.985455</td>
      <td>1.0</td>
      <td>0.015364</td>
    </tr>
    <tr>
      <th>153</th>
      <td>0.974545</td>
      <td>1.0</td>
      <td>0.015584</td>
    </tr>
    <tr>
      <th>152</th>
      <td>0.960000</td>
      <td>1.0</td>
      <td>0.028427</td>
    </tr>
    <tr>
      <th>151</th>
      <td>0.952727</td>
      <td>1.0</td>
      <td>0.028829</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Closing remarks on LDA:</strong>
 - LDA can not be applied to regression problems, it is useful only for classification.
 - LDA does provide class membership probabilities, using the predict_proba() method.
 - There are additional variations to LDA, eg Quadratic Discriminant Analysis, and those may yield better results by allowing a non-linear decision boundary.</p>
<pre><code class="language-python">
</code></pre>
<h2 id="support-vector-machines">Support Vector Machines</h2>
<h3 id="classification-with-svm">Classification with SVM</h3>
<p>SVMs use linear classification techniques, ie, they classify instances based on a linear function of the features.  The idea behind SVMs is simple: instead of thinking about separating with a line, fit the fattest possible bar between the two classes.  </p>
<p>The objective function for SVM incorporates the idea that a wider bar is better.  </p>
<p>Once the widest bar is found, the linear discriminant will be the center line through the bar.  </p>
<p>The distance between the dashed parallel lines is called the margin around the linear discriminant, and the objective function attempts to maximize the margin.  </p>
<p>SVMs require data to be standardized for best results  </p>
<p><strong>SVM Example</strong>  </p>
<p>We will use the same data as before – collegePlace.csv.  However this time we will include all the variables, including the categorical variables.
We convert the categorical variables to numerical using dummy variables with pd.get_dummies().</p>
<pre><code class="language-python"># load the data &amp; convert categoricals into numerical variables
college = pd.read_csv('collegePlace.csv')

college = pd.get_dummies(college)
college
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Internships</th>
      <th>CGPA</th>
      <th>Hostel</th>
      <th>HistoryOfBacklogs</th>
      <th>PlacedOrNot</th>
      <th>Gender_Female</th>
      <th>Gender_Male</th>
      <th>Stream_Civil</th>
      <th>Stream_Computer Science</th>
      <th>Stream_Electrical</th>
      <th>Stream_Electronics And Communication</th>
      <th>Stream_Information Technology</th>
      <th>Stream_Mechanical</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22</td>
      <td>1</td>
      <td>8</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21</td>
      <td>0</td>
      <td>7</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>1</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>21</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>22</td>
      <td>0</td>
      <td>8</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2961</th>
      <td>23</td>
      <td>0</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2962</th>
      <td>23</td>
      <td>1</td>
      <td>7</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2963</th>
      <td>22</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2964</th>
      <td>22</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2965</th>
      <td>23</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
<p>2966 rows × 14 columns</p>
</div>

<p>At this point, fitting a simple SVM SVC (Support Vector Classification) model is trivial.  Refer code below.  </p>
<p>SVM has several variations, including LinearSVC, SVC with Polynomial, etc, refer documentation at https://scikit-learn.org/stable/modules/svm.html.  </p>
<p>Note that we have chosen to pre-process and standardize the input data first.  </p>
<pre><code class="language-python"># divide the dataset into train and test sets, separating the features and target variable
X = college.drop(['PlacedOrNot'], axis=1).values
y = college['PlacedOrNot'].values

scale = preproc.StandardScaler().fit(X)
X = scale.transform(X)
# X = preproc.StandardScaler().fit_transform(X) 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
</code></pre>
<pre><code class="language-python"># fit the model
clf = SVC(probability=True) # setting probability=True here can allow us to get probabilities later
model_svm = clf.fit(X_train, y_train)
pred = model_svm.predict(X_test)
</code></pre>
<pre><code class="language-python"># evaluate performance
ConfusionMatrixDisplay.from_estimator(model_svm, X_test, y_test);
print(classification_report(y_true = y_test, y_pred = pred))
</code></pre>
<pre><code>              precision    recall  f1-score   support

           0       0.83      0.93      0.88       254
           1       0.94      0.86      0.90       340

    accuracy                           0.89       594
   macro avg       0.89      0.89      0.89       594
weighted avg       0.89      0.89      0.89       594
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_179_1.png" /></p>
<pre><code class="language-python">pred_prob = model_svm.predict_proba(X_test)
pred_prob
</code></pre>
<pre><code>array([[3.28070341e-06, 9.99996719e-01],
       [6.65439974e-01, 3.34560026e-01],
       [8.46441402e-01, 1.53558598e-01],
       ...,
       [1.44786221e-02, 9.85521378e-01],
       [3.99281913e-01, 6.00718087e-01],
       [6.46269728e-01, 3.53730272e-01]])
</code></pre>
<pre><code class="language-python"># Source for code below: https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python
fpr, tpr, thresholds = metrics.roc_curve(y_test, pred_prob[:,1])
roc_auc = metrics.auc(fpr, tpr)
plt.figure(figsize = (9,8))
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
for i, txt in enumerate(thresholds):
    if i in np.arange(1, len(thresholds), 10): # print every 10th point to prevent overplotting:
        plt.annotate(text = round(txt,3), xy = (fpr[i], tpr[i]),  
                     xytext=(-44, 0), textcoords='offset points',
                     arrowprops={'arrowstyle':&quot;simple&quot;}, color='green',fontsize=8)
plt.show()

threshold_dataframe = pd.DataFrame({'fpr':fpr, 'tpr': tpr, 'threshold':thresholds}).sort_values(by='threshold')
threshold_dataframe.head()
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_181_0.png" /></p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fpr</th>
      <th>tpr</th>
      <th>threshold</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>192</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.030754</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.988189</td>
      <td>1.000000</td>
      <td>0.037075</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.980315</td>
      <td>1.000000</td>
      <td>0.037614</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.968504</td>
      <td>1.000000</td>
      <td>0.044648</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.964567</td>
      <td>0.997059</td>
      <td>0.045496</td>
    </tr>
  </tbody>
</table>
</div>

<p>SVMs can predict class probabilities, if probability calculations have been set to True as part of the model fitting process.  However, these are not calculated by default by the sklearn algorithm.  </p>
<p>SVMs can also be used for regression problems, using the model type SVR (‘R’ standing for regression), which we examine next.  </p>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<h3 id="regression-with-svm">Regression with SVM</h3>
<p>We perform regression using <code>SVR</code> from sklearn. </p>
<pre><code class="language-python"># Load the data

from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()

X = housing['data']
y = housing['target']
features = housing['feature_names']
DESCR = housing['DESCR']

cali_df = pd.DataFrame(X, columns = features)
cali_df.insert(0,'medv', y)
</code></pre>
<pre><code class="language-python">cali_df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>medv</th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4.526</td>
      <td>8.3252</td>
      <td>41.0</td>
      <td>6.984127</td>
      <td>1.023810</td>
      <td>322.0</td>
      <td>2.555556</td>
      <td>37.88</td>
      <td>-122.23</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.585</td>
      <td>8.3014</td>
      <td>21.0</td>
      <td>6.238137</td>
      <td>0.971880</td>
      <td>2401.0</td>
      <td>2.109842</td>
      <td>37.86</td>
      <td>-122.22</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.521</td>
      <td>7.2574</td>
      <td>52.0</td>
      <td>8.288136</td>
      <td>1.073446</td>
      <td>496.0</td>
      <td>2.802260</td>
      <td>37.85</td>
      <td>-122.24</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.413</td>
      <td>5.6431</td>
      <td>52.0</td>
      <td>5.817352</td>
      <td>1.073059</td>
      <td>558.0</td>
      <td>2.547945</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.422</td>
      <td>3.8462</td>
      <td>52.0</td>
      <td>6.281853</td>
      <td>1.081081</td>
      <td>565.0</td>
      <td>2.181467</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>20635</th>
      <td>0.781</td>
      <td>1.5603</td>
      <td>25.0</td>
      <td>5.045455</td>
      <td>1.133333</td>
      <td>845.0</td>
      <td>2.560606</td>
      <td>39.48</td>
      <td>-121.09</td>
    </tr>
    <tr>
      <th>20636</th>
      <td>0.771</td>
      <td>2.5568</td>
      <td>18.0</td>
      <td>6.114035</td>
      <td>1.315789</td>
      <td>356.0</td>
      <td>3.122807</td>
      <td>39.49</td>
      <td>-121.21</td>
    </tr>
    <tr>
      <th>20637</th>
      <td>0.923</td>
      <td>1.7000</td>
      <td>17.0</td>
      <td>5.205543</td>
      <td>1.120092</td>
      <td>1007.0</td>
      <td>2.325635</td>
      <td>39.43</td>
      <td>-121.22</td>
    </tr>
    <tr>
      <th>20638</th>
      <td>0.847</td>
      <td>1.8672</td>
      <td>18.0</td>
      <td>5.329513</td>
      <td>1.171920</td>
      <td>741.0</td>
      <td>2.123209</td>
      <td>39.43</td>
      <td>-121.32</td>
    </tr>
    <tr>
      <th>20639</th>
      <td>0.894</td>
      <td>2.3886</td>
      <td>16.0</td>
      <td>5.254717</td>
      <td>1.162264</td>
      <td>1387.0</td>
      <td>2.616981</td>
      <td>39.37</td>
      <td>-121.24</td>
    </tr>
  </tbody>
</table>
<p>20640 rows × 9 columns</p>
</div>

<pre><code class="language-python"># Train test split

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
</code></pre>
<pre><code class="language-python"># Fit model

from sklearn.svm import SVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
model_svr = make_pipeline(StandardScaler(), SVR())
model_svr.fit(X, y)
model_svr = model_svr.fit(X_train, y_train)
model_svr.predict(X_test)
</code></pre>
<pre><code>array([2.89622439, 1.92606932, 1.55771122, ..., 1.60987874, 0.82130714,
       2.96297243])
</code></pre>
<pre><code class="language-python"># Evaluate model

y_pred  =  model_svr.predict(X_test)
from sklearn.metrics import mean_absolute_error, mean_squared_error
print('MSE = ', mean_squared_error(y_test,y_pred))
print('RMSE = ', np.sqrt(mean_squared_error(y_test,y_pred)))
print('MAE = ', mean_absolute_error(y_test,y_pred))
</code></pre>
<pre><code>MSE =  0.3382225528180732
RMSE =  0.5815690438959704
MAE =  0.39084152978427034
</code></pre>
<pre><code class="language-python"># Look at residuals

plt.figure(figsize = (8,8))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.title('Actual vs Predicted Home Value in $000s \n Closer to red \
line (identity) means more accurate prediction')
plt.plot( [y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red' )
plt.xlabel(&quot;Actual&quot;)
plt.ylabel(&quot;Predicted&quot;);
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_192_0.png" /></p>
<pre><code class="language-python">print(cali_df.medv.describe())
cali_df.medv.plot.hist(bins=20)
</code></pre>
<pre><code>count    20640.000000
mean         2.068558
std          1.153956
min          0.149990
25%          1.196000
50%          1.797000
75%          2.647250
max          5.000010
Name: medv, dtype: float64





&lt;Axes: ylabel='Frequency'&gt;
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_193_2.png" /></p>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<h2 id="naive-bayes">Naive Bayes</h2>
<p>Essentially, the logic behind Naive Bayes is as follows: Instead of taking the absolute probability of something happening, we look at the probability of something happening given other things we know have already happened.  </p>
<p>So the probability of a flood in the next 1 week may be say 0.1%, but this probability would be different if we already know that 6 inches of rain has already fallen in the past 24 hours.  </p>
<p>For each of the categories to be predicted, Naive Bayes considers the conditional probability given the values of other independent variables.  </p>
<p>Naïve Bayes uses categorical predictors.  For continuous predictors, it assumes a distribution with a mean and standard deviation, which are used to calculate probabilities used in the algorithm.
We do not need to standardize the feature set before using Naïve Bayes.  </p>
<pre><code class="language-python">from sklearn import datasets

X = datasets.load_wine()['data']
y = datasets.load_wine()['target']
features = datasets.load_wine()['feature_names']
DESCR = datasets.load_wine()['DESCR']
classes = datasets.load_wine()['target_names']

wine_df = pd.DataFrame(X, columns = features)
wine_df.insert(0,'class', y)
</code></pre>
<pre><code class="language-python">wine_df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class</th>
      <th>alcohol</th>
      <th>malic_acid</th>
      <th>ash</th>
      <th>alcalinity_of_ash</th>
      <th>magnesium</th>
      <th>total_phenols</th>
      <th>flavanoids</th>
      <th>nonflavanoid_phenols</th>
      <th>proanthocyanins</th>
      <th>color_intensity</th>
      <th>hue</th>
      <th>od280/od315_of_diluted_wines</th>
      <th>proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127.0</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.640000</td>
      <td>1.040</td>
      <td>3.92</td>
      <td>1065.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100.0</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.380000</td>
      <td>1.050</td>
      <td>3.40</td>
      <td>1050.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101.0</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.680000</td>
      <td>1.030</td>
      <td>3.17</td>
      <td>1185.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113.0</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.800000</td>
      <td>0.860</td>
      <td>3.45</td>
      <td>1480.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118.0</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.320000</td>
      <td>1.040</td>
      <td>2.93</td>
      <td>735.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>14.20</td>
      <td>1.76</td>
      <td>2.45</td>
      <td>15.2</td>
      <td>112.0</td>
      <td>3.27</td>
      <td>3.39</td>
      <td>0.34</td>
      <td>1.97</td>
      <td>6.750000</td>
      <td>1.050</td>
      <td>2.85</td>
      <td>1450.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>14.39</td>
      <td>1.87</td>
      <td>2.45</td>
      <td>14.6</td>
      <td>96.0</td>
      <td>2.50</td>
      <td>2.52</td>
      <td>0.30</td>
      <td>1.98</td>
      <td>5.250000</td>
      <td>1.020</td>
      <td>3.58</td>
      <td>1290.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>14.06</td>
      <td>2.15</td>
      <td>2.61</td>
      <td>17.6</td>
      <td>121.0</td>
      <td>2.60</td>
      <td>2.51</td>
      <td>0.31</td>
      <td>1.25</td>
      <td>5.050000</td>
      <td>1.060</td>
      <td>3.58</td>
      <td>1295.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>14.83</td>
      <td>1.64</td>
      <td>2.17</td>
      <td>14.0</td>
      <td>97.0</td>
      <td>2.80</td>
      <td>2.98</td>
      <td>0.29</td>
      <td>1.98</td>
      <td>5.200000</td>
      <td>1.080</td>
      <td>2.85</td>
      <td>1045.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>13.86</td>
      <td>1.35</td>
      <td>2.27</td>
      <td>16.0</td>
      <td>98.0</td>
      <td>2.98</td>
      <td>3.15</td>
      <td>0.22</td>
      <td>1.85</td>
      <td>7.220000</td>
      <td>1.010</td>
      <td>3.55</td>
      <td>1045.0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0</td>
      <td>14.10</td>
      <td>2.16</td>
      <td>2.30</td>
      <td>18.0</td>
      <td>105.0</td>
      <td>2.95</td>
      <td>3.32</td>
      <td>0.22</td>
      <td>2.38</td>
      <td>5.750000</td>
      <td>1.250</td>
      <td>3.17</td>
      <td>1510.0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>14.12</td>
      <td>1.48</td>
      <td>2.32</td>
      <td>16.8</td>
      <td>95.0</td>
      <td>2.20</td>
      <td>2.43</td>
      <td>0.26</td>
      <td>1.57</td>
      <td>5.000000</td>
      <td>1.170</td>
      <td>2.82</td>
      <td>1280.0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0</td>
      <td>13.75</td>
      <td>1.73</td>
      <td>2.41</td>
      <td>16.0</td>
      <td>89.0</td>
      <td>2.60</td>
      <td>2.76</td>
      <td>0.29</td>
      <td>1.81</td>
      <td>5.600000</td>
      <td>1.150</td>
      <td>2.90</td>
      <td>1320.0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0</td>
      <td>14.75</td>
      <td>1.73</td>
      <td>2.39</td>
      <td>11.4</td>
      <td>91.0</td>
      <td>3.10</td>
      <td>3.69</td>
      <td>0.43</td>
      <td>2.81</td>
      <td>5.400000</td>
      <td>1.250</td>
      <td>2.73</td>
      <td>1150.0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0</td>
      <td>14.38</td>
      <td>1.87</td>
      <td>2.38</td>
      <td>12.0</td>
      <td>102.0</td>
      <td>3.30</td>
      <td>3.64</td>
      <td>0.29</td>
      <td>2.96</td>
      <td>7.500000</td>
      <td>1.200</td>
      <td>3.00</td>
      <td>1547.0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0</td>
      <td>13.63</td>
      <td>1.81</td>
      <td>2.70</td>
      <td>17.2</td>
      <td>112.0</td>
      <td>2.85</td>
      <td>2.91</td>
      <td>0.30</td>
      <td>1.46</td>
      <td>7.300000</td>
      <td>1.280</td>
      <td>2.88</td>
      <td>1310.0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0</td>
      <td>14.30</td>
      <td>1.92</td>
      <td>2.72</td>
      <td>20.0</td>
      <td>120.0</td>
      <td>2.80</td>
      <td>3.14</td>
      <td>0.33</td>
      <td>1.97</td>
      <td>6.200000</td>
      <td>1.070</td>
      <td>2.65</td>
      <td>1280.0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0</td>
      <td>13.83</td>
      <td>1.57</td>
      <td>2.62</td>
      <td>20.0</td>
      <td>115.0</td>
      <td>2.95</td>
      <td>3.40</td>
      <td>0.40</td>
      <td>1.72</td>
      <td>6.600000</td>
      <td>1.130</td>
      <td>2.57</td>
      <td>1130.0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0</td>
      <td>14.19</td>
      <td>1.59</td>
      <td>2.48</td>
      <td>16.5</td>
      <td>108.0</td>
      <td>3.30</td>
      <td>3.93</td>
      <td>0.32</td>
      <td>1.86</td>
      <td>8.700000</td>
      <td>1.230</td>
      <td>2.82</td>
      <td>1680.0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0</td>
      <td>13.64</td>
      <td>3.10</td>
      <td>2.56</td>
      <td>15.2</td>
      <td>116.0</td>
      <td>2.70</td>
      <td>3.03</td>
      <td>0.17</td>
      <td>1.66</td>
      <td>5.100000</td>
      <td>0.960</td>
      <td>3.36</td>
      <td>845.0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0</td>
      <td>14.06</td>
      <td>1.63</td>
      <td>2.28</td>
      <td>16.0</td>
      <td>126.0</td>
      <td>3.00</td>
      <td>3.17</td>
      <td>0.24</td>
      <td>2.10</td>
      <td>5.650000</td>
      <td>1.090</td>
      <td>3.71</td>
      <td>780.0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0</td>
      <td>12.93</td>
      <td>3.80</td>
      <td>2.65</td>
      <td>18.6</td>
      <td>102.0</td>
      <td>2.41</td>
      <td>2.41</td>
      <td>0.25</td>
      <td>1.98</td>
      <td>4.500000</td>
      <td>1.030</td>
      <td>3.52</td>
      <td>770.0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>13.71</td>
      <td>1.86</td>
      <td>2.36</td>
      <td>16.6</td>
      <td>101.0</td>
      <td>2.61</td>
      <td>2.88</td>
      <td>0.27</td>
      <td>1.69</td>
      <td>3.800000</td>
      <td>1.110</td>
      <td>4.00</td>
      <td>1035.0</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0</td>
      <td>12.85</td>
      <td>1.60</td>
      <td>2.52</td>
      <td>17.8</td>
      <td>95.0</td>
      <td>2.48</td>
      <td>2.37</td>
      <td>0.26</td>
      <td>1.46</td>
      <td>3.930000</td>
      <td>1.090</td>
      <td>3.63</td>
      <td>1015.0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>13.50</td>
      <td>1.81</td>
      <td>2.61</td>
      <td>20.0</td>
      <td>96.0</td>
      <td>2.53</td>
      <td>2.61</td>
      <td>0.28</td>
      <td>1.66</td>
      <td>3.520000</td>
      <td>1.120</td>
      <td>3.82</td>
      <td>845.0</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0</td>
      <td>13.05</td>
      <td>2.05</td>
      <td>3.22</td>
      <td>25.0</td>
      <td>124.0</td>
      <td>2.63</td>
      <td>2.68</td>
      <td>0.47</td>
      <td>1.92</td>
      <td>3.580000</td>
      <td>1.130</td>
      <td>3.20</td>
      <td>830.0</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0</td>
      <td>13.39</td>
      <td>1.77</td>
      <td>2.62</td>
      <td>16.1</td>
      <td>93.0</td>
      <td>2.85</td>
      <td>2.94</td>
      <td>0.34</td>
      <td>1.45</td>
      <td>4.800000</td>
      <td>0.920</td>
      <td>3.22</td>
      <td>1195.0</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0</td>
      <td>13.30</td>
      <td>1.72</td>
      <td>2.14</td>
      <td>17.0</td>
      <td>94.0</td>
      <td>2.40</td>
      <td>2.19</td>
      <td>0.27</td>
      <td>1.35</td>
      <td>3.950000</td>
      <td>1.020</td>
      <td>2.77</td>
      <td>1285.0</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0</td>
      <td>13.87</td>
      <td>1.90</td>
      <td>2.80</td>
      <td>19.4</td>
      <td>107.0</td>
      <td>2.95</td>
      <td>2.97</td>
      <td>0.37</td>
      <td>1.76</td>
      <td>4.500000</td>
      <td>1.250</td>
      <td>3.40</td>
      <td>915.0</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0</td>
      <td>14.02</td>
      <td>1.68</td>
      <td>2.21</td>
      <td>16.0</td>
      <td>96.0</td>
      <td>2.65</td>
      <td>2.33</td>
      <td>0.26</td>
      <td>1.98</td>
      <td>4.700000</td>
      <td>1.040</td>
      <td>3.59</td>
      <td>1035.0</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0</td>
      <td>13.73</td>
      <td>1.50</td>
      <td>2.70</td>
      <td>22.5</td>
      <td>101.0</td>
      <td>3.00</td>
      <td>3.25</td>
      <td>0.29</td>
      <td>2.38</td>
      <td>5.700000</td>
      <td>1.190</td>
      <td>2.71</td>
      <td>1285.0</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0</td>
      <td>13.58</td>
      <td>1.66</td>
      <td>2.36</td>
      <td>19.1</td>
      <td>106.0</td>
      <td>2.86</td>
      <td>3.19</td>
      <td>0.22</td>
      <td>1.95</td>
      <td>6.900000</td>
      <td>1.090</td>
      <td>2.88</td>
      <td>1515.0</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0</td>
      <td>13.68</td>
      <td>1.83</td>
      <td>2.36</td>
      <td>17.2</td>
      <td>104.0</td>
      <td>2.42</td>
      <td>2.69</td>
      <td>0.42</td>
      <td>1.97</td>
      <td>3.840000</td>
      <td>1.230</td>
      <td>2.87</td>
      <td>990.0</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0</td>
      <td>13.76</td>
      <td>1.53</td>
      <td>2.70</td>
      <td>19.5</td>
      <td>132.0</td>
      <td>2.95</td>
      <td>2.74</td>
      <td>0.50</td>
      <td>1.35</td>
      <td>5.400000</td>
      <td>1.250</td>
      <td>3.00</td>
      <td>1235.0</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0</td>
      <td>13.51</td>
      <td>1.80</td>
      <td>2.65</td>
      <td>19.0</td>
      <td>110.0</td>
      <td>2.35</td>
      <td>2.53</td>
      <td>0.29</td>
      <td>1.54</td>
      <td>4.200000</td>
      <td>1.100</td>
      <td>2.87</td>
      <td>1095.0</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0</td>
      <td>13.48</td>
      <td>1.81</td>
      <td>2.41</td>
      <td>20.5</td>
      <td>100.0</td>
      <td>2.70</td>
      <td>2.98</td>
      <td>0.26</td>
      <td>1.86</td>
      <td>5.100000</td>
      <td>1.040</td>
      <td>3.47</td>
      <td>920.0</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0</td>
      <td>13.28</td>
      <td>1.64</td>
      <td>2.84</td>
      <td>15.5</td>
      <td>110.0</td>
      <td>2.60</td>
      <td>2.68</td>
      <td>0.34</td>
      <td>1.36</td>
      <td>4.600000</td>
      <td>1.090</td>
      <td>2.78</td>
      <td>880.0</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0</td>
      <td>13.05</td>
      <td>1.65</td>
      <td>2.55</td>
      <td>18.0</td>
      <td>98.0</td>
      <td>2.45</td>
      <td>2.43</td>
      <td>0.29</td>
      <td>1.44</td>
      <td>4.250000</td>
      <td>1.120</td>
      <td>2.51</td>
      <td>1105.0</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0</td>
      <td>13.07</td>
      <td>1.50</td>
      <td>2.10</td>
      <td>15.5</td>
      <td>98.0</td>
      <td>2.40</td>
      <td>2.64</td>
      <td>0.28</td>
      <td>1.37</td>
      <td>3.700000</td>
      <td>1.180</td>
      <td>2.69</td>
      <td>1020.0</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0</td>
      <td>14.22</td>
      <td>3.99</td>
      <td>2.51</td>
      <td>13.2</td>
      <td>128.0</td>
      <td>3.00</td>
      <td>3.04</td>
      <td>0.20</td>
      <td>2.08</td>
      <td>5.100000</td>
      <td>0.890</td>
      <td>3.53</td>
      <td>760.0</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0</td>
      <td>13.56</td>
      <td>1.71</td>
      <td>2.31</td>
      <td>16.2</td>
      <td>117.0</td>
      <td>3.15</td>
      <td>3.29</td>
      <td>0.34</td>
      <td>2.34</td>
      <td>6.130000</td>
      <td>0.950</td>
      <td>3.38</td>
      <td>795.0</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0</td>
      <td>13.41</td>
      <td>3.84</td>
      <td>2.12</td>
      <td>18.8</td>
      <td>90.0</td>
      <td>2.45</td>
      <td>2.68</td>
      <td>0.27</td>
      <td>1.48</td>
      <td>4.280000</td>
      <td>0.910</td>
      <td>3.00</td>
      <td>1035.0</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0</td>
      <td>13.88</td>
      <td>1.89</td>
      <td>2.59</td>
      <td>15.0</td>
      <td>101.0</td>
      <td>3.25</td>
      <td>3.56</td>
      <td>0.17</td>
      <td>1.70</td>
      <td>5.430000</td>
      <td>0.880</td>
      <td>3.56</td>
      <td>1095.0</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0</td>
      <td>13.24</td>
      <td>3.98</td>
      <td>2.29</td>
      <td>17.5</td>
      <td>103.0</td>
      <td>2.64</td>
      <td>2.63</td>
      <td>0.32</td>
      <td>1.66</td>
      <td>4.360000</td>
      <td>0.820</td>
      <td>3.00</td>
      <td>680.0</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0</td>
      <td>13.05</td>
      <td>1.77</td>
      <td>2.10</td>
      <td>17.0</td>
      <td>107.0</td>
      <td>3.00</td>
      <td>3.00</td>
      <td>0.28</td>
      <td>2.03</td>
      <td>5.040000</td>
      <td>0.880</td>
      <td>3.35</td>
      <td>885.0</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0</td>
      <td>14.21</td>
      <td>4.04</td>
      <td>2.44</td>
      <td>18.9</td>
      <td>111.0</td>
      <td>2.85</td>
      <td>2.65</td>
      <td>0.30</td>
      <td>1.25</td>
      <td>5.240000</td>
      <td>0.870</td>
      <td>3.33</td>
      <td>1080.0</td>
    </tr>
    <tr>
      <th>46</th>
      <td>0</td>
      <td>14.38</td>
      <td>3.59</td>
      <td>2.28</td>
      <td>16.0</td>
      <td>102.0</td>
      <td>3.25</td>
      <td>3.17</td>
      <td>0.27</td>
      <td>2.19</td>
      <td>4.900000</td>
      <td>1.040</td>
      <td>3.44</td>
      <td>1065.0</td>
    </tr>
    <tr>
      <th>47</th>
      <td>0</td>
      <td>13.90</td>
      <td>1.68</td>
      <td>2.12</td>
      <td>16.0</td>
      <td>101.0</td>
      <td>3.10</td>
      <td>3.39</td>
      <td>0.21</td>
      <td>2.14</td>
      <td>6.100000</td>
      <td>0.910</td>
      <td>3.33</td>
      <td>985.0</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0</td>
      <td>14.10</td>
      <td>2.02</td>
      <td>2.40</td>
      <td>18.8</td>
      <td>103.0</td>
      <td>2.75</td>
      <td>2.92</td>
      <td>0.32</td>
      <td>2.38</td>
      <td>6.200000</td>
      <td>1.070</td>
      <td>2.75</td>
      <td>1060.0</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0</td>
      <td>13.94</td>
      <td>1.73</td>
      <td>2.27</td>
      <td>17.4</td>
      <td>108.0</td>
      <td>2.88</td>
      <td>3.54</td>
      <td>0.32</td>
      <td>2.08</td>
      <td>8.900000</td>
      <td>1.120</td>
      <td>3.10</td>
      <td>1260.0</td>
    </tr>
    <tr>
      <th>50</th>
      <td>0</td>
      <td>13.05</td>
      <td>1.73</td>
      <td>2.04</td>
      <td>12.4</td>
      <td>92.0</td>
      <td>2.72</td>
      <td>3.27</td>
      <td>0.17</td>
      <td>2.91</td>
      <td>7.200000</td>
      <td>1.120</td>
      <td>2.91</td>
      <td>1150.0</td>
    </tr>
    <tr>
      <th>51</th>
      <td>0</td>
      <td>13.83</td>
      <td>1.65</td>
      <td>2.60</td>
      <td>17.2</td>
      <td>94.0</td>
      <td>2.45</td>
      <td>2.99</td>
      <td>0.22</td>
      <td>2.29</td>
      <td>5.600000</td>
      <td>1.240</td>
      <td>3.37</td>
      <td>1265.0</td>
    </tr>
    <tr>
      <th>52</th>
      <td>0</td>
      <td>13.82</td>
      <td>1.75</td>
      <td>2.42</td>
      <td>14.0</td>
      <td>111.0</td>
      <td>3.88</td>
      <td>3.74</td>
      <td>0.32</td>
      <td>1.87</td>
      <td>7.050000</td>
      <td>1.010</td>
      <td>3.26</td>
      <td>1190.0</td>
    </tr>
    <tr>
      <th>53</th>
      <td>0</td>
      <td>13.77</td>
      <td>1.90</td>
      <td>2.68</td>
      <td>17.1</td>
      <td>115.0</td>
      <td>3.00</td>
      <td>2.79</td>
      <td>0.39</td>
      <td>1.68</td>
      <td>6.300000</td>
      <td>1.130</td>
      <td>2.93</td>
      <td>1375.0</td>
    </tr>
    <tr>
      <th>54</th>
      <td>0</td>
      <td>13.74</td>
      <td>1.67</td>
      <td>2.25</td>
      <td>16.4</td>
      <td>118.0</td>
      <td>2.60</td>
      <td>2.90</td>
      <td>0.21</td>
      <td>1.62</td>
      <td>5.850000</td>
      <td>0.920</td>
      <td>3.20</td>
      <td>1060.0</td>
    </tr>
    <tr>
      <th>55</th>
      <td>0</td>
      <td>13.56</td>
      <td>1.73</td>
      <td>2.46</td>
      <td>20.5</td>
      <td>116.0</td>
      <td>2.96</td>
      <td>2.78</td>
      <td>0.20</td>
      <td>2.45</td>
      <td>6.250000</td>
      <td>0.980</td>
      <td>3.03</td>
      <td>1120.0</td>
    </tr>
    <tr>
      <th>56</th>
      <td>0</td>
      <td>14.22</td>
      <td>1.70</td>
      <td>2.30</td>
      <td>16.3</td>
      <td>118.0</td>
      <td>3.20</td>
      <td>3.00</td>
      <td>0.26</td>
      <td>2.03</td>
      <td>6.380000</td>
      <td>0.940</td>
      <td>3.31</td>
      <td>970.0</td>
    </tr>
    <tr>
      <th>57</th>
      <td>0</td>
      <td>13.29</td>
      <td>1.97</td>
      <td>2.68</td>
      <td>16.8</td>
      <td>102.0</td>
      <td>3.00</td>
      <td>3.23</td>
      <td>0.31</td>
      <td>1.66</td>
      <td>6.000000</td>
      <td>1.070</td>
      <td>2.84</td>
      <td>1270.0</td>
    </tr>
    <tr>
      <th>58</th>
      <td>0</td>
      <td>13.72</td>
      <td>1.43</td>
      <td>2.50</td>
      <td>16.7</td>
      <td>108.0</td>
      <td>3.40</td>
      <td>3.67</td>
      <td>0.19</td>
      <td>2.04</td>
      <td>6.800000</td>
      <td>0.890</td>
      <td>2.87</td>
      <td>1285.0</td>
    </tr>
    <tr>
      <th>59</th>
      <td>1</td>
      <td>12.37</td>
      <td>0.94</td>
      <td>1.36</td>
      <td>10.6</td>
      <td>88.0</td>
      <td>1.98</td>
      <td>0.57</td>
      <td>0.28</td>
      <td>0.42</td>
      <td>1.950000</td>
      <td>1.050</td>
      <td>1.82</td>
      <td>520.0</td>
    </tr>
    <tr>
      <th>60</th>
      <td>1</td>
      <td>12.33</td>
      <td>1.10</td>
      <td>2.28</td>
      <td>16.0</td>
      <td>101.0</td>
      <td>2.05</td>
      <td>1.09</td>
      <td>0.63</td>
      <td>0.41</td>
      <td>3.270000</td>
      <td>1.250</td>
      <td>1.67</td>
      <td>680.0</td>
    </tr>
    <tr>
      <th>61</th>
      <td>1</td>
      <td>12.64</td>
      <td>1.36</td>
      <td>2.02</td>
      <td>16.8</td>
      <td>100.0</td>
      <td>2.02</td>
      <td>1.41</td>
      <td>0.53</td>
      <td>0.62</td>
      <td>5.750000</td>
      <td>0.980</td>
      <td>1.59</td>
      <td>450.0</td>
    </tr>
    <tr>
      <th>62</th>
      <td>1</td>
      <td>13.67</td>
      <td>1.25</td>
      <td>1.92</td>
      <td>18.0</td>
      <td>94.0</td>
      <td>2.10</td>
      <td>1.79</td>
      <td>0.32</td>
      <td>0.73</td>
      <td>3.800000</td>
      <td>1.230</td>
      <td>2.46</td>
      <td>630.0</td>
    </tr>
    <tr>
      <th>63</th>
      <td>1</td>
      <td>12.37</td>
      <td>1.13</td>
      <td>2.16</td>
      <td>19.0</td>
      <td>87.0</td>
      <td>3.50</td>
      <td>3.10</td>
      <td>0.19</td>
      <td>1.87</td>
      <td>4.450000</td>
      <td>1.220</td>
      <td>2.87</td>
      <td>420.0</td>
    </tr>
    <tr>
      <th>64</th>
      <td>1</td>
      <td>12.17</td>
      <td>1.45</td>
      <td>2.53</td>
      <td>19.0</td>
      <td>104.0</td>
      <td>1.89</td>
      <td>1.75</td>
      <td>0.45</td>
      <td>1.03</td>
      <td>2.950000</td>
      <td>1.450</td>
      <td>2.23</td>
      <td>355.0</td>
    </tr>
    <tr>
      <th>65</th>
      <td>1</td>
      <td>12.37</td>
      <td>1.21</td>
      <td>2.56</td>
      <td>18.1</td>
      <td>98.0</td>
      <td>2.42</td>
      <td>2.65</td>
      <td>0.37</td>
      <td>2.08</td>
      <td>4.600000</td>
      <td>1.190</td>
      <td>2.30</td>
      <td>678.0</td>
    </tr>
    <tr>
      <th>66</th>
      <td>1</td>
      <td>13.11</td>
      <td>1.01</td>
      <td>1.70</td>
      <td>15.0</td>
      <td>78.0</td>
      <td>2.98</td>
      <td>3.18</td>
      <td>0.26</td>
      <td>2.28</td>
      <td>5.300000</td>
      <td>1.120</td>
      <td>3.18</td>
      <td>502.0</td>
    </tr>
    <tr>
      <th>67</th>
      <td>1</td>
      <td>12.37</td>
      <td>1.17</td>
      <td>1.92</td>
      <td>19.6</td>
      <td>78.0</td>
      <td>2.11</td>
      <td>2.00</td>
      <td>0.27</td>
      <td>1.04</td>
      <td>4.680000</td>
      <td>1.120</td>
      <td>3.48</td>
      <td>510.0</td>
    </tr>
    <tr>
      <th>68</th>
      <td>1</td>
      <td>13.34</td>
      <td>0.94</td>
      <td>2.36</td>
      <td>17.0</td>
      <td>110.0</td>
      <td>2.53</td>
      <td>1.30</td>
      <td>0.55</td>
      <td>0.42</td>
      <td>3.170000</td>
      <td>1.020</td>
      <td>1.93</td>
      <td>750.0</td>
    </tr>
    <tr>
      <th>69</th>
      <td>1</td>
      <td>12.21</td>
      <td>1.19</td>
      <td>1.75</td>
      <td>16.8</td>
      <td>151.0</td>
      <td>1.85</td>
      <td>1.28</td>
      <td>0.14</td>
      <td>2.50</td>
      <td>2.850000</td>
      <td>1.280</td>
      <td>3.07</td>
      <td>718.0</td>
    </tr>
    <tr>
      <th>70</th>
      <td>1</td>
      <td>12.29</td>
      <td>1.61</td>
      <td>2.21</td>
      <td>20.4</td>
      <td>103.0</td>
      <td>1.10</td>
      <td>1.02</td>
      <td>0.37</td>
      <td>1.46</td>
      <td>3.050000</td>
      <td>0.906</td>
      <td>1.82</td>
      <td>870.0</td>
    </tr>
    <tr>
      <th>71</th>
      <td>1</td>
      <td>13.86</td>
      <td>1.51</td>
      <td>2.67</td>
      <td>25.0</td>
      <td>86.0</td>
      <td>2.95</td>
      <td>2.86</td>
      <td>0.21</td>
      <td>1.87</td>
      <td>3.380000</td>
      <td>1.360</td>
      <td>3.16</td>
      <td>410.0</td>
    </tr>
    <tr>
      <th>72</th>
      <td>1</td>
      <td>13.49</td>
      <td>1.66</td>
      <td>2.24</td>
      <td>24.0</td>
      <td>87.0</td>
      <td>1.88</td>
      <td>1.84</td>
      <td>0.27</td>
      <td>1.03</td>
      <td>3.740000</td>
      <td>0.980</td>
      <td>2.78</td>
      <td>472.0</td>
    </tr>
    <tr>
      <th>73</th>
      <td>1</td>
      <td>12.99</td>
      <td>1.67</td>
      <td>2.60</td>
      <td>30.0</td>
      <td>139.0</td>
      <td>3.30</td>
      <td>2.89</td>
      <td>0.21</td>
      <td>1.96</td>
      <td>3.350000</td>
      <td>1.310</td>
      <td>3.50</td>
      <td>985.0</td>
    </tr>
    <tr>
      <th>74</th>
      <td>1</td>
      <td>11.96</td>
      <td>1.09</td>
      <td>2.30</td>
      <td>21.0</td>
      <td>101.0</td>
      <td>3.38</td>
      <td>2.14</td>
      <td>0.13</td>
      <td>1.65</td>
      <td>3.210000</td>
      <td>0.990</td>
      <td>3.13</td>
      <td>886.0</td>
    </tr>
    <tr>
      <th>75</th>
      <td>1</td>
      <td>11.66</td>
      <td>1.88</td>
      <td>1.92</td>
      <td>16.0</td>
      <td>97.0</td>
      <td>1.61</td>
      <td>1.57</td>
      <td>0.34</td>
      <td>1.15</td>
      <td>3.800000</td>
      <td>1.230</td>
      <td>2.14</td>
      <td>428.0</td>
    </tr>
    <tr>
      <th>76</th>
      <td>1</td>
      <td>13.03</td>
      <td>0.90</td>
      <td>1.71</td>
      <td>16.0</td>
      <td>86.0</td>
      <td>1.95</td>
      <td>2.03</td>
      <td>0.24</td>
      <td>1.46</td>
      <td>4.600000</td>
      <td>1.190</td>
      <td>2.48</td>
      <td>392.0</td>
    </tr>
    <tr>
      <th>77</th>
      <td>1</td>
      <td>11.84</td>
      <td>2.89</td>
      <td>2.23</td>
      <td>18.0</td>
      <td>112.0</td>
      <td>1.72</td>
      <td>1.32</td>
      <td>0.43</td>
      <td>0.95</td>
      <td>2.650000</td>
      <td>0.960</td>
      <td>2.52</td>
      <td>500.0</td>
    </tr>
    <tr>
      <th>78</th>
      <td>1</td>
      <td>12.33</td>
      <td>0.99</td>
      <td>1.95</td>
      <td>14.8</td>
      <td>136.0</td>
      <td>1.90</td>
      <td>1.85</td>
      <td>0.35</td>
      <td>2.76</td>
      <td>3.400000</td>
      <td>1.060</td>
      <td>2.31</td>
      <td>750.0</td>
    </tr>
    <tr>
      <th>79</th>
      <td>1</td>
      <td>12.70</td>
      <td>3.87</td>
      <td>2.40</td>
      <td>23.0</td>
      <td>101.0</td>
      <td>2.83</td>
      <td>2.55</td>
      <td>0.43</td>
      <td>1.95</td>
      <td>2.570000</td>
      <td>1.190</td>
      <td>3.13</td>
      <td>463.0</td>
    </tr>
    <tr>
      <th>80</th>
      <td>1</td>
      <td>12.00</td>
      <td>0.92</td>
      <td>2.00</td>
      <td>19.0</td>
      <td>86.0</td>
      <td>2.42</td>
      <td>2.26</td>
      <td>0.30</td>
      <td>1.43</td>
      <td>2.500000</td>
      <td>1.380</td>
      <td>3.12</td>
      <td>278.0</td>
    </tr>
    <tr>
      <th>81</th>
      <td>1</td>
      <td>12.72</td>
      <td>1.81</td>
      <td>2.20</td>
      <td>18.8</td>
      <td>86.0</td>
      <td>2.20</td>
      <td>2.53</td>
      <td>0.26</td>
      <td>1.77</td>
      <td>3.900000</td>
      <td>1.160</td>
      <td>3.14</td>
      <td>714.0</td>
    </tr>
    <tr>
      <th>82</th>
      <td>1</td>
      <td>12.08</td>
      <td>1.13</td>
      <td>2.51</td>
      <td>24.0</td>
      <td>78.0</td>
      <td>2.00</td>
      <td>1.58</td>
      <td>0.40</td>
      <td>1.40</td>
      <td>2.200000</td>
      <td>1.310</td>
      <td>2.72</td>
      <td>630.0</td>
    </tr>
    <tr>
      <th>83</th>
      <td>1</td>
      <td>13.05</td>
      <td>3.86</td>
      <td>2.32</td>
      <td>22.5</td>
      <td>85.0</td>
      <td>1.65</td>
      <td>1.59</td>
      <td>0.61</td>
      <td>1.62</td>
      <td>4.800000</td>
      <td>0.840</td>
      <td>2.01</td>
      <td>515.0</td>
    </tr>
    <tr>
      <th>84</th>
      <td>1</td>
      <td>11.84</td>
      <td>0.89</td>
      <td>2.58</td>
      <td>18.0</td>
      <td>94.0</td>
      <td>2.20</td>
      <td>2.21</td>
      <td>0.22</td>
      <td>2.35</td>
      <td>3.050000</td>
      <td>0.790</td>
      <td>3.08</td>
      <td>520.0</td>
    </tr>
    <tr>
      <th>85</th>
      <td>1</td>
      <td>12.67</td>
      <td>0.98</td>
      <td>2.24</td>
      <td>18.0</td>
      <td>99.0</td>
      <td>2.20</td>
      <td>1.94</td>
      <td>0.30</td>
      <td>1.46</td>
      <td>2.620000</td>
      <td>1.230</td>
      <td>3.16</td>
      <td>450.0</td>
    </tr>
    <tr>
      <th>86</th>
      <td>1</td>
      <td>12.16</td>
      <td>1.61</td>
      <td>2.31</td>
      <td>22.8</td>
      <td>90.0</td>
      <td>1.78</td>
      <td>1.69</td>
      <td>0.43</td>
      <td>1.56</td>
      <td>2.450000</td>
      <td>1.330</td>
      <td>2.26</td>
      <td>495.0</td>
    </tr>
    <tr>
      <th>87</th>
      <td>1</td>
      <td>11.65</td>
      <td>1.67</td>
      <td>2.62</td>
      <td>26.0</td>
      <td>88.0</td>
      <td>1.92</td>
      <td>1.61</td>
      <td>0.40</td>
      <td>1.34</td>
      <td>2.600000</td>
      <td>1.360</td>
      <td>3.21</td>
      <td>562.0</td>
    </tr>
    <tr>
      <th>88</th>
      <td>1</td>
      <td>11.64</td>
      <td>2.06</td>
      <td>2.46</td>
      <td>21.6</td>
      <td>84.0</td>
      <td>1.95</td>
      <td>1.69</td>
      <td>0.48</td>
      <td>1.35</td>
      <td>2.800000</td>
      <td>1.000</td>
      <td>2.75</td>
      <td>680.0</td>
    </tr>
    <tr>
      <th>89</th>
      <td>1</td>
      <td>12.08</td>
      <td>1.33</td>
      <td>2.30</td>
      <td>23.6</td>
      <td>70.0</td>
      <td>2.20</td>
      <td>1.59</td>
      <td>0.42</td>
      <td>1.38</td>
      <td>1.740000</td>
      <td>1.070</td>
      <td>3.21</td>
      <td>625.0</td>
    </tr>
    <tr>
      <th>90</th>
      <td>1</td>
      <td>12.08</td>
      <td>1.83</td>
      <td>2.32</td>
      <td>18.5</td>
      <td>81.0</td>
      <td>1.60</td>
      <td>1.50</td>
      <td>0.52</td>
      <td>1.64</td>
      <td>2.400000</td>
      <td>1.080</td>
      <td>2.27</td>
      <td>480.0</td>
    </tr>
    <tr>
      <th>91</th>
      <td>1</td>
      <td>12.00</td>
      <td>1.51</td>
      <td>2.42</td>
      <td>22.0</td>
      <td>86.0</td>
      <td>1.45</td>
      <td>1.25</td>
      <td>0.50</td>
      <td>1.63</td>
      <td>3.600000</td>
      <td>1.050</td>
      <td>2.65</td>
      <td>450.0</td>
    </tr>
    <tr>
      <th>92</th>
      <td>1</td>
      <td>12.69</td>
      <td>1.53</td>
      <td>2.26</td>
      <td>20.7</td>
      <td>80.0</td>
      <td>1.38</td>
      <td>1.46</td>
      <td>0.58</td>
      <td>1.62</td>
      <td>3.050000</td>
      <td>0.960</td>
      <td>2.06</td>
      <td>495.0</td>
    </tr>
    <tr>
      <th>93</th>
      <td>1</td>
      <td>12.29</td>
      <td>2.83</td>
      <td>2.22</td>
      <td>18.0</td>
      <td>88.0</td>
      <td>2.45</td>
      <td>2.25</td>
      <td>0.25</td>
      <td>1.99</td>
      <td>2.150000</td>
      <td>1.150</td>
      <td>3.30</td>
      <td>290.0</td>
    </tr>
    <tr>
      <th>94</th>
      <td>1</td>
      <td>11.62</td>
      <td>1.99</td>
      <td>2.28</td>
      <td>18.0</td>
      <td>98.0</td>
      <td>3.02</td>
      <td>2.26</td>
      <td>0.17</td>
      <td>1.35</td>
      <td>3.250000</td>
      <td>1.160</td>
      <td>2.96</td>
      <td>345.0</td>
    </tr>
    <tr>
      <th>95</th>
      <td>1</td>
      <td>12.47</td>
      <td>1.52</td>
      <td>2.20</td>
      <td>19.0</td>
      <td>162.0</td>
      <td>2.50</td>
      <td>2.27</td>
      <td>0.32</td>
      <td>3.28</td>
      <td>2.600000</td>
      <td>1.160</td>
      <td>2.63</td>
      <td>937.0</td>
    </tr>
    <tr>
      <th>96</th>
      <td>1</td>
      <td>11.81</td>
      <td>2.12</td>
      <td>2.74</td>
      <td>21.5</td>
      <td>134.0</td>
      <td>1.60</td>
      <td>0.99</td>
      <td>0.14</td>
      <td>1.56</td>
      <td>2.500000</td>
      <td>0.950</td>
      <td>2.26</td>
      <td>625.0</td>
    </tr>
    <tr>
      <th>97</th>
      <td>1</td>
      <td>12.29</td>
      <td>1.41</td>
      <td>1.98</td>
      <td>16.0</td>
      <td>85.0</td>
      <td>2.55</td>
      <td>2.50</td>
      <td>0.29</td>
      <td>1.77</td>
      <td>2.900000</td>
      <td>1.230</td>
      <td>2.74</td>
      <td>428.0</td>
    </tr>
    <tr>
      <th>98</th>
      <td>1</td>
      <td>12.37</td>
      <td>1.07</td>
      <td>2.10</td>
      <td>18.5</td>
      <td>88.0</td>
      <td>3.52</td>
      <td>3.75</td>
      <td>0.24</td>
      <td>1.95</td>
      <td>4.500000</td>
      <td>1.040</td>
      <td>2.77</td>
      <td>660.0</td>
    </tr>
    <tr>
      <th>99</th>
      <td>1</td>
      <td>12.29</td>
      <td>3.17</td>
      <td>2.21</td>
      <td>18.0</td>
      <td>88.0</td>
      <td>2.85</td>
      <td>2.99</td>
      <td>0.45</td>
      <td>2.81</td>
      <td>2.300000</td>
      <td>1.420</td>
      <td>2.83</td>
      <td>406.0</td>
    </tr>
    <tr>
      <th>100</th>
      <td>1</td>
      <td>12.08</td>
      <td>2.08</td>
      <td>1.70</td>
      <td>17.5</td>
      <td>97.0</td>
      <td>2.23</td>
      <td>2.17</td>
      <td>0.26</td>
      <td>1.40</td>
      <td>3.300000</td>
      <td>1.270</td>
      <td>2.96</td>
      <td>710.0</td>
    </tr>
    <tr>
      <th>101</th>
      <td>1</td>
      <td>12.60</td>
      <td>1.34</td>
      <td>1.90</td>
      <td>18.5</td>
      <td>88.0</td>
      <td>1.45</td>
      <td>1.36</td>
      <td>0.29</td>
      <td>1.35</td>
      <td>2.450000</td>
      <td>1.040</td>
      <td>2.77</td>
      <td>562.0</td>
    </tr>
    <tr>
      <th>102</th>
      <td>1</td>
      <td>12.34</td>
      <td>2.45</td>
      <td>2.46</td>
      <td>21.0</td>
      <td>98.0</td>
      <td>2.56</td>
      <td>2.11</td>
      <td>0.34</td>
      <td>1.31</td>
      <td>2.800000</td>
      <td>0.800</td>
      <td>3.38</td>
      <td>438.0</td>
    </tr>
    <tr>
      <th>103</th>
      <td>1</td>
      <td>11.82</td>
      <td>1.72</td>
      <td>1.88</td>
      <td>19.5</td>
      <td>86.0</td>
      <td>2.50</td>
      <td>1.64</td>
      <td>0.37</td>
      <td>1.42</td>
      <td>2.060000</td>
      <td>0.940</td>
      <td>2.44</td>
      <td>415.0</td>
    </tr>
    <tr>
      <th>104</th>
      <td>1</td>
      <td>12.51</td>
      <td>1.73</td>
      <td>1.98</td>
      <td>20.5</td>
      <td>85.0</td>
      <td>2.20</td>
      <td>1.92</td>
      <td>0.32</td>
      <td>1.48</td>
      <td>2.940000</td>
      <td>1.040</td>
      <td>3.57</td>
      <td>672.0</td>
    </tr>
    <tr>
      <th>105</th>
      <td>1</td>
      <td>12.42</td>
      <td>2.55</td>
      <td>2.27</td>
      <td>22.0</td>
      <td>90.0</td>
      <td>1.68</td>
      <td>1.84</td>
      <td>0.66</td>
      <td>1.42</td>
      <td>2.700000</td>
      <td>0.860</td>
      <td>3.30</td>
      <td>315.0</td>
    </tr>
    <tr>
      <th>106</th>
      <td>1</td>
      <td>12.25</td>
      <td>1.73</td>
      <td>2.12</td>
      <td>19.0</td>
      <td>80.0</td>
      <td>1.65</td>
      <td>2.03</td>
      <td>0.37</td>
      <td>1.63</td>
      <td>3.400000</td>
      <td>1.000</td>
      <td>3.17</td>
      <td>510.0</td>
    </tr>
    <tr>
      <th>107</th>
      <td>1</td>
      <td>12.72</td>
      <td>1.75</td>
      <td>2.28</td>
      <td>22.5</td>
      <td>84.0</td>
      <td>1.38</td>
      <td>1.76</td>
      <td>0.48</td>
      <td>1.63</td>
      <td>3.300000</td>
      <td>0.880</td>
      <td>2.42</td>
      <td>488.0</td>
    </tr>
    <tr>
      <th>108</th>
      <td>1</td>
      <td>12.22</td>
      <td>1.29</td>
      <td>1.94</td>
      <td>19.0</td>
      <td>92.0</td>
      <td>2.36</td>
      <td>2.04</td>
      <td>0.39</td>
      <td>2.08</td>
      <td>2.700000</td>
      <td>0.860</td>
      <td>3.02</td>
      <td>312.0</td>
    </tr>
    <tr>
      <th>109</th>
      <td>1</td>
      <td>11.61</td>
      <td>1.35</td>
      <td>2.70</td>
      <td>20.0</td>
      <td>94.0</td>
      <td>2.74</td>
      <td>2.92</td>
      <td>0.29</td>
      <td>2.49</td>
      <td>2.650000</td>
      <td>0.960</td>
      <td>3.26</td>
      <td>680.0</td>
    </tr>
    <tr>
      <th>110</th>
      <td>1</td>
      <td>11.46</td>
      <td>3.74</td>
      <td>1.82</td>
      <td>19.5</td>
      <td>107.0</td>
      <td>3.18</td>
      <td>2.58</td>
      <td>0.24</td>
      <td>3.58</td>
      <td>2.900000</td>
      <td>0.750</td>
      <td>2.81</td>
      <td>562.0</td>
    </tr>
    <tr>
      <th>111</th>
      <td>1</td>
      <td>12.52</td>
      <td>2.43</td>
      <td>2.17</td>
      <td>21.0</td>
      <td>88.0</td>
      <td>2.55</td>
      <td>2.27</td>
      <td>0.26</td>
      <td>1.22</td>
      <td>2.000000</td>
      <td>0.900</td>
      <td>2.78</td>
      <td>325.0</td>
    </tr>
    <tr>
      <th>112</th>
      <td>1</td>
      <td>11.76</td>
      <td>2.68</td>
      <td>2.92</td>
      <td>20.0</td>
      <td>103.0</td>
      <td>1.75</td>
      <td>2.03</td>
      <td>0.60</td>
      <td>1.05</td>
      <td>3.800000</td>
      <td>1.230</td>
      <td>2.50</td>
      <td>607.0</td>
    </tr>
    <tr>
      <th>113</th>
      <td>1</td>
      <td>11.41</td>
      <td>0.74</td>
      <td>2.50</td>
      <td>21.0</td>
      <td>88.0</td>
      <td>2.48</td>
      <td>2.01</td>
      <td>0.42</td>
      <td>1.44</td>
      <td>3.080000</td>
      <td>1.100</td>
      <td>2.31</td>
      <td>434.0</td>
    </tr>
    <tr>
      <th>114</th>
      <td>1</td>
      <td>12.08</td>
      <td>1.39</td>
      <td>2.50</td>
      <td>22.5</td>
      <td>84.0</td>
      <td>2.56</td>
      <td>2.29</td>
      <td>0.43</td>
      <td>1.04</td>
      <td>2.900000</td>
      <td>0.930</td>
      <td>3.19</td>
      <td>385.0</td>
    </tr>
    <tr>
      <th>115</th>
      <td>1</td>
      <td>11.03</td>
      <td>1.51</td>
      <td>2.20</td>
      <td>21.5</td>
      <td>85.0</td>
      <td>2.46</td>
      <td>2.17</td>
      <td>0.52</td>
      <td>2.01</td>
      <td>1.900000</td>
      <td>1.710</td>
      <td>2.87</td>
      <td>407.0</td>
    </tr>
    <tr>
      <th>116</th>
      <td>1</td>
      <td>11.82</td>
      <td>1.47</td>
      <td>1.99</td>
      <td>20.8</td>
      <td>86.0</td>
      <td>1.98</td>
      <td>1.60</td>
      <td>0.30</td>
      <td>1.53</td>
      <td>1.950000</td>
      <td>0.950</td>
      <td>3.33</td>
      <td>495.0</td>
    </tr>
    <tr>
      <th>117</th>
      <td>1</td>
      <td>12.42</td>
      <td>1.61</td>
      <td>2.19</td>
      <td>22.5</td>
      <td>108.0</td>
      <td>2.00</td>
      <td>2.09</td>
      <td>0.34</td>
      <td>1.61</td>
      <td>2.060000</td>
      <td>1.060</td>
      <td>2.96</td>
      <td>345.0</td>
    </tr>
    <tr>
      <th>118</th>
      <td>1</td>
      <td>12.77</td>
      <td>3.43</td>
      <td>1.98</td>
      <td>16.0</td>
      <td>80.0</td>
      <td>1.63</td>
      <td>1.25</td>
      <td>0.43</td>
      <td>0.83</td>
      <td>3.400000</td>
      <td>0.700</td>
      <td>2.12</td>
      <td>372.0</td>
    </tr>
    <tr>
      <th>119</th>
      <td>1</td>
      <td>12.00</td>
      <td>3.43</td>
      <td>2.00</td>
      <td>19.0</td>
      <td>87.0</td>
      <td>2.00</td>
      <td>1.64</td>
      <td>0.37</td>
      <td>1.87</td>
      <td>1.280000</td>
      <td>0.930</td>
      <td>3.05</td>
      <td>564.0</td>
    </tr>
    <tr>
      <th>120</th>
      <td>1</td>
      <td>11.45</td>
      <td>2.40</td>
      <td>2.42</td>
      <td>20.0</td>
      <td>96.0</td>
      <td>2.90</td>
      <td>2.79</td>
      <td>0.32</td>
      <td>1.83</td>
      <td>3.250000</td>
      <td>0.800</td>
      <td>3.39</td>
      <td>625.0</td>
    </tr>
    <tr>
      <th>121</th>
      <td>1</td>
      <td>11.56</td>
      <td>2.05</td>
      <td>3.23</td>
      <td>28.5</td>
      <td>119.0</td>
      <td>3.18</td>
      <td>5.08</td>
      <td>0.47</td>
      <td>1.87</td>
      <td>6.000000</td>
      <td>0.930</td>
      <td>3.69</td>
      <td>465.0</td>
    </tr>
    <tr>
      <th>122</th>
      <td>1</td>
      <td>12.42</td>
      <td>4.43</td>
      <td>2.73</td>
      <td>26.5</td>
      <td>102.0</td>
      <td>2.20</td>
      <td>2.13</td>
      <td>0.43</td>
      <td>1.71</td>
      <td>2.080000</td>
      <td>0.920</td>
      <td>3.12</td>
      <td>365.0</td>
    </tr>
    <tr>
      <th>123</th>
      <td>1</td>
      <td>13.05</td>
      <td>5.80</td>
      <td>2.13</td>
      <td>21.5</td>
      <td>86.0</td>
      <td>2.62</td>
      <td>2.65</td>
      <td>0.30</td>
      <td>2.01</td>
      <td>2.600000</td>
      <td>0.730</td>
      <td>3.10</td>
      <td>380.0</td>
    </tr>
    <tr>
      <th>124</th>
      <td>1</td>
      <td>11.87</td>
      <td>4.31</td>
      <td>2.39</td>
      <td>21.0</td>
      <td>82.0</td>
      <td>2.86</td>
      <td>3.03</td>
      <td>0.21</td>
      <td>2.91</td>
      <td>2.800000</td>
      <td>0.750</td>
      <td>3.64</td>
      <td>380.0</td>
    </tr>
    <tr>
      <th>125</th>
      <td>1</td>
      <td>12.07</td>
      <td>2.16</td>
      <td>2.17</td>
      <td>21.0</td>
      <td>85.0</td>
      <td>2.60</td>
      <td>2.65</td>
      <td>0.37</td>
      <td>1.35</td>
      <td>2.760000</td>
      <td>0.860</td>
      <td>3.28</td>
      <td>378.0</td>
    </tr>
    <tr>
      <th>126</th>
      <td>1</td>
      <td>12.43</td>
      <td>1.53</td>
      <td>2.29</td>
      <td>21.5</td>
      <td>86.0</td>
      <td>2.74</td>
      <td>3.15</td>
      <td>0.39</td>
      <td>1.77</td>
      <td>3.940000</td>
      <td>0.690</td>
      <td>2.84</td>
      <td>352.0</td>
    </tr>
    <tr>
      <th>127</th>
      <td>1</td>
      <td>11.79</td>
      <td>2.13</td>
      <td>2.78</td>
      <td>28.5</td>
      <td>92.0</td>
      <td>2.13</td>
      <td>2.24</td>
      <td>0.58</td>
      <td>1.76</td>
      <td>3.000000</td>
      <td>0.970</td>
      <td>2.44</td>
      <td>466.0</td>
    </tr>
    <tr>
      <th>128</th>
      <td>1</td>
      <td>12.37</td>
      <td>1.63</td>
      <td>2.30</td>
      <td>24.5</td>
      <td>88.0</td>
      <td>2.22</td>
      <td>2.45</td>
      <td>0.40</td>
      <td>1.90</td>
      <td>2.120000</td>
      <td>0.890</td>
      <td>2.78</td>
      <td>342.0</td>
    </tr>
    <tr>
      <th>129</th>
      <td>1</td>
      <td>12.04</td>
      <td>4.30</td>
      <td>2.38</td>
      <td>22.0</td>
      <td>80.0</td>
      <td>2.10</td>
      <td>1.75</td>
      <td>0.42</td>
      <td>1.35</td>
      <td>2.600000</td>
      <td>0.790</td>
      <td>2.57</td>
      <td>580.0</td>
    </tr>
    <tr>
      <th>130</th>
      <td>2</td>
      <td>12.86</td>
      <td>1.35</td>
      <td>2.32</td>
      <td>18.0</td>
      <td>122.0</td>
      <td>1.51</td>
      <td>1.25</td>
      <td>0.21</td>
      <td>0.94</td>
      <td>4.100000</td>
      <td>0.760</td>
      <td>1.29</td>
      <td>630.0</td>
    </tr>
    <tr>
      <th>131</th>
      <td>2</td>
      <td>12.88</td>
      <td>2.99</td>
      <td>2.40</td>
      <td>20.0</td>
      <td>104.0</td>
      <td>1.30</td>
      <td>1.22</td>
      <td>0.24</td>
      <td>0.83</td>
      <td>5.400000</td>
      <td>0.740</td>
      <td>1.42</td>
      <td>530.0</td>
    </tr>
    <tr>
      <th>132</th>
      <td>2</td>
      <td>12.81</td>
      <td>2.31</td>
      <td>2.40</td>
      <td>24.0</td>
      <td>98.0</td>
      <td>1.15</td>
      <td>1.09</td>
      <td>0.27</td>
      <td>0.83</td>
      <td>5.700000</td>
      <td>0.660</td>
      <td>1.36</td>
      <td>560.0</td>
    </tr>
    <tr>
      <th>133</th>
      <td>2</td>
      <td>12.70</td>
      <td>3.55</td>
      <td>2.36</td>
      <td>21.5</td>
      <td>106.0</td>
      <td>1.70</td>
      <td>1.20</td>
      <td>0.17</td>
      <td>0.84</td>
      <td>5.000000</td>
      <td>0.780</td>
      <td>1.29</td>
      <td>600.0</td>
    </tr>
    <tr>
      <th>134</th>
      <td>2</td>
      <td>12.51</td>
      <td>1.24</td>
      <td>2.25</td>
      <td>17.5</td>
      <td>85.0</td>
      <td>2.00</td>
      <td>0.58</td>
      <td>0.60</td>
      <td>1.25</td>
      <td>5.450000</td>
      <td>0.750</td>
      <td>1.51</td>
      <td>650.0</td>
    </tr>
    <tr>
      <th>135</th>
      <td>2</td>
      <td>12.60</td>
      <td>2.46</td>
      <td>2.20</td>
      <td>18.5</td>
      <td>94.0</td>
      <td>1.62</td>
      <td>0.66</td>
      <td>0.63</td>
      <td>0.94</td>
      <td>7.100000</td>
      <td>0.730</td>
      <td>1.58</td>
      <td>695.0</td>
    </tr>
    <tr>
      <th>136</th>
      <td>2</td>
      <td>12.25</td>
      <td>4.72</td>
      <td>2.54</td>
      <td>21.0</td>
      <td>89.0</td>
      <td>1.38</td>
      <td>0.47</td>
      <td>0.53</td>
      <td>0.80</td>
      <td>3.850000</td>
      <td>0.750</td>
      <td>1.27</td>
      <td>720.0</td>
    </tr>
    <tr>
      <th>137</th>
      <td>2</td>
      <td>12.53</td>
      <td>5.51</td>
      <td>2.64</td>
      <td>25.0</td>
      <td>96.0</td>
      <td>1.79</td>
      <td>0.60</td>
      <td>0.63</td>
      <td>1.10</td>
      <td>5.000000</td>
      <td>0.820</td>
      <td>1.69</td>
      <td>515.0</td>
    </tr>
    <tr>
      <th>138</th>
      <td>2</td>
      <td>13.49</td>
      <td>3.59</td>
      <td>2.19</td>
      <td>19.5</td>
      <td>88.0</td>
      <td>1.62</td>
      <td>0.48</td>
      <td>0.58</td>
      <td>0.88</td>
      <td>5.700000</td>
      <td>0.810</td>
      <td>1.82</td>
      <td>580.0</td>
    </tr>
    <tr>
      <th>139</th>
      <td>2</td>
      <td>12.84</td>
      <td>2.96</td>
      <td>2.61</td>
      <td>24.0</td>
      <td>101.0</td>
      <td>2.32</td>
      <td>0.60</td>
      <td>0.53</td>
      <td>0.81</td>
      <td>4.920000</td>
      <td>0.890</td>
      <td>2.15</td>
      <td>590.0</td>
    </tr>
    <tr>
      <th>140</th>
      <td>2</td>
      <td>12.93</td>
      <td>2.81</td>
      <td>2.70</td>
      <td>21.0</td>
      <td>96.0</td>
      <td>1.54</td>
      <td>0.50</td>
      <td>0.53</td>
      <td>0.75</td>
      <td>4.600000</td>
      <td>0.770</td>
      <td>2.31</td>
      <td>600.0</td>
    </tr>
    <tr>
      <th>141</th>
      <td>2</td>
      <td>13.36</td>
      <td>2.56</td>
      <td>2.35</td>
      <td>20.0</td>
      <td>89.0</td>
      <td>1.40</td>
      <td>0.50</td>
      <td>0.37</td>
      <td>0.64</td>
      <td>5.600000</td>
      <td>0.700</td>
      <td>2.47</td>
      <td>780.0</td>
    </tr>
    <tr>
      <th>142</th>
      <td>2</td>
      <td>13.52</td>
      <td>3.17</td>
      <td>2.72</td>
      <td>23.5</td>
      <td>97.0</td>
      <td>1.55</td>
      <td>0.52</td>
      <td>0.50</td>
      <td>0.55</td>
      <td>4.350000</td>
      <td>0.890</td>
      <td>2.06</td>
      <td>520.0</td>
    </tr>
    <tr>
      <th>143</th>
      <td>2</td>
      <td>13.62</td>
      <td>4.95</td>
      <td>2.35</td>
      <td>20.0</td>
      <td>92.0</td>
      <td>2.00</td>
      <td>0.80</td>
      <td>0.47</td>
      <td>1.02</td>
      <td>4.400000</td>
      <td>0.910</td>
      <td>2.05</td>
      <td>550.0</td>
    </tr>
    <tr>
      <th>144</th>
      <td>2</td>
      <td>12.25</td>
      <td>3.88</td>
      <td>2.20</td>
      <td>18.5</td>
      <td>112.0</td>
      <td>1.38</td>
      <td>0.78</td>
      <td>0.29</td>
      <td>1.14</td>
      <td>8.210000</td>
      <td>0.650</td>
      <td>2.00</td>
      <td>855.0</td>
    </tr>
    <tr>
      <th>145</th>
      <td>2</td>
      <td>13.16</td>
      <td>3.57</td>
      <td>2.15</td>
      <td>21.0</td>
      <td>102.0</td>
      <td>1.50</td>
      <td>0.55</td>
      <td>0.43</td>
      <td>1.30</td>
      <td>4.000000</td>
      <td>0.600</td>
      <td>1.68</td>
      <td>830.0</td>
    </tr>
    <tr>
      <th>146</th>
      <td>2</td>
      <td>13.88</td>
      <td>5.04</td>
      <td>2.23</td>
      <td>20.0</td>
      <td>80.0</td>
      <td>0.98</td>
      <td>0.34</td>
      <td>0.40</td>
      <td>0.68</td>
      <td>4.900000</td>
      <td>0.580</td>
      <td>1.33</td>
      <td>415.0</td>
    </tr>
    <tr>
      <th>147</th>
      <td>2</td>
      <td>12.87</td>
      <td>4.61</td>
      <td>2.48</td>
      <td>21.5</td>
      <td>86.0</td>
      <td>1.70</td>
      <td>0.65</td>
      <td>0.47</td>
      <td>0.86</td>
      <td>7.650000</td>
      <td>0.540</td>
      <td>1.86</td>
      <td>625.0</td>
    </tr>
    <tr>
      <th>148</th>
      <td>2</td>
      <td>13.32</td>
      <td>3.24</td>
      <td>2.38</td>
      <td>21.5</td>
      <td>92.0</td>
      <td>1.93</td>
      <td>0.76</td>
      <td>0.45</td>
      <td>1.25</td>
      <td>8.420000</td>
      <td>0.550</td>
      <td>1.62</td>
      <td>650.0</td>
    </tr>
    <tr>
      <th>149</th>
      <td>2</td>
      <td>13.08</td>
      <td>3.90</td>
      <td>2.36</td>
      <td>21.5</td>
      <td>113.0</td>
      <td>1.41</td>
      <td>1.39</td>
      <td>0.34</td>
      <td>1.14</td>
      <td>9.400000</td>
      <td>0.570</td>
      <td>1.33</td>
      <td>550.0</td>
    </tr>
    <tr>
      <th>150</th>
      <td>2</td>
      <td>13.50</td>
      <td>3.12</td>
      <td>2.62</td>
      <td>24.0</td>
      <td>123.0</td>
      <td>1.40</td>
      <td>1.57</td>
      <td>0.22</td>
      <td>1.25</td>
      <td>8.600000</td>
      <td>0.590</td>
      <td>1.30</td>
      <td>500.0</td>
    </tr>
    <tr>
      <th>151</th>
      <td>2</td>
      <td>12.79</td>
      <td>2.67</td>
      <td>2.48</td>
      <td>22.0</td>
      <td>112.0</td>
      <td>1.48</td>
      <td>1.36</td>
      <td>0.24</td>
      <td>1.26</td>
      <td>10.800000</td>
      <td>0.480</td>
      <td>1.47</td>
      <td>480.0</td>
    </tr>
    <tr>
      <th>152</th>
      <td>2</td>
      <td>13.11</td>
      <td>1.90</td>
      <td>2.75</td>
      <td>25.5</td>
      <td>116.0</td>
      <td>2.20</td>
      <td>1.28</td>
      <td>0.26</td>
      <td>1.56</td>
      <td>7.100000</td>
      <td>0.610</td>
      <td>1.33</td>
      <td>425.0</td>
    </tr>
    <tr>
      <th>153</th>
      <td>2</td>
      <td>13.23</td>
      <td>3.30</td>
      <td>2.28</td>
      <td>18.5</td>
      <td>98.0</td>
      <td>1.80</td>
      <td>0.83</td>
      <td>0.61</td>
      <td>1.87</td>
      <td>10.520000</td>
      <td>0.560</td>
      <td>1.51</td>
      <td>675.0</td>
    </tr>
    <tr>
      <th>154</th>
      <td>2</td>
      <td>12.58</td>
      <td>1.29</td>
      <td>2.10</td>
      <td>20.0</td>
      <td>103.0</td>
      <td>1.48</td>
      <td>0.58</td>
      <td>0.53</td>
      <td>1.40</td>
      <td>7.600000</td>
      <td>0.580</td>
      <td>1.55</td>
      <td>640.0</td>
    </tr>
    <tr>
      <th>155</th>
      <td>2</td>
      <td>13.17</td>
      <td>5.19</td>
      <td>2.32</td>
      <td>22.0</td>
      <td>93.0</td>
      <td>1.74</td>
      <td>0.63</td>
      <td>0.61</td>
      <td>1.55</td>
      <td>7.900000</td>
      <td>0.600</td>
      <td>1.48</td>
      <td>725.0</td>
    </tr>
    <tr>
      <th>156</th>
      <td>2</td>
      <td>13.84</td>
      <td>4.12</td>
      <td>2.38</td>
      <td>19.5</td>
      <td>89.0</td>
      <td>1.80</td>
      <td>0.83</td>
      <td>0.48</td>
      <td>1.56</td>
      <td>9.010000</td>
      <td>0.570</td>
      <td>1.64</td>
      <td>480.0</td>
    </tr>
    <tr>
      <th>157</th>
      <td>2</td>
      <td>12.45</td>
      <td>3.03</td>
      <td>2.64</td>
      <td>27.0</td>
      <td>97.0</td>
      <td>1.90</td>
      <td>0.58</td>
      <td>0.63</td>
      <td>1.14</td>
      <td>7.500000</td>
      <td>0.670</td>
      <td>1.73</td>
      <td>880.0</td>
    </tr>
    <tr>
      <th>158</th>
      <td>2</td>
      <td>14.34</td>
      <td>1.68</td>
      <td>2.70</td>
      <td>25.0</td>
      <td>98.0</td>
      <td>2.80</td>
      <td>1.31</td>
      <td>0.53</td>
      <td>2.70</td>
      <td>13.000000</td>
      <td>0.570</td>
      <td>1.96</td>
      <td>660.0</td>
    </tr>
    <tr>
      <th>159</th>
      <td>2</td>
      <td>13.48</td>
      <td>1.67</td>
      <td>2.64</td>
      <td>22.5</td>
      <td>89.0</td>
      <td>2.60</td>
      <td>1.10</td>
      <td>0.52</td>
      <td>2.29</td>
      <td>11.750000</td>
      <td>0.570</td>
      <td>1.78</td>
      <td>620.0</td>
    </tr>
    <tr>
      <th>160</th>
      <td>2</td>
      <td>12.36</td>
      <td>3.83</td>
      <td>2.38</td>
      <td>21.0</td>
      <td>88.0</td>
      <td>2.30</td>
      <td>0.92</td>
      <td>0.50</td>
      <td>1.04</td>
      <td>7.650000</td>
      <td>0.560</td>
      <td>1.58</td>
      <td>520.0</td>
    </tr>
    <tr>
      <th>161</th>
      <td>2</td>
      <td>13.69</td>
      <td>3.26</td>
      <td>2.54</td>
      <td>20.0</td>
      <td>107.0</td>
      <td>1.83</td>
      <td>0.56</td>
      <td>0.50</td>
      <td>0.80</td>
      <td>5.880000</td>
      <td>0.960</td>
      <td>1.82</td>
      <td>680.0</td>
    </tr>
    <tr>
      <th>162</th>
      <td>2</td>
      <td>12.85</td>
      <td>3.27</td>
      <td>2.58</td>
      <td>22.0</td>
      <td>106.0</td>
      <td>1.65</td>
      <td>0.60</td>
      <td>0.60</td>
      <td>0.96</td>
      <td>5.580000</td>
      <td>0.870</td>
      <td>2.11</td>
      <td>570.0</td>
    </tr>
    <tr>
      <th>163</th>
      <td>2</td>
      <td>12.96</td>
      <td>3.45</td>
      <td>2.35</td>
      <td>18.5</td>
      <td>106.0</td>
      <td>1.39</td>
      <td>0.70</td>
      <td>0.40</td>
      <td>0.94</td>
      <td>5.280000</td>
      <td>0.680</td>
      <td>1.75</td>
      <td>675.0</td>
    </tr>
    <tr>
      <th>164</th>
      <td>2</td>
      <td>13.78</td>
      <td>2.76</td>
      <td>2.30</td>
      <td>22.0</td>
      <td>90.0</td>
      <td>1.35</td>
      <td>0.68</td>
      <td>0.41</td>
      <td>1.03</td>
      <td>9.580000</td>
      <td>0.700</td>
      <td>1.68</td>
      <td>615.0</td>
    </tr>
    <tr>
      <th>165</th>
      <td>2</td>
      <td>13.73</td>
      <td>4.36</td>
      <td>2.26</td>
      <td>22.5</td>
      <td>88.0</td>
      <td>1.28</td>
      <td>0.47</td>
      <td>0.52</td>
      <td>1.15</td>
      <td>6.620000</td>
      <td>0.780</td>
      <td>1.75</td>
      <td>520.0</td>
    </tr>
    <tr>
      <th>166</th>
      <td>2</td>
      <td>13.45</td>
      <td>3.70</td>
      <td>2.60</td>
      <td>23.0</td>
      <td>111.0</td>
      <td>1.70</td>
      <td>0.92</td>
      <td>0.43</td>
      <td>1.46</td>
      <td>10.680000</td>
      <td>0.850</td>
      <td>1.56</td>
      <td>695.0</td>
    </tr>
    <tr>
      <th>167</th>
      <td>2</td>
      <td>12.82</td>
      <td>3.37</td>
      <td>2.30</td>
      <td>19.5</td>
      <td>88.0</td>
      <td>1.48</td>
      <td>0.66</td>
      <td>0.40</td>
      <td>0.97</td>
      <td>10.260000</td>
      <td>0.720</td>
      <td>1.75</td>
      <td>685.0</td>
    </tr>
    <tr>
      <th>168</th>
      <td>2</td>
      <td>13.58</td>
      <td>2.58</td>
      <td>2.69</td>
      <td>24.5</td>
      <td>105.0</td>
      <td>1.55</td>
      <td>0.84</td>
      <td>0.39</td>
      <td>1.54</td>
      <td>8.660000</td>
      <td>0.740</td>
      <td>1.80</td>
      <td>750.0</td>
    </tr>
    <tr>
      <th>169</th>
      <td>2</td>
      <td>13.40</td>
      <td>4.60</td>
      <td>2.86</td>
      <td>25.0</td>
      <td>112.0</td>
      <td>1.98</td>
      <td>0.96</td>
      <td>0.27</td>
      <td>1.11</td>
      <td>8.500000</td>
      <td>0.670</td>
      <td>1.92</td>
      <td>630.0</td>
    </tr>
    <tr>
      <th>170</th>
      <td>2</td>
      <td>12.20</td>
      <td>3.03</td>
      <td>2.32</td>
      <td>19.0</td>
      <td>96.0</td>
      <td>1.25</td>
      <td>0.49</td>
      <td>0.40</td>
      <td>0.73</td>
      <td>5.500000</td>
      <td>0.660</td>
      <td>1.83</td>
      <td>510.0</td>
    </tr>
    <tr>
      <th>171</th>
      <td>2</td>
      <td>12.77</td>
      <td>2.39</td>
      <td>2.28</td>
      <td>19.5</td>
      <td>86.0</td>
      <td>1.39</td>
      <td>0.51</td>
      <td>0.48</td>
      <td>0.64</td>
      <td>9.899999</td>
      <td>0.570</td>
      <td>1.63</td>
      <td>470.0</td>
    </tr>
    <tr>
      <th>172</th>
      <td>2</td>
      <td>14.16</td>
      <td>2.51</td>
      <td>2.48</td>
      <td>20.0</td>
      <td>91.0</td>
      <td>1.68</td>
      <td>0.70</td>
      <td>0.44</td>
      <td>1.24</td>
      <td>9.700000</td>
      <td>0.620</td>
      <td>1.71</td>
      <td>660.0</td>
    </tr>
    <tr>
      <th>173</th>
      <td>2</td>
      <td>13.71</td>
      <td>5.65</td>
      <td>2.45</td>
      <td>20.5</td>
      <td>95.0</td>
      <td>1.68</td>
      <td>0.61</td>
      <td>0.52</td>
      <td>1.06</td>
      <td>7.700000</td>
      <td>0.640</td>
      <td>1.74</td>
      <td>740.0</td>
    </tr>
    <tr>
      <th>174</th>
      <td>2</td>
      <td>13.40</td>
      <td>3.91</td>
      <td>2.48</td>
      <td>23.0</td>
      <td>102.0</td>
      <td>1.80</td>
      <td>0.75</td>
      <td>0.43</td>
      <td>1.41</td>
      <td>7.300000</td>
      <td>0.700</td>
      <td>1.56</td>
      <td>750.0</td>
    </tr>
    <tr>
      <th>175</th>
      <td>2</td>
      <td>13.27</td>
      <td>4.28</td>
      <td>2.26</td>
      <td>20.0</td>
      <td>120.0</td>
      <td>1.59</td>
      <td>0.69</td>
      <td>0.43</td>
      <td>1.35</td>
      <td>10.200000</td>
      <td>0.590</td>
      <td>1.56</td>
      <td>835.0</td>
    </tr>
    <tr>
      <th>176</th>
      <td>2</td>
      <td>13.17</td>
      <td>2.59</td>
      <td>2.37</td>
      <td>20.0</td>
      <td>120.0</td>
      <td>1.65</td>
      <td>0.68</td>
      <td>0.53</td>
      <td>1.46</td>
      <td>9.300000</td>
      <td>0.600</td>
      <td>1.62</td>
      <td>840.0</td>
    </tr>
    <tr>
      <th>177</th>
      <td>2</td>
      <td>14.13</td>
      <td>4.10</td>
      <td>2.74</td>
      <td>24.5</td>
      <td>96.0</td>
      <td>2.05</td>
      <td>0.76</td>
      <td>0.56</td>
      <td>1.35</td>
      <td>9.200000</td>
      <td>0.610</td>
      <td>1.60</td>
      <td>560.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Let us look at the distribution of the observations across classes
wine_df['class'].value_counts()
</code></pre>
<pre><code>class
1    71
0    59
2    48
Name: count, dtype: int64
</code></pre>
<pre><code class="language-python"># Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=123)
</code></pre>
<pre><code class="language-python"># Fit a NB model, and predict
from sklearn.naive_bayes import GaussianNB
model_nb = GaussianNB()
model_nb.fit(X_train, y_train)
pred = model_nb.predict(X_test)
</code></pre>
<pre><code class="language-python"># Evaluate the model accuracy
y_pred  =  model_nb.predict(X_test)
from sklearn.metrics import confusion_matrix,accuracy_score
print('Accuracy = ', accuracy_score(y_test,y_pred))
</code></pre>
<pre><code>Accuracy =  1.0
</code></pre>
<pre><code class="language-python"># Model evaluation using the classification report and the confusion matrix

ConfusionMatrixDisplay.from_estimator(model_nb, X_test, y_test);
print(classification_report(y_true = y_test, y_pred = pred))

</code></pre>
<pre><code>              precision    recall  f1-score   support

           0       1.00      1.00      1.00         8
           1       1.00      1.00      1.00        11
           2       1.00      1.00      1.00        17

    accuracy                           1.00        36
   macro avg       1.00      1.00      1.00        36
weighted avg       1.00      1.00      1.00        36
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_203_1.png" /></p>
<pre><code class="language-python">wine_df['class'].value_counts()
</code></pre>
<pre><code>class
1    71
0    59
2    48
Name: count, dtype: int64
</code></pre>
<pre><code class="language-python">model_nb.classes_
</code></pre>
<pre><code>array([0, 1, 2])
</code></pre>
<pre><code class="language-python">model_nb.predict_proba(X_test)
</code></pre>
<pre><code>array([[5.93957711e-24, 6.55944182e-07, 9.99999344e-01],
       [1.17999186e-18, 9.99998706e-01, 1.29386818e-06],
       [1.28963191e-29, 2.29833079e-06, 9.99997702e-01],
       [4.24204725e-14, 9.99995948e-01, 4.05248392e-06],
       [1.11923629e-15, 9.99999937e-01, 6.32987582e-08],
       [4.45783147e-18, 3.45360852e-18, 1.00000000e+00],
       [9.96369215e-01, 3.63078524e-03, 1.41680626e-18],
       [8.63917919e-31, 1.78055017e-06, 9.99998219e-01],
       [1.32009669e-20, 1.51909745e-18, 1.00000000e+00],
       [5.37112173e-07, 9.99999463e-01, 6.89392222e-24],
       [2.30365630e-25, 1.06216462e-08, 9.99999989e-01],
       [1.91497199e-16, 4.50596573e-04, 9.99549403e-01],
       [5.09908113e-24, 4.58569400e-16, 1.00000000e+00],
       [9.99999633e-01, 3.66916909e-07, 1.18786467e-29],
       [1.00000000e+00, 1.68595020e-15, 7.56482058e-39],
       [9.82033272e-15, 2.78899850e-05, 9.99972110e-01],
       [3.32691011e-13, 1.00000000e+00, 1.16501960e-15],
       [1.17131670e-10, 1.00000000e+00, 4.47996641e-15],
       [9.99999998e-01, 2.24766861e-09, 3.94855627e-43],
       [2.20594330e-14, 1.00000000e+00, 1.33543221e-21],
       [3.51550512e-16, 4.13954961e-02, 9.58604504e-01],
       [3.55974671e-27, 1.53210419e-11, 1.00000000e+00],
       [4.61982286e-23, 3.19318406e-17, 1.00000000e+00],
       [3.10631399e-21, 6.97558616e-05, 9.99930244e-01],
       [1.01889857e-05, 9.99989811e-01, 1.85439383e-22],
       [3.97166946e-18, 1.95609517e-11, 1.00000000e+00],
       [2.73836673e-25, 1.21551153e-08, 9.99999988e-01],
       [9.82596161e-05, 9.99901740e-01, 5.51859667e-22],
       [1.00000000e+00, 5.58263839e-13, 2.48529450e-33],
       [1.00000000e+00, 2.89393898e-15, 1.82467084e-47],
       [9.93380089e-01, 6.61991141e-03, 5.81320602e-27],
       [9.99179926e-01, 8.20073577e-04, 1.33912510e-17],
       [2.91531470e-22, 1.00486802e-03, 9.98995132e-01],
       [1.25246874e-09, 9.99999999e-01, 7.80563851e-24],
       [2.40587871e-24, 7.15436698e-17, 1.00000000e+00],
       [2.27484144e-06, 9.99997725e-01, 5.43313331e-25]])
</code></pre>
<pre><code class="language-python">
</code></pre>
<h2 id="k-nearest-neighbors">k-Nearest Neighbors</h2>
<p>Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data.
Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.  </p>
<p>The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors.  </p>
<p>Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the weights parameter (weights = 'uniform’, or weights = 'distance’).   </p>
<p>kNN is an easy to use, intuitive algorithm.   </p>
<p>kNN requires variables to be normalized or scaled, else distance calculations can be skewed by numerically large features.  </p>
<p>kNN can be used to predict categories as well as continuous variables.  </p>
<h3 id="knn-classifier">kNN classifier</h3>
<p><strong>Example</strong><br />
Let us consider our college placement dataset again.  We load the data, and perform a train-test split.  We also standard-scale the data <script type="math/tex">((x - mean)/stdev)</script>. </p>
<pre><code class="language-python"># load the data
college = pd.read_csv('collegePlace.csv')
college = pd.get_dummies(college)
</code></pre>
<pre><code class="language-python">college
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Internships</th>
      <th>CGPA</th>
      <th>Hostel</th>
      <th>HistoryOfBacklogs</th>
      <th>PlacedOrNot</th>
      <th>Gender_Female</th>
      <th>Gender_Male</th>
      <th>Stream_Civil</th>
      <th>Stream_Computer Science</th>
      <th>Stream_Electrical</th>
      <th>Stream_Electronics And Communication</th>
      <th>Stream_Information Technology</th>
      <th>Stream_Mechanical</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>22</td>
      <td>1</td>
      <td>8</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21</td>
      <td>0</td>
      <td>7</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>1</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>21</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>22</td>
      <td>0</td>
      <td>8</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2961</th>
      <td>23</td>
      <td>0</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2962</th>
      <td>23</td>
      <td>1</td>
      <td>7</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2963</th>
      <td>22</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2964</th>
      <td>22</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2965</th>
      <td>23</td>
      <td>0</td>
      <td>8</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>False</td>
      <td>True</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
<p>2966 rows × 14 columns</p>
</div>

<pre><code class="language-python"># Test train split

X = college.loc[:, college.columns != 'PlacedOrNot'].values
X = preproc.StandardScaler().fit_transform(X) 
y = college['PlacedOrNot'].values
feature_names = college.loc[:, college.columns != 'PlacedOrNot'].columns

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
</code></pre>
<pre><code class="language-python">pd.DataFrame(X, columns = college.loc[:, college.columns != 'PlacedOrNot'].columns).describe()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Internships</th>
      <th>CGPA</th>
      <th>Hostel</th>
      <th>HistoryOfBacklogs</th>
      <th>Gender_Female</th>
      <th>Gender_Male</th>
      <th>Stream_Civil</th>
      <th>Stream_Computer Science</th>
      <th>Stream_Electrical</th>
      <th>Stream_Electronics And Communication</th>
      <th>Stream_Information Technology</th>
      <th>Stream_Mechanical</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
      <td>2.966000e+03</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>-1.073241e-15</td>
      <td>7.666004e-17</td>
      <td>2.443539e-16</td>
      <td>3.293986e-17</td>
      <td>-4.551690e-17</td>
      <td>1.916501e-17</td>
      <td>-1.916501e-17</td>
      <td>-1.676938e-17</td>
      <td>-5.270378e-17</td>
      <td>6.587972e-18</td>
      <td>-2.395626e-17</td>
      <td>3.114314e-17</td>
      <td>-3.114314e-17</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
      <td>1.000169e+00</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-1.876516e+00</td>
      <td>-9.507732e-01</td>
      <td>-2.143313e+00</td>
      <td>-6.066969e-01</td>
      <td>-4.877463e-01</td>
      <td>-4.454030e-01</td>
      <td>-2.245158e+00</td>
      <td>-3.459303e-01</td>
      <td>-5.952629e-01</td>
      <td>-3.562298e-01</td>
      <td>-4.084089e-01</td>
      <td>-5.511227e-01</td>
      <td>-4.084089e-01</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-3.667516e-01</td>
      <td>-9.507732e-01</td>
      <td>-1.109812e+00</td>
      <td>-6.066969e-01</td>
      <td>-4.877463e-01</td>
      <td>-4.454030e-01</td>
      <td>4.454030e-01</td>
      <td>-3.459303e-01</td>
      <td>-5.952629e-01</td>
      <td>-3.562298e-01</td>
      <td>-4.084089e-01</td>
      <td>-5.511227e-01</td>
      <td>-4.084089e-01</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>-3.667516e-01</td>
      <td>4.004454e-01</td>
      <td>-7.631043e-02</td>
      <td>-6.066969e-01</td>
      <td>-4.877463e-01</td>
      <td>-4.454030e-01</td>
      <td>4.454030e-01</td>
      <td>-3.459303e-01</td>
      <td>-5.952629e-01</td>
      <td>-3.562298e-01</td>
      <td>-4.084089e-01</td>
      <td>-5.511227e-01</td>
      <td>-4.084089e-01</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>3.881306e-01</td>
      <td>4.004454e-01</td>
      <td>9.571907e-01</td>
      <td>1.648269e+00</td>
      <td>-4.877463e-01</td>
      <td>-4.454030e-01</td>
      <td>4.454030e-01</td>
      <td>-3.459303e-01</td>
      <td>1.679930e+00</td>
      <td>-3.562298e-01</td>
      <td>-4.084089e-01</td>
      <td>-5.511227e-01</td>
      <td>-4.084089e-01</td>
    </tr>
    <tr>
      <th>max</th>
      <td>6.427188e+00</td>
      <td>3.102883e+00</td>
      <td>1.990692e+00</td>
      <td>1.648269e+00</td>
      <td>2.050246e+00</td>
      <td>2.245158e+00</td>
      <td>4.454030e-01</td>
      <td>2.890755e+00</td>
      <td>1.679930e+00</td>
      <td>2.807176e+00</td>
      <td>2.448527e+00</td>
      <td>1.814478e+00</td>
      <td>2.448527e+00</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Fit the model using 5 neighbors

from sklearn.neighbors import KNeighborsClassifier
model_knn = KNeighborsClassifier(n_neighbors=5)
model_knn.fit(X_train, y_train)

</code></pre>
<style>#sk-container-id-13 {color: black;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-13" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-13" type="checkbox" checked><label for="sk-estimator-id-13" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>

<pre><code class="language-python"># Perform predictions, and store the results in a variable called 'pred'

pred = model_knn.predict(X_test)
</code></pre>
<pre><code class="language-python"># Check the classification report and the confusion matrix

print(classification_report(y_true = y_test, y_pred = pred))
ConfusionMatrixDisplay.from_estimator(model_knn, X_test, y_test);
</code></pre>
<pre><code>              precision    recall  f1-score   support

           0       0.82      0.85      0.83       265
           1       0.87      0.85      0.86       329

    accuracy                           0.85       594
   macro avg       0.85      0.85      0.85       594
weighted avg       0.85      0.85      0.85       594
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_215_1.png" /></p>
<pre><code class="language-python">pred_prob = model_knn.predict_proba(X_test).round(3)
pred_prob
</code></pre>
<pre><code>array([[0.6, 0.4],
       [0. , 1. ],
       [0. , 1. ],
       ...,
       [0. , 1. ],
       [1. , 0. ],
       [0. , 1. ]])
</code></pre>
<pre><code class="language-python"># Source for code below: https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python
fpr, tpr, thresholds = metrics.roc_curve(y_test, pred_prob[:,1])
roc_auc = metrics.auc(fpr, tpr)
plt.figure(figsize = (9,8))
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
for i, txt in enumerate(thresholds):
    if i in np.arange(1, len(thresholds), 1): # print every n-th point to prevent overplotting:
        plt.annotate(text = round(txt,3), xy = (fpr[i], tpr[i]),  
                     xytext=(-44, 0), textcoords='offset points',
                     arrowprops={'arrowstyle':&quot;simple&quot;}, color='green',fontsize=8)
plt.show()

threshold_dataframe = pd.DataFrame({'fpr':fpr, 'tpr': tpr, 'threshold':thresholds}).sort_values(by='threshold')
threshold_dataframe.head()
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_217_0.png" /></p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fpr</th>
      <th>tpr</th>
      <th>threshold</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.566038</td>
      <td>0.966565</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.309434</td>
      <td>0.908815</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.154717</td>
      <td>0.851064</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.052830</td>
      <td>0.693009</td>
      <td>0.8</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>What is the right number of neighbors to use?</strong><br />
We used 5 as the count of neighbors in the example above.  But how do we know that 5 is the correct number of neighbors?  The fact is, we don't know.  </p>
<p>So we can try several counts of number of neighbors to see what gives us the best result for the metric we are interested in.  Let us consider accuracy as the measure we are looking to improve.  We will build the model several times, each time with a different count of the number of neighbors, and calculate the accuracy each time.  </p>
<pre><code class="language-python"># Loop through n from 1 to 25 to find the best n  

acc = []
for n in range(1,25):
    model_knn = KNeighborsClassifier(n_neighbors=n)
    model_knn.fit(X_train, y_train)
    pred = model_knn.predict(X_test)
    acc.append([n, accuracy_score(y_test, pred)])
sns.lineplot(data = pd.DataFrame(acc, columns=['n','accuracy']), x = 'n', y = 'accuracy')
plt.show()
pd.DataFrame(acc, columns=['n','accuracy']).sort_values(by='accuracy', ascending=False).head()
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_219_0.png" /></p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>n</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0.848485</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>0.840067</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>0.840067</td>
    </tr>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0.836700</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>0.836700</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="knn-regressor">kNN Regressor</h3>
<pre><code class="language-python"># Load data
diamonds = sns.load_dataset(&quot;diamonds&quot;)
</code></pre>
<pre><code class="language-python">diamonds.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>carat</th>
      <th>cut</th>
      <th>color</th>
      <th>clarity</th>
      <th>depth</th>
      <th>table</th>
      <th>price</th>
      <th>x</th>
      <th>y</th>
      <th>z</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.23</td>
      <td>Ideal</td>
      <td>E</td>
      <td>SI2</td>
      <td>61.5</td>
      <td>55.0</td>
      <td>326</td>
      <td>3.95</td>
      <td>3.98</td>
      <td>2.43</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.21</td>
      <td>Premium</td>
      <td>E</td>
      <td>SI1</td>
      <td>59.8</td>
      <td>61.0</td>
      <td>326</td>
      <td>3.89</td>
      <td>3.84</td>
      <td>2.31</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.23</td>
      <td>Good</td>
      <td>E</td>
      <td>VS1</td>
      <td>56.9</td>
      <td>65.0</td>
      <td>327</td>
      <td>4.05</td>
      <td>4.07</td>
      <td>2.31</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.29</td>
      <td>Premium</td>
      <td>I</td>
      <td>VS2</td>
      <td>62.4</td>
      <td>58.0</td>
      <td>334</td>
      <td>4.20</td>
      <td>4.23</td>
      <td>2.63</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.31</td>
      <td>Good</td>
      <td>J</td>
      <td>SI2</td>
      <td>63.3</td>
      <td>58.0</td>
      <td>335</td>
      <td>4.34</td>
      <td>4.35</td>
      <td>2.75</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Get dummy variables
diamonds = pd.get_dummies(diamonds)
</code></pre>
<pre><code class="language-python">diamonds.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>carat</th>
      <th>depth</th>
      <th>table</th>
      <th>price</th>
      <th>x</th>
      <th>y</th>
      <th>z</th>
      <th>cut_Ideal</th>
      <th>cut_Premium</th>
      <th>cut_Very Good</th>
      <th>cut_Good</th>
      <th>cut_Fair</th>
      <th>color_D</th>
      <th>color_E</th>
      <th>color_F</th>
      <th>color_G</th>
      <th>color_H</th>
      <th>color_I</th>
      <th>color_J</th>
      <th>clarity_IF</th>
      <th>clarity_VVS1</th>
      <th>clarity_VVS2</th>
      <th>clarity_VS1</th>
      <th>clarity_VS2</th>
      <th>clarity_SI1</th>
      <th>clarity_SI2</th>
      <th>clarity_I1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.23</td>
      <td>61.5</td>
      <td>55.0</td>
      <td>326</td>
      <td>3.95</td>
      <td>3.98</td>
      <td>2.43</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.21</td>
      <td>59.8</td>
      <td>61.0</td>
      <td>326</td>
      <td>3.89</td>
      <td>3.84</td>
      <td>2.31</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.23</td>
      <td>56.9</td>
      <td>65.0</td>
      <td>327</td>
      <td>4.05</td>
      <td>4.07</td>
      <td>2.31</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.29</td>
      <td>62.4</td>
      <td>58.0</td>
      <td>334</td>
      <td>4.20</td>
      <td>4.23</td>
      <td>2.63</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.31</td>
      <td>63.3</td>
      <td>58.0</td>
      <td>335</td>
      <td>4.34</td>
      <td>4.35</td>
      <td>2.75</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Define X and y as arrays. y is the price column, X is everything else
X = diamonds.loc[:, diamonds.columns != 'price'].values
X = preproc.StandardScaler().fit_transform(X) 
y = diamonds.price.values
</code></pre>
<pre><code class="language-python">X.shape
</code></pre>
<pre><code>(53940, 26)
</code></pre>
<pre><code class="language-python"># Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)
</code></pre>
<pre><code class="language-python"># Fit model
from sklearn.neighbors import KNeighborsRegressor
model_knn_regress = KNeighborsRegressor(n_neighbors=1)
model_knn_regress.fit(X_train, y_train)
</code></pre>
<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-10" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsRegressor(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox" checked><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsRegressor</label><div class="sk-toggleable__content"><pre>KNeighborsRegressor(n_neighbors=1)</pre></div></div></div></div></div>

<pre><code class="language-python"># Evaluate model
y_pred  =  model_knn_regress.predict(X_test)
from sklearn.metrics import mean_absolute_error, mean_squared_error
print('MSE = ', mean_squared_error(y_test,y_pred))
print('RMSE = ', np.sqrt(mean_squared_error(y_test,y_pred)))
print('MAE = ', mean_absolute_error(y_test,y_pred))

</code></pre>
<pre><code>MSE =  819278.694475343
RMSE =  905.1401518413284
MAE =  448.56952169076754
</code></pre>
<pre><code class="language-python"># Evaluate residuals
plt.figure(figsize = (8,8))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.title('Actual vs Predicted Diamond Value\n Closer to red line (identity) means more accurate prediction')
plt.plot( [0,19000],[0,19000], color='red' )
plt.xlabel(&quot;Actual&quot;)
plt.ylabel(&quot;Predicted&quot;);
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_230_0.png" /></p>
<pre><code class="language-python"># R-squared calculation
pd.DataFrame({'actual':y_test, 'predicted':y_pred}).corr()**2
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>actual</th>
      <th>predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>actual</th>
      <td>1.000000</td>
      <td>0.948071</td>
    </tr>
    <tr>
      <th>predicted</th>
      <td>0.948071</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">diamonds.price.describe()
</code></pre>
<pre><code>count    53940.000000
mean      3932.799722
std       3989.439738
min        326.000000
25%        950.000000
50%       2401.000000
75%       5324.250000
max      18823.000000
Name: price, dtype: float64
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<h2 id="k-means-clustering">k-Means Clustering</h2>
<p>k-means clustering is a non-hierarchical approach to clustering.  The goal is to divide the observations into a number of non-overlapping groups, or clusters, in a way that the clusters are as homogenous as possible.  </p>
<p>k stands for the number of clusters the observations are divided into.  There is no natural number of clusters, it is a user provided parameter.  </p>
<p>Homogeneity within the cluster is measured using some measure of dispersion, for example, the sum of Euclidean distances.  </p>
<p>The algorithm is iterative, and roughly works as follows:<br />
1. Select any k data points as cluster centers (the centroid).<br />
2. Assign all observations to the cluster centroid closest to the observations.<br />
3. Recompute the location of the centroids once all data points have been assigned.<br />
4. Repeat steps 2 and 3.<br />
5. Stop when the measure of dispersion stops improving, or a certain number of repetitions have been performed.  </p>
<p><strong>Limitations of k-Means clustering</strong><br />
 - k is chosen manually and the correct value for k may be difficult to know.  (Algorithms, such as the ‘elbow method’, are available to identify an appropriate value of k.)<br />
 - Clusters have no intuitive meaning.<br />
 - You may get different results each time due to dependence on initial values. You can overcome this by running k-means several times and picking the best result.<br />
 - As the count of dimensions increases (say 1000), PCA may need to be used to prevent similarity measures converging to a constant value. <br />
 - Outliers may impact k-means disproportionately.  </p>
<p>In spite of the above, k-means clustering remains a preferred clustering technique given its simplicity, scalability to large data sets, and adaptability to different kinds of data.</p>
<h3 id="k-means-example">k-means - example</h3>
<p>Let us use the Iris dataset to create a classification model.  We will try to cluster the Iris data into 3 clusters using the data in the first four columns.  We would like to see the clusters correspond to the species as members of the same species have similar features.  </p>
<pre><code class="language-python">import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
</code></pre>
<pre><code class="language-python">iris = sm.datasets.get_rdataset('iris').data
</code></pre>
<pre><code class="language-python">iris
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5.4</td>
      <td>3.9</td>
      <td>1.7</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>6</th>
      <td>4.6</td>
      <td>3.4</td>
      <td>1.4</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>7</th>
      <td>5.0</td>
      <td>3.4</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>8</th>
      <td>4.4</td>
      <td>2.9</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>9</th>
      <td>4.9</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>10</th>
      <td>5.4</td>
      <td>3.7</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>11</th>
      <td>4.8</td>
      <td>3.4</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>12</th>
      <td>4.8</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>13</th>
      <td>4.3</td>
      <td>3.0</td>
      <td>1.1</td>
      <td>0.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>14</th>
      <td>5.8</td>
      <td>4.0</td>
      <td>1.2</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>15</th>
      <td>5.7</td>
      <td>4.4</td>
      <td>1.5</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>16</th>
      <td>5.4</td>
      <td>3.9</td>
      <td>1.3</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>17</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>18</th>
      <td>5.7</td>
      <td>3.8</td>
      <td>1.7</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>19</th>
      <td>5.1</td>
      <td>3.8</td>
      <td>1.5</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>20</th>
      <td>5.4</td>
      <td>3.4</td>
      <td>1.7</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>21</th>
      <td>5.1</td>
      <td>3.7</td>
      <td>1.5</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>22</th>
      <td>4.6</td>
      <td>3.6</td>
      <td>1.0</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>23</th>
      <td>5.1</td>
      <td>3.3</td>
      <td>1.7</td>
      <td>0.5</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>24</th>
      <td>4.8</td>
      <td>3.4</td>
      <td>1.9</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>25</th>
      <td>5.0</td>
      <td>3.0</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>26</th>
      <td>5.0</td>
      <td>3.4</td>
      <td>1.6</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>27</th>
      <td>5.2</td>
      <td>3.5</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>28</th>
      <td>5.2</td>
      <td>3.4</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>29</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>30</th>
      <td>4.8</td>
      <td>3.1</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>31</th>
      <td>5.4</td>
      <td>3.4</td>
      <td>1.5</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>32</th>
      <td>5.2</td>
      <td>4.1</td>
      <td>1.5</td>
      <td>0.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>33</th>
      <td>5.5</td>
      <td>4.2</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>34</th>
      <td>4.9</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>35</th>
      <td>5.0</td>
      <td>3.2</td>
      <td>1.2</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>36</th>
      <td>5.5</td>
      <td>3.5</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>37</th>
      <td>4.9</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>38</th>
      <td>4.4</td>
      <td>3.0</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>39</th>
      <td>5.1</td>
      <td>3.4</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>40</th>
      <td>5.0</td>
      <td>3.5</td>
      <td>1.3</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>41</th>
      <td>4.5</td>
      <td>2.3</td>
      <td>1.3</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>42</th>
      <td>4.4</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>43</th>
      <td>5.0</td>
      <td>3.5</td>
      <td>1.6</td>
      <td>0.6</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>44</th>
      <td>5.1</td>
      <td>3.8</td>
      <td>1.9</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>45</th>
      <td>4.8</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>46</th>
      <td>5.1</td>
      <td>3.8</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>47</th>
      <td>4.6</td>
      <td>3.2</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>48</th>
      <td>5.3</td>
      <td>3.7</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>49</th>
      <td>5.0</td>
      <td>3.3</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>50</th>
      <td>7.0</td>
      <td>3.2</td>
      <td>4.7</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>51</th>
      <td>6.4</td>
      <td>3.2</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>52</th>
      <td>6.9</td>
      <td>3.1</td>
      <td>4.9</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>53</th>
      <td>5.5</td>
      <td>2.3</td>
      <td>4.0</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>54</th>
      <td>6.5</td>
      <td>2.8</td>
      <td>4.6</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>55</th>
      <td>5.7</td>
      <td>2.8</td>
      <td>4.5</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>56</th>
      <td>6.3</td>
      <td>3.3</td>
      <td>4.7</td>
      <td>1.6</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>57</th>
      <td>4.9</td>
      <td>2.4</td>
      <td>3.3</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>58</th>
      <td>6.6</td>
      <td>2.9</td>
      <td>4.6</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>59</th>
      <td>5.2</td>
      <td>2.7</td>
      <td>3.9</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>60</th>
      <td>5.0</td>
      <td>2.0</td>
      <td>3.5</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>61</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>4.2</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>62</th>
      <td>6.0</td>
      <td>2.2</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>63</th>
      <td>6.1</td>
      <td>2.9</td>
      <td>4.7</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>64</th>
      <td>5.6</td>
      <td>2.9</td>
      <td>3.6</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>65</th>
      <td>6.7</td>
      <td>3.1</td>
      <td>4.4</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>66</th>
      <td>5.6</td>
      <td>3.0</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>67</th>
      <td>5.8</td>
      <td>2.7</td>
      <td>4.1</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>68</th>
      <td>6.2</td>
      <td>2.2</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>69</th>
      <td>5.6</td>
      <td>2.5</td>
      <td>3.9</td>
      <td>1.1</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>70</th>
      <td>5.9</td>
      <td>3.2</td>
      <td>4.8</td>
      <td>1.8</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>71</th>
      <td>6.1</td>
      <td>2.8</td>
      <td>4.0</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>72</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>4.9</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>73</th>
      <td>6.1</td>
      <td>2.8</td>
      <td>4.7</td>
      <td>1.2</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>74</th>
      <td>6.4</td>
      <td>2.9</td>
      <td>4.3</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>75</th>
      <td>6.6</td>
      <td>3.0</td>
      <td>4.4</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>76</th>
      <td>6.8</td>
      <td>2.8</td>
      <td>4.8</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>77</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>1.7</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>78</th>
      <td>6.0</td>
      <td>2.9</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>79</th>
      <td>5.7</td>
      <td>2.6</td>
      <td>3.5</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>80</th>
      <td>5.5</td>
      <td>2.4</td>
      <td>3.8</td>
      <td>1.1</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>81</th>
      <td>5.5</td>
      <td>2.4</td>
      <td>3.7</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>82</th>
      <td>5.8</td>
      <td>2.7</td>
      <td>3.9</td>
      <td>1.2</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>83</th>
      <td>6.0</td>
      <td>2.7</td>
      <td>5.1</td>
      <td>1.6</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>84</th>
      <td>5.4</td>
      <td>3.0</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>85</th>
      <td>6.0</td>
      <td>3.4</td>
      <td>4.5</td>
      <td>1.6</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>86</th>
      <td>6.7</td>
      <td>3.1</td>
      <td>4.7</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>87</th>
      <td>6.3</td>
      <td>2.3</td>
      <td>4.4</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>88</th>
      <td>5.6</td>
      <td>3.0</td>
      <td>4.1</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>89</th>
      <td>5.5</td>
      <td>2.5</td>
      <td>4.0</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>90</th>
      <td>5.5</td>
      <td>2.6</td>
      <td>4.4</td>
      <td>1.2</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>91</th>
      <td>6.1</td>
      <td>3.0</td>
      <td>4.6</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>92</th>
      <td>5.8</td>
      <td>2.6</td>
      <td>4.0</td>
      <td>1.2</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>93</th>
      <td>5.0</td>
      <td>2.3</td>
      <td>3.3</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>94</th>
      <td>5.6</td>
      <td>2.7</td>
      <td>4.2</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>95</th>
      <td>5.7</td>
      <td>3.0</td>
      <td>4.2</td>
      <td>1.2</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>96</th>
      <td>5.7</td>
      <td>2.9</td>
      <td>4.2</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>97</th>
      <td>6.2</td>
      <td>2.9</td>
      <td>4.3</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>98</th>
      <td>5.1</td>
      <td>2.5</td>
      <td>3.0</td>
      <td>1.1</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>99</th>
      <td>5.7</td>
      <td>2.8</td>
      <td>4.1</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>100</th>
      <td>6.3</td>
      <td>3.3</td>
      <td>6.0</td>
      <td>2.5</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>101</th>
      <td>5.8</td>
      <td>2.7</td>
      <td>5.1</td>
      <td>1.9</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>102</th>
      <td>7.1</td>
      <td>3.0</td>
      <td>5.9</td>
      <td>2.1</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>103</th>
      <td>6.3</td>
      <td>2.9</td>
      <td>5.6</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>104</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.8</td>
      <td>2.2</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>105</th>
      <td>7.6</td>
      <td>3.0</td>
      <td>6.6</td>
      <td>2.1</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>106</th>
      <td>4.9</td>
      <td>2.5</td>
      <td>4.5</td>
      <td>1.7</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>107</th>
      <td>7.3</td>
      <td>2.9</td>
      <td>6.3</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>108</th>
      <td>6.7</td>
      <td>2.5</td>
      <td>5.8</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>109</th>
      <td>7.2</td>
      <td>3.6</td>
      <td>6.1</td>
      <td>2.5</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>110</th>
      <td>6.5</td>
      <td>3.2</td>
      <td>5.1</td>
      <td>2.0</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>111</th>
      <td>6.4</td>
      <td>2.7</td>
      <td>5.3</td>
      <td>1.9</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>112</th>
      <td>6.8</td>
      <td>3.0</td>
      <td>5.5</td>
      <td>2.1</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>113</th>
      <td>5.7</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>2.0</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>114</th>
      <td>5.8</td>
      <td>2.8</td>
      <td>5.1</td>
      <td>2.4</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>115</th>
      <td>6.4</td>
      <td>3.2</td>
      <td>5.3</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>116</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.5</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>117</th>
      <td>7.7</td>
      <td>3.8</td>
      <td>6.7</td>
      <td>2.2</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>118</th>
      <td>7.7</td>
      <td>2.6</td>
      <td>6.9</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>119</th>
      <td>6.0</td>
      <td>2.2</td>
      <td>5.0</td>
      <td>1.5</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>120</th>
      <td>6.9</td>
      <td>3.2</td>
      <td>5.7</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>121</th>
      <td>5.6</td>
      <td>2.8</td>
      <td>4.9</td>
      <td>2.0</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>122</th>
      <td>7.7</td>
      <td>2.8</td>
      <td>6.7</td>
      <td>2.0</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>123</th>
      <td>6.3</td>
      <td>2.7</td>
      <td>4.9</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>124</th>
      <td>6.7</td>
      <td>3.3</td>
      <td>5.7</td>
      <td>2.1</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>125</th>
      <td>7.2</td>
      <td>3.2</td>
      <td>6.0</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>126</th>
      <td>6.2</td>
      <td>2.8</td>
      <td>4.8</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>127</th>
      <td>6.1</td>
      <td>3.0</td>
      <td>4.9</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>128</th>
      <td>6.4</td>
      <td>2.8</td>
      <td>5.6</td>
      <td>2.1</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>129</th>
      <td>7.2</td>
      <td>3.0</td>
      <td>5.8</td>
      <td>1.6</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>130</th>
      <td>7.4</td>
      <td>2.8</td>
      <td>6.1</td>
      <td>1.9</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>131</th>
      <td>7.9</td>
      <td>3.8</td>
      <td>6.4</td>
      <td>2.0</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>132</th>
      <td>6.4</td>
      <td>2.8</td>
      <td>5.6</td>
      <td>2.2</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>133</th>
      <td>6.3</td>
      <td>2.8</td>
      <td>5.1</td>
      <td>1.5</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>134</th>
      <td>6.1</td>
      <td>2.6</td>
      <td>5.6</td>
      <td>1.4</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>135</th>
      <td>7.7</td>
      <td>3.0</td>
      <td>6.1</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>136</th>
      <td>6.3</td>
      <td>3.4</td>
      <td>5.6</td>
      <td>2.4</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>137</th>
      <td>6.4</td>
      <td>3.1</td>
      <td>5.5</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>138</th>
      <td>6.0</td>
      <td>3.0</td>
      <td>4.8</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>139</th>
      <td>6.9</td>
      <td>3.1</td>
      <td>5.4</td>
      <td>2.1</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>140</th>
      <td>6.7</td>
      <td>3.1</td>
      <td>5.6</td>
      <td>2.4</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>141</th>
      <td>6.9</td>
      <td>3.1</td>
      <td>5.1</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>142</th>
      <td>5.8</td>
      <td>2.7</td>
      <td>5.1</td>
      <td>1.9</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>143</th>
      <td>6.8</td>
      <td>3.2</td>
      <td>5.9</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>144</th>
      <td>6.7</td>
      <td>3.3</td>
      <td>5.7</td>
      <td>2.5</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">x = iris.iloc[:, 0:4]
kmeans = KMeans(3, n_init='auto')
clusters = kmeans.fit_predict(x)
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">iris['clusters'] = clusters
</code></pre>
<pre><code class="language-python">df = iris.loc[:,['Species', 'clusters']]
pd.crosstab(index = df['Species'], 
            columns = df['clusters'], margins=True)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>clusters</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>All</th>
    </tr>
    <tr>
      <th>Species</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>setosa</th>
      <td>0</td>
      <td>0</td>
      <td>50</td>
      <td>50</td>
    </tr>
    <tr>
      <th>versicolor</th>
      <td>47</td>
      <td>3</td>
      <td>0</td>
      <td>50</td>
    </tr>
    <tr>
      <th>virginica</th>
      <td>14</td>
      <td>36</td>
      <td>0</td>
      <td>50</td>
    </tr>
    <tr>
      <th>All</th>
      <td>61</td>
      <td>39</td>
      <td>50</td>
      <td>150</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">plt.figure(figsize = (8,6))
sns.scatterplot(x='Sepal.Width', y='Petal.Width', data=iris, 
                hue='Species', style='clusters', 
                markers= {0: &quot;s&quot;, 1: &quot;X&quot;, 2: &quot;P&quot;});
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_247_0.png" /></p>
<p>We run the KMeans algorithm (from scikit learn) on the data  and obtain 3 clusters.</p>
<p>How do the clusters look? </p>
<ul>
<li>All setosa are neatly included in cluster 1.</li>
<li>Versicolor are mostly in cluster 0, but 2 are in a different cluster.</li>
<li>Verginica is spread across two clusters.</li>
</ul>
<pre><code class="language-python">kmeans.fit_predict(x)
</code></pre>
<pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2,
       2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2,
       2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1])
</code></pre>
<pre><code class="language-python">len(kmeans.fit_transform(x))
</code></pre>
<pre><code>150
</code></pre>
<pre><code class="language-python">kmeans.fit_transform(x)[:3]
</code></pre>
<pre><code>array([[0.14135063, 5.03132789, 3.41251117],
       [0.44763825, 5.08750645, 3.38963991],
       [0.4171091 , 5.25229169, 3.56011415]])
</code></pre>
<pre><code class="language-python">x[:3]
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="right-number-of-clusters">Right number of clusters</h3>
<p>kmeans score is a measure of how far the data points are from the cluster centroids, expressed as a negative number.  The closer it is to zero, the better it is.  Of course, if we have the number of clusters equal to the number of observations, the score will be zero as each point will be its own centroid, with a sum of zero.  If we have only one cluster, we will have a large negative score.  </p>
<p>The ideal number of clusters is somewhere when we start getting diminished returns to adding more clusters.  We can run the kmeans algorithm for a range of cluster numbers, and compare the score.  </p>
<p>KMeans works by minimizing the sum of squared distance of each observation to their respective cluster center.  In an extreme situation, all observations would coincide with their centroid center, and the sum of squared distances will be zero.</p>
<p>With sklearn, we can get sum of squared distances of samples to their closest cluster center using _model_name.intertia__.  </p>
<p>The negative of inertia_ is model_name.score(x), where x is the dataset kmeans was fitted on.</p>
<h4 id="elbow-method">Elbow Method</h4>
<p>The elbow method tracks the sum of squares against the number of clusters, and we can make a subjective judgement on the appropriate number of clusters based on graphing the sum of squares as below.  The sum of squares is calculated using the distance between cluster centers and each observation in that cluster.  As an extreme case, when the number of clusters is equal to the number of observations, the sum of squares will be zero.</p>
<pre><code class="language-python">num_clusters = []
score = []
for cluster_count in range(1,15):
    kmeans = KMeans(cluster_count, n_init='auto')
    kmeans.fit(x)
    kmeans.score(x)
    num_clusters.append(cluster_count)
    # score.append(kmeans.score(x)) # score is just the negative of inertia_
    score.append(kmeans.inertia_)

plt.plot(num_clusters, score)
</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x1e7a09c3ad0&gt;]
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_254_1.png" /></p>
<pre><code class="language-python">print(kmeans.score(x))
kmeans.inertia_
</code></pre>
<pre><code>-21.55109126984127





21.55109126984127
</code></pre>
<pre><code class="language-python"># Alternative way of listing labels for the training data
kmeans.labels_
</code></pre>
<pre><code>array([ 1, 11, 11, 11,  1,  7, 11,  1, 11, 11,  1,  1, 11, 11,  7,  7,  7,
        1,  7,  1,  1,  1, 11,  1,  1, 11,  1,  1,  1, 11, 11,  1,  7,  7,
       11, 11,  1,  1, 11,  1,  1, 10, 11,  1,  1, 11,  1, 11,  1,  1,  5,
        5,  5, 12,  5,  6,  5,  9,  5, 12,  9,  6,  0,  5, 12,  5,  6, 12,
        0, 12,  8, 12,  3,  5,  5,  5,  5,  5,  5, 12, 12, 12, 12,  3,  6,
        5,  5,  0,  6, 12,  6,  5, 12,  9,  6,  6,  6,  5,  9,  6, 13,  8,
        2, 13, 13,  4,  6,  4, 13,  2, 13, 13,  2,  8,  8, 13, 13,  4,  4,
        3,  2,  8,  4,  3,  2,  2,  3,  8, 13,  2,  4,  4, 13,  3,  3,  4,
       13, 13,  8,  2,  2,  2,  8,  2,  2, 13,  3, 13, 13,  8])
</code></pre>
<pre><code class="language-python">
</code></pre>
<h4 id="silhouette-plot">Silhouette Plot</h4>
<p>The silhouette plot is a measure of how close each point in one cluster is to points in the neighboring clusters.  It provides a visual way to assess parameters such as the number of clusters visually.  It does so using the silhouette coefficient.</p>
<p>Silhouette coefficient - This measure has a range of [-1, 1].  Higher the score the better, so +1 is the best result.</p>
<p>The silhouette coefficient is calculated individually for every observation in a cluster as follows:  (b - a) / max(a, b). 'b' is the distance between a sample and the nearest cluster that the sample is not a part of.  'a' is the distance between the sample and the cluster it is a part of.  One would expect b - a to be a positive number, but if it is not, then likely the point is misclassified.</p>
<p><code>sklearn.metrics.silhouette_samples(X)</code> - gives the silhouette coefficient for every point in X.<br />
<code>sklearn.metrics.silhouette_score(X)</code> - gives mean of the above.</p>
<p>The silhouette plot gives the mean (ie silhouette_score) as a red vertical line for the entire dataset for all clusters.  Then each cluster is presented as a sideways histogram of the distances of each of the datapoints.  The fatter the representation of a cluster, the more datapoints are included in that cluster. </p>
<p>Negative points on the histogram indicate misclassifications that may be difficult to correct as moving them changes the centroid center.</p>
<pre><code class="language-python"># Source: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

# Generating the sample data from make_blobs
# This particular setting has one distinct cluster and 3 clusters placed close
# together.
# X, y = make_blobs(
#     n_samples=500,
#     n_features=2,
#     centers=4,
#     cluster_std=1,
#     center_box=(-10.0, 10.0),
#     shuffle=True,
#     random_state=1,
# )  # For reproducibility

range_n_clusters = [2, 3, 4, 5, 6]

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(x) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, n_init=&quot;auto&quot;, random_state=10)
    cluster_labels = clusterer.fit_predict(x)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(x, cluster_labels)
    print(
        &quot;For n_clusters =&quot;,
        n_clusters,
        &quot;The average silhouette_score is :&quot;,
        silhouette_avg,
    )

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(x, cluster_labels)

    y_lower = 2
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(
            np.arange(y_lower, y_upper),
            0,
            ith_cluster_silhouette_values,
            facecolor=color,
            edgecolor=color,
            alpha=0.7,
        )

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title(&quot;The silhouette plot for the various clusters.&quot;)
    ax1.set_xlabel(&quot;The silhouette coefficient values&quot;)
    ax1.set_ylabel(&quot;Cluster label&quot;)

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;)

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([ 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(
        np.array(x)[:, 0], np.array(x)[:, 1], marker=&quot;.&quot;, s=30, lw=0, alpha=0.7, c=colors, edgecolor=&quot;k&quot;
    )

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(
        centers[:, 0],
        centers[:, 1],
        marker=&quot;o&quot;,
        c=&quot;white&quot;,
        alpha=1,
        s=200,
        edgecolor=&quot;k&quot;,
    )

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker=&quot;$%d$&quot; % i, alpha=1, s=50, edgecolor=&quot;k&quot;)

    ax2.set_title(&quot;The visualization of the clustered data.&quot;)
    ax2.set_xlabel(&quot;Feature space for the 1st feature&quot;)
    ax2.set_ylabel(&quot;Feature space for the 2nd feature&quot;)

    plt.suptitle(
        &quot;Silhouette analysis for KMeans clustering on sample data with n_clusters = %d&quot;
        % n_clusters,
        fontsize=14,
        fontweight=&quot;bold&quot;,
    )

plt.show()
</code></pre>
<pre><code>For n_clusters = 2 The average silhouette_score is : 0.6810461692117465
For n_clusters = 3 The average silhouette_score is : 0.5511916046195927
For n_clusters = 4 The average silhouette_score is : 0.49535632852885064
For n_clusters = 5 The average silhouette_score is : 0.48989824728439524
For n_clusters = 6 The average silhouette_score is : 0.47711750058213453
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_259_1.png" /></p>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_259_2.png" /></p>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_259_3.png" /></p>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_259_4.png" /></p>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_259_5.png" /></p>
<h2 id="hierarchical-clustering">Hierarchical Clustering</h2>
<p>Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively.  The point of hierarchical clustering is to organize observations that are close together, and separate them out into groups/clusters.  Closeness is generally a measure of distance between observations, the primary measures being Euclidean, Manhattan or Cosine. You have to pick the one that makes sense for your situation.  </p>
<p>For most uses, Euclidean distance (often the default) does a great job.  Cosine distances are more useful when doing natural language analysis.  </p>
<p><strong>Agglomerative Clustering</strong><br />
Agglomerative Clustering is a bottom up approach: each observation starts in its own cluster, and closest clusters are successively merged together. The ‘linkage criteria’ is a parameter passed to the sklearn function for performing the clustering.<br />
 - Single linkage (default) minimizes the distance between the closest observations of pairs of clusters.<br />
 - Ward minimizes the sum of squared differences within all clusters. <br />
 - Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.<br />
 - Average linkage minimizes the average of the distances between all observations of pairs of clusters.  </p>
<p>Agglomerative cluster has a "rich get richer" behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes.  </p>
<p><strong>Example</strong><br />
Recall the mtcars dataset that has numerical information on 32 models of cars.  Let us apply agglomerative clustering to it.  We will first standardize, or rescale the data, to make sure no individual feature overwhelms the other due to its scale.  Then we will run the clustering algorithm, and present the result as a dendrogram.  </p>
<pre><code class="language-python">mtcars = sm.datasets.get_rdataset('mtcars').data
data = mtcars.iloc[:, :]
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data) 
data_scaled = pd.DataFrame(data_scaled, columns=data.columns)
data_scaled.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mpg</th>
      <th>cyl</th>
      <th>disp</th>
      <th>hp</th>
      <th>drat</th>
      <th>wt</th>
      <th>qsec</th>
      <th>vs</th>
      <th>am</th>
      <th>gear</th>
      <th>carb</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.153299</td>
      <td>-0.106668</td>
      <td>-0.579750</td>
      <td>-0.543655</td>
      <td>0.576594</td>
      <td>-0.620167</td>
      <td>-0.789601</td>
      <td>-0.881917</td>
      <td>1.208941</td>
      <td>0.430331</td>
      <td>0.746967</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.153299</td>
      <td>-0.106668</td>
      <td>-0.579750</td>
      <td>-0.543655</td>
      <td>0.576594</td>
      <td>-0.355382</td>
      <td>-0.471202</td>
      <td>-0.881917</td>
      <td>1.208941</td>
      <td>0.430331</td>
      <td>0.746967</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.456737</td>
      <td>-1.244457</td>
      <td>-1.006026</td>
      <td>-0.795570</td>
      <td>0.481584</td>
      <td>-0.931678</td>
      <td>0.432823</td>
      <td>1.133893</td>
      <td>1.208941</td>
      <td>0.430331</td>
      <td>-1.140108</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.220730</td>
      <td>-0.106668</td>
      <td>0.223615</td>
      <td>-0.543655</td>
      <td>-0.981576</td>
      <td>-0.002336</td>
      <td>0.904736</td>
      <td>1.133893</td>
      <td>-0.827170</td>
      <td>-0.946729</td>
      <td>-1.140108</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.234427</td>
      <td>1.031121</td>
      <td>1.059772</td>
      <td>0.419550</td>
      <td>-0.848562</td>
      <td>0.231297</td>
      <td>-0.471202</td>
      <td>-0.881917</td>
      <td>-0.827170</td>
      <td>-0.946729</td>
      <td>-0.511083</td>
    </tr>
  </tbody>
</table>
</div>

<p>Next, we perform the clustering and present the results as a dendrogram.
The x-axis is the observations, and the y axis is the distances</p>
<pre><code class="language-python">from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree
X = data_scaled
Z = linkage(X, method='ward')
fig = plt.figure(figsize=(18, 6))
dn = dendrogram(Z)

</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_263_0.png" /></p>
<p>Question is, what can you do with this dendrogram? 
Answer is, that by ‘cutting’ the dendrogram at the right height, you can get any number of clusters or groups that you desire.</p>
<pre><code class="language-python"># Fixing some pandas display options
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 150)
</code></pre>
<pre><code class="language-python"># Look at the clusters to which each observation has been assigned

pd.DataFrame(cut_tree(Z, n_clusters=4, height=None)).transpose()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
      <th>30</th>
      <th>31</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Look at the value counts by cluster number

pd.DataFrame(cut_tree(Z, n_clusters=4, height=None)).value_counts()
</code></pre>
<pre><code>3    12
1     8
2     7
0     5
Name: count, dtype: int64
</code></pre>
<pre><code class="language-python">
Z = linkage(X, method='single')
fig = plt.figure(figsize=(15, 8))
dn = dendrogram(Z)
plt.show()
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_268_0.png" /></p>
<pre><code class="language-python"># Look at the clusters to which each observation has been assigned

pd.DataFrame(cut_tree(Z, n_clusters=4, height=None)).transpose()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
      <th>30</th>
      <th>31</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Look at the value counts by cluster number

pd.DataFrame(cut_tree(Z, n_clusters=4, height=None)).value_counts()
</code></pre>
<pre><code>0    18
1    12
2     1
3     1
Name: count, dtype: int64
</code></pre>
<h2 id="key-takeaways">Key Takeaways</h2>
<p>Understanding and defining the <script type="math/tex">X</script> and <script type="math/tex">y</script> (the predictors and the target variable) is the most important activity in modeling.  If this isn’t done right, no model can help.  (Garbage in, garbage everywhere!)</p>
<p>Models pre-built in libraries have default settings for parameters that often work out of the box with reasonable performance.  Once a modeling technique is decided, then the parameters should be reviewed and tweaked if needed.  </p>
<p>A model once built needs to be monitored for drift, which means the world may shift while the model stays the same.  Models will need retraining every once in a while as new data becomes available.  </p>
<p>Model objects in Python can be saved as a pickle file using either the Pickle or Joblib library.  (How? Refer next page.)  Various libraries offer the ability to save pickle file, but sometimes a pickle file created by one library may error out if loaded back through another library.  </p>
<hr />
<h2 id="pickle">Pickle</h2>
<p>Once you create a model, you can save it as a pickle file.  Example code below. </p>
<pre><code class="language-python">from joblib import dump, load
dump(model_name, 'filename.pickle') 
</code></pre>
<p>Then reload it as follows:</p>
<pre><code class="language-python">model_reloaded = load('filename.pickle') 
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<hr />
<h2 id="end">END</h2>
<hr />
<h2 id="random-stuff">Random stuff</h2>
<h3 id="distances">Distances</h3>
<pre><code class="language-python">X = [[0, 1, 2],
     [3, 4, 5]]

from sklearn.metrics import DistanceMetric
dist = DistanceMetric.get_metric(metric = 'euclidean')
dist.pairwise(X)
</code></pre>
<pre><code>array([[0.        , 5.19615242],
       [5.19615242, 0.        ]])
</code></pre>
<pre><code class="language-python">diamonds = sns.load_dataset(&quot;diamonds&quot;)
</code></pre>
<pre><code class="language-python">X = diamonds.iloc[:4,4:]
</code></pre>
<pre><code class="language-python">X
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>depth</th>
      <th>table</th>
      <th>price</th>
      <th>x</th>
      <th>y</th>
      <th>z</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>61.5</td>
      <td>55.0</td>
      <td>326</td>
      <td>3.95</td>
      <td>3.98</td>
      <td>2.43</td>
    </tr>
    <tr>
      <th>1</th>
      <td>59.8</td>
      <td>61.0</td>
      <td>326</td>
      <td>3.89</td>
      <td>3.84</td>
      <td>2.31</td>
    </tr>
    <tr>
      <th>2</th>
      <td>56.9</td>
      <td>65.0</td>
      <td>327</td>
      <td>4.05</td>
      <td>4.07</td>
      <td>2.31</td>
    </tr>
    <tr>
      <th>3</th>
      <td>62.4</td>
      <td>58.0</td>
      <td>334</td>
      <td>4.20</td>
      <td>4.23</td>
      <td>2.63</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">dist.pairwise(X)
</code></pre>
<pre><code>array([[ 0.        ,  6.23919867, 11.05407165,  8.60087205],
       [ 6.23919867,  0.        ,  5.04861367,  8.9504525 ],
       [11.05407165,  5.04861367,  0.        , 11.33139444],
       [ 8.60087205,  8.9504525 , 11.33139444,  0.        ]])
</code></pre>
<pre><code class="language-python">from sklearn.metrics.pairwise import cosine_similarity
</code></pre>
<pre><code class="language-python">x = [1, 1, 0]
y = [0, 1, 0]
</code></pre>
<pre><code class="language-python">import scipy
scipy.spatial.distance.cosine(x,y)
</code></pre>
<pre><code>0.29289321881345254
</code></pre>
<pre><code class="language-python">1- scipy.spatial.distance.cosine(x,y)
</code></pre>
<pre><code>0.7071067811865475
</code></pre>
<pre><code class="language-python">cosine_similarity([x,y])
</code></pre>
<pre><code>array([[1.        , 0.70710678],
       [0.70710678, 1.        ]])
</code></pre>
<h4 id="diagram-for-lda">Diagram for LDA</h4>
<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns
iris = sm.datasets.get_rdataset('iris').data
</code></pre>
<pre><code class="language-python">iris[iris['Species'].isin(['setosa', 'versicolor'])]

</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5.4</td>
      <td>3.9</td>
      <td>1.7</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>6</th>
      <td>4.6</td>
      <td>3.4</td>
      <td>1.4</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>7</th>
      <td>5.0</td>
      <td>3.4</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>8</th>
      <td>4.4</td>
      <td>2.9</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>9</th>
      <td>4.9</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>10</th>
      <td>5.4</td>
      <td>3.7</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>11</th>
      <td>4.8</td>
      <td>3.4</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>12</th>
      <td>4.8</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>13</th>
      <td>4.3</td>
      <td>3.0</td>
      <td>1.1</td>
      <td>0.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>14</th>
      <td>5.8</td>
      <td>4.0</td>
      <td>1.2</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>15</th>
      <td>5.7</td>
      <td>4.4</td>
      <td>1.5</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>16</th>
      <td>5.4</td>
      <td>3.9</td>
      <td>1.3</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>17</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>18</th>
      <td>5.7</td>
      <td>3.8</td>
      <td>1.7</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>19</th>
      <td>5.1</td>
      <td>3.8</td>
      <td>1.5</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>20</th>
      <td>5.4</td>
      <td>3.4</td>
      <td>1.7</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>21</th>
      <td>5.1</td>
      <td>3.7</td>
      <td>1.5</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>22</th>
      <td>4.6</td>
      <td>3.6</td>
      <td>1.0</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>23</th>
      <td>5.1</td>
      <td>3.3</td>
      <td>1.7</td>
      <td>0.5</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>24</th>
      <td>4.8</td>
      <td>3.4</td>
      <td>1.9</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>25</th>
      <td>5.0</td>
      <td>3.0</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>26</th>
      <td>5.0</td>
      <td>3.4</td>
      <td>1.6</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>27</th>
      <td>5.2</td>
      <td>3.5</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>28</th>
      <td>5.2</td>
      <td>3.4</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>29</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>30</th>
      <td>4.8</td>
      <td>3.1</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>31</th>
      <td>5.4</td>
      <td>3.4</td>
      <td>1.5</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>32</th>
      <td>5.2</td>
      <td>4.1</td>
      <td>1.5</td>
      <td>0.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>33</th>
      <td>5.5</td>
      <td>4.2</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>34</th>
      <td>4.9</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>35</th>
      <td>5.0</td>
      <td>3.2</td>
      <td>1.2</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>36</th>
      <td>5.5</td>
      <td>3.5</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>37</th>
      <td>4.9</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>38</th>
      <td>4.4</td>
      <td>3.0</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>39</th>
      <td>5.1</td>
      <td>3.4</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>40</th>
      <td>5.0</td>
      <td>3.5</td>
      <td>1.3</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>41</th>
      <td>4.5</td>
      <td>2.3</td>
      <td>1.3</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>42</th>
      <td>4.4</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>43</th>
      <td>5.0</td>
      <td>3.5</td>
      <td>1.6</td>
      <td>0.6</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>44</th>
      <td>5.1</td>
      <td>3.8</td>
      <td>1.9</td>
      <td>0.4</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>45</th>
      <td>4.8</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.3</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>46</th>
      <td>5.1</td>
      <td>3.8</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>47</th>
      <td>4.6</td>
      <td>3.2</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>48</th>
      <td>5.3</td>
      <td>3.7</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>49</th>
      <td>5.0</td>
      <td>3.3</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>50</th>
      <td>7.0</td>
      <td>3.2</td>
      <td>4.7</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>51</th>
      <td>6.4</td>
      <td>3.2</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>52</th>
      <td>6.9</td>
      <td>3.1</td>
      <td>4.9</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>53</th>
      <td>5.5</td>
      <td>2.3</td>
      <td>4.0</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>54</th>
      <td>6.5</td>
      <td>2.8</td>
      <td>4.6</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>55</th>
      <td>5.7</td>
      <td>2.8</td>
      <td>4.5</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>56</th>
      <td>6.3</td>
      <td>3.3</td>
      <td>4.7</td>
      <td>1.6</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>57</th>
      <td>4.9</td>
      <td>2.4</td>
      <td>3.3</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>58</th>
      <td>6.6</td>
      <td>2.9</td>
      <td>4.6</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>59</th>
      <td>5.2</td>
      <td>2.7</td>
      <td>3.9</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>60</th>
      <td>5.0</td>
      <td>2.0</td>
      <td>3.5</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>61</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>4.2</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>62</th>
      <td>6.0</td>
      <td>2.2</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>63</th>
      <td>6.1</td>
      <td>2.9</td>
      <td>4.7</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>64</th>
      <td>5.6</td>
      <td>2.9</td>
      <td>3.6</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>65</th>
      <td>6.7</td>
      <td>3.1</td>
      <td>4.4</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>66</th>
      <td>5.6</td>
      <td>3.0</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>67</th>
      <td>5.8</td>
      <td>2.7</td>
      <td>4.1</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>68</th>
      <td>6.2</td>
      <td>2.2</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>69</th>
      <td>5.6</td>
      <td>2.5</td>
      <td>3.9</td>
      <td>1.1</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>70</th>
      <td>5.9</td>
      <td>3.2</td>
      <td>4.8</td>
      <td>1.8</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>71</th>
      <td>6.1</td>
      <td>2.8</td>
      <td>4.0</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>72</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>4.9</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>73</th>
      <td>6.1</td>
      <td>2.8</td>
      <td>4.7</td>
      <td>1.2</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>74</th>
      <td>6.4</td>
      <td>2.9</td>
      <td>4.3</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>75</th>
      <td>6.6</td>
      <td>3.0</td>
      <td>4.4</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>76</th>
      <td>6.8</td>
      <td>2.8</td>
      <td>4.8</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>77</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>1.7</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>78</th>
      <td>6.0</td>
      <td>2.9</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>79</th>
      <td>5.7</td>
      <td>2.6</td>
      <td>3.5</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>80</th>
      <td>5.5</td>
      <td>2.4</td>
      <td>3.8</td>
      <td>1.1</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>81</th>
      <td>5.5</td>
      <td>2.4</td>
      <td>3.7</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>82</th>
      <td>5.8</td>
      <td>2.7</td>
      <td>3.9</td>
      <td>1.2</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>83</th>
      <td>6.0</td>
      <td>2.7</td>
      <td>5.1</td>
      <td>1.6</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>84</th>
      <td>5.4</td>
      <td>3.0</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>85</th>
      <td>6.0</td>
      <td>3.4</td>
      <td>4.5</td>
      <td>1.6</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>86</th>
      <td>6.7</td>
      <td>3.1</td>
      <td>4.7</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>87</th>
      <td>6.3</td>
      <td>2.3</td>
      <td>4.4</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>88</th>
      <td>5.6</td>
      <td>3.0</td>
      <td>4.1</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>89</th>
      <td>5.5</td>
      <td>2.5</td>
      <td>4.0</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>90</th>
      <td>5.5</td>
      <td>2.6</td>
      <td>4.4</td>
      <td>1.2</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>91</th>
      <td>6.1</td>
      <td>3.0</td>
      <td>4.6</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>92</th>
      <td>5.8</td>
      <td>2.6</td>
      <td>4.0</td>
      <td>1.2</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>93</th>
      <td>5.0</td>
      <td>2.3</td>
      <td>3.3</td>
      <td>1.0</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>94</th>
      <td>5.6</td>
      <td>2.7</td>
      <td>4.2</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>95</th>
      <td>5.7</td>
      <td>3.0</td>
      <td>4.2</td>
      <td>1.2</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>96</th>
      <td>5.7</td>
      <td>2.9</td>
      <td>4.2</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>97</th>
      <td>6.2</td>
      <td>2.9</td>
      <td>4.3</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>98</th>
      <td>5.1</td>
      <td>2.5</td>
      <td>3.0</td>
      <td>1.1</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <th>99</th>
      <td>5.7</td>
      <td>2.8</td>
      <td>4.1</td>
      <td>1.3</td>
      <td>versicolor</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">iris = iris[iris['Species'].isin(['setosa', 'versicolor'])]
sns.set_style(style='white')
plt.figure(figsize = (5,5))
sns.scatterplot(data = iris, x = 'Sepal.Width', y = 'Petal.Width', hue = 'Species', alpha = .8, edgecolor = 'None');
</code></pre>
<p><img alt="png" src="../09_Machine_Learning_files/09_Machine_Learning_289_0.png" /></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../08_Feature_Engineering/" class="btn btn-neutral float-left" title="Feature Engineering"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../10_Deep_Learning/" class="btn btn-neutral float-right" title="Deep Learning">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../08_Feature_Engineering/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../10_Deep_Learning/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
